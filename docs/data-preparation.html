<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2.2 Data Preparation | Designing a relational marketing model to maximize revenues of targeted marketing campaigns.</title>
  <meta name="description" content="Thesis for the programme Master in Statistics at the University of Neuchâtel, Switzerland.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="2.2 Data Preparation | Designing a relational marketing model to maximize revenues of targeted marketing campaigns." />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Thesis for the programme Master in Statistics at the University of Neuchâtel, Switzerland." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.2 Data Preparation | Designing a relational marketing model to maximize revenues of targeted marketing campaigns." />
  
  <meta name="twitter:description" content="Thesis for the programme Master in Statistics at the University of Neuchâtel, Switzerland." />
  

<meta name="author" content="Florian Hochstrasser">


<meta name="date" content="2019-04-16">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="tools-used.html">
<link rel="next" href="model-evaluation.html">
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="assets/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="task-background.html"><a href="task-background.html"><i class="fa fa-check"></i><b>1.1</b> Task Background</a></li>
<li class="chapter" data-level="1.2" data-path="goal.html"><a href="goal.html"><i class="fa fa-check"></i><b>1.2</b> Goal</a></li>
<li class="chapter" data-level="1.3" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i><b>1.3</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2</b> Methods</a><ul>
<li class="chapter" data-level="2.1" data-path="tools-used.html"><a href="tools-used.html"><i class="fa fa-check"></i><b>2.1</b> Tools Used</a><ul>
<li class="chapter" data-level="2.1.1" data-path="tools-used.html"><a href="tools-used.html#jupyter-notebook"><i class="fa fa-check"></i><b>2.1.1</b> Jupyter Notebook</a></li>
<li class="chapter" data-level="2.1.2" data-path="tools-used.html"><a href="tools-used.html#pandas"><i class="fa fa-check"></i><b>2.1.2</b> Pandas</a></li>
<li class="chapter" data-level="2.1.3" data-path="tools-used.html"><a href="tools-used.html#scikit-learn"><i class="fa fa-check"></i><b>2.1.3</b> Scikit-learn</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-preparation.html"><a href="data-preparation.html"><i class="fa fa-check"></i><b>2.2</b> Data Preparation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="data-preparation.html"><a href="data-preparation.html#data-set-splitting"><i class="fa fa-check"></i><b>2.2.1</b> Data Set Splitting</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-preparation.html"><a href="data-preparation.html#cleaning-and-preprocessing"><i class="fa fa-check"></i><b>2.2.2</b> Cleaning and Preprocessing</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-preparation.html"><a href="data-preparation.html#feature-engineering"><i class="fa fa-check"></i><b>2.2.3</b> Feature Engineering</a></li>
<li class="chapter" data-level="2.2.4" data-path="data-preparation.html"><a href="data-preparation.html#imputation"><i class="fa fa-check"></i><b>2.2.4</b> Imputation</a></li>
<li class="chapter" data-level="2.2.5" data-path="data-preparation.html"><a href="data-preparation.html#feature-selection"><i class="fa fa-check"></i><b>2.2.5</b> Feature Selection</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="model-evaluation.html"><a href="model-evaluation.html"><i class="fa fa-check"></i><b>2.3</b> Model Evaluation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="model-evaluation.html"><a href="model-evaluation.html#performance-metrics"><i class="fa fa-check"></i><b>2.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3.2" data-path="model-evaluation.html"><a href="model-evaluation.html#considered-algorithms"><i class="fa fa-check"></i><b>2.3.2</b> Considered Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Data</a><ul>
<li class="chapter" data-level="3.1" data-path="raw-data-import.html"><a href="raw-data-import.html"><i class="fa fa-check"></i><b>3.1</b> Raw data import</a></li>
<li class="chapter" data-level="3.2" data-path="cleaning-preprocessing.html"><a href="cleaning-preprocessing.html"><i class="fa fa-check"></i><b>3.2</b> Cleaning &amp; Preprocessing</a><ul>
<li class="chapter" data-level="3.2.1" data-path="cleaning-preprocessing.html"><a href="cleaning-preprocessing.html#date-features"><i class="fa fa-check"></i><b>3.2.1</b> Date features</a></li>
<li class="chapter" data-level="3.2.2" data-path="cleaning-preprocessing.html"><a href="cleaning-preprocessing.html#further-sanitizations"><i class="fa fa-check"></i><b>3.2.2</b> Further sanitizations</a></li>
<li class="chapter" data-level="3.2.3" data-path="cleaning-preprocessing.html"><a href="cleaning-preprocessing.html#binary-features"><i class="fa fa-check"></i><b>3.2.3</b> Binary Features</a></li>
<li class="chapter" data-level="3.2.4" data-path="cleaning-preprocessing.html"><a href="cleaning-preprocessing.html#multi-byte-categorical-features"><i class="fa fa-check"></i><b>3.2.4</b> Multi-byte Categorical Features</a></li>
<li class="chapter" data-level="3.2.5" data-path="cleaning-preprocessing.html"><a href="cleaning-preprocessing.html#missing-values"><i class="fa fa-check"></i><b>3.2.5</b> Missing Values</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>3.3</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#general-structure"><i class="fa fa-check"></i><b>3.3.1</b> General structure</a></li>
<li class="chapter" data-level="3.3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.2</b> Correlations</a></li>
<li class="chapter" data-level="3.3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#skewness"><i class="fa fa-check"></i><b>3.3.3</b> Skewness</a></li>
<li class="chapter" data-level="3.3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#targets"><i class="fa fa-check"></i><b>3.3.4</b> Targets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>3.4</b> Preprocessing</a><ul>
<li class="chapter" data-level="3.4.1" data-path="preprocessing.html"><a href="preprocessing.html#noisy-data"><i class="fa fa-check"></i><b>3.4.1</b> Noisy data</a></li>
<li class="chapter" data-level="3.4.2" data-path="preprocessing.html"><a href="preprocessing.html#constant-features"><i class="fa fa-check"></i><b>3.4.2</b> Constant features</a></li>
<li class="chapter" data-level="3.4.3" data-path="preprocessing.html"><a href="preprocessing.html#missing-values-sparse-features"><i class="fa fa-check"></i><b>3.4.3</b> Missing values / sparse features</a></li>
<li class="chapter" data-level="3.4.4" data-path="preprocessing.html"><a href="preprocessing.html#categorical-features"><i class="fa fa-check"></i><b>3.4.4</b> Categorical features</a></li>
<li class="chapter" data-level="3.4.5" data-path="preprocessing.html"><a href="preprocessing.html#feature-engineering-1"><i class="fa fa-check"></i><b>3.4.5</b> Feature Engineering</a></li>
<li class="chapter" data-level="3.4.6" data-path="preprocessing.html"><a href="preprocessing.html#feature-selection-1"><i class="fa fa-check"></i><b>3.4.6</b> Feature Selection</a></li>
<li class="chapter" data-level="3.4.7" data-path="preprocessing.html"><a href="preprocessing.html#feature-extraction"><i class="fa fa-check"></i><b>3.4.7</b> Feature Extraction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-evaluation-1.html"><a href="model-evaluation-1.html"><i class="fa fa-check"></i><b>4</b> Model evaluation</a></li>
<li class="chapter" data-level="5" data-path="results-and-discussion.html"><a href="results-and-discussion.html"><i class="fa fa-check"></i><b>5</b> Results and Discussion</a></li>
<li class="chapter" data-level="6" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>6</b> Conclusions</a><ul>
<li class="chapter" data-level="6.1" data-path="comparison-with-cup-winners.html"><a href="comparison-with-cup-winners.html"><i class="fa fa-check"></i><b>6.1</b> Comparison with Cup winners</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="6.2" data-path="code.html"><a href="code.html"><i class="fa fa-check"></i><b>6.2</b> Code</a><ul>
<li class="chapter" data-level="6.2.1" data-path="code.html"><a href="code.html#preprocessing-1"><i class="fa fa-check"></i><b>6.2.1</b> Preprocessing</a></li>
<li class="chapter" data-level="6.2.2" data-path="code.html"><a href="code.html#appendix-transformers"><i class="fa fa-check"></i><b>6.2.2</b> Transformers</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="python-environment.html"><a href="python-environment.html"><i class="fa fa-check"></i><b>6.3</b> Python Environment</a></li>
<li class="chapter" data-level="6.4" data-path="dataset-dictionary.html"><a href="dataset-dictionary.html"><i class="fa fa-check"></i><b>6.4</b> Dataset Dictionary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Designing a relational marketing model to maximize revenues of targeted marketing campaigns.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-preparation" class="section level2">
<h2><span class="header-section-number">2.2</span> Data Preparation</h2>
<p>To make data usable for learning algorithms, it generally has to be prepared. Preprations encompass fixing input errors, coercing data to correct types, encoding categorical (string) data and dealing with missing values through imputation or removal. The result of this process is an all-numeric data set.</p>
<p>The necessary transformations were determined interactively in jupyter notebooks. Once finalized, the tranformations were implemented in the python package <code>kdd98</code><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. The package can be used to download and read in raw data and apply all transformations. Each transformation is enclosed in a class (a transformer) that implements scikit-learn’s API, which allows for the transformations to be applied within a pipeline. Furthermore, the transformers can be trained and then persisted on disk for later use on other data.</p>
<p>The data set can be obtained at the following intermediate steps from <code>kdd98.data_handler.KDD98DataProvider</code>:</p>
<ul>
<li><strong>raw</strong>, as imported from csv using <code>pandas.read_csv()</code></li>
<li><strong>preprocessed</strong>, input errors removed, correct data types for all features, missing at random (MAR) imputations applied</li>
<li><strong>numeric</strong>, after feature engineering (encoded categories, date and zip code transformations)</li>
<li><strong>imputed</strong>, with NaN-values replaced by modelled values</li>
<li><strong>all-relevant</strong>, filtered down to a set of relevant features</li>
</ul>
<div id="data-set-splitting" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Data Set Splitting</h3>
<p>The complete data is distributed pre-split into a learning and test data set. The test data was set aside and treated as <em>unseen</em> for the final prediction. Preprocessing and model selection were performed on the learning data set.</p>
<p>After preprocessing and feature engineering, the learning data set was split 80/20 into training and validation sets (see Figure <a href="data-preparation.html#fig:data-splitting">2.2</a>). The training set was used to train different models while the validation set served to tune hyperparameters. The split was performed using a stratified sampling algorithm to preserve the target class frequencies.</p>
<div class="figure"><span id="fig:data-splitting"></span>
<img src="figures/preprocessing/data-splitting.png" alt="Data set use for training and predictions." width="50%" />
<p class="caption">
Figure 2.2: Data set use for training and predictions.
</p>
</div>
</div>
<div id="cleaning-and-preprocessing" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Cleaning and Preprocessing</h3>
<p>The necessary transformations were determined iteratively and interactively on the learning data set, respecting information from the data set documentation provided.</p>
<p>The goal of cleaning and preprocessing was to obtain a data set with input errors removed, consistent encoding of binary features and the treatment of features that could not be cast to numeric or categorical types automatically on import. The transformations applied can be studied in the jupyter notebook <em>1_Preprocessing.ipynb</em><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.
In short, the main areas of interest were:</p>
<ul>
<li>Removing ‘noise’: Input errors, inconsistent encoding</li>
<li>Dropping constant and sparse (i.e. those where only few examples have a value set) features</li>
<li>Imputation of MAR values</li>
</ul>
</div>
<div id="feature-engineering" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Feature Engineering</h3>
<p>During feature engineering, all non-numeric (i.e. categorical) features were encoded into numeric values. Also, several features were transformed to better usable representations. Care was taken to keep the dimensionality of the data set as low as possible.</p>
<p>The result of this transformation step was an all-numeric data set usable for downstream learning. The transformations applied in feature engineering are demonstrated in the jupyter notebook <em>2_Feature Engineering.ipynb</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
All date features were transformed into time differences against a reference date according to Table <a href="data-preparation.html#tab:date-encoding">2.1</a>.
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:date-encoding">Table 2.1: </span>Transformation of dates to time differences
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Feature Explanation
</th>
<th style="text-align:left;">
Reference date
</th>
<th style="text-align:left;">
Unit
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
DOB
</td>
<td style="text-align:left;width: 4cm; ">
Date of birth
</td>
<td style="text-align:left;width: 4cm; ">
1997-06-01 (date of most recent campaign)
</td>
<td style="text-align:left;">
Years
</td>
</tr>
<tr>
<td style="text-align:left;">
RDATE
</td>
<td style="text-align:left;width: 4cm; ">
Month when donation was received
</td>
<td style="text-align:left;width: 4cm; ">
ADATE (sending date of the corresponding campaign)
</td>
<td style="text-align:left;">
Months
</td>
</tr>
<tr>
<td style="text-align:left;">
LASTDATE
</td>
<td style="text-align:left;width: 4cm; ">
Most recent donation prior to last campaign
</td>
<td style="text-align:left;width: 4cm; ">
1997-06-01
</td>
<td style="text-align:left;">
Months
</td>
</tr>
<tr>
<td style="text-align:left;">
MINRDATE
</td>
<td style="text-align:left;width: 4cm; ">
Date of smallest donation
</td>
<td style="text-align:left;width: 4cm; ">
1997-06-01
</td>
<td style="text-align:left;">
Months
</td>
</tr>
<tr>
<td style="text-align:left;">
MAXRDATE
</td>
<td style="text-align:left;width: 4cm; ">
Date of highest donation
</td>
<td style="text-align:left;width: 4cm; ">
1997-06-01
</td>
<td style="text-align:left;">
Months
</td>
</tr>
<tr>
<td style="text-align:left;">
MAXADATE
</td>
<td style="text-align:left;width: 4cm; ">
Date of the most recent promotion received
</td>
<td style="text-align:left;width: 4cm; ">
1997-06-01
</td>
<td style="text-align:left;">
Months
</td>
</tr>
</tbody>
</table>
<div id="zip-codes" class="section level4">
<h4><span class="header-section-number">2.2.3.1</span> Zip Codes</h4>
<p>U.S. zip codes were transformed into coordinates (latitude, longitude of the centroid for a given zip) using zip code tabulation data from <span class="citation">United States Census Bureau (<a href="#ref-census-gazet">2018</a>)</span>. Several of the examples are army members, identifiable through their values in feature ‘STATE’. For these zip codes, no geographical data is available. These example’s latitude and longitude were set to the coordinates of the pentagon, the U.S. department of defense.
The 2018 zip code tabulation data was missing several zip codes that existed in 1997, at the time of the campaign. These missing zip codes were looked up on the fly using a web service<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
</div>
<div id="categorical-encoding" class="section level4">
<h4><span class="header-section-number">2.2.3.2</span> Categorical Encoding</h4>
</div>
</div>
<div id="imputation" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Imputation</h3>
<p>As required by the cup documentation, missing values were imputed. Instead of choosing a simple approach like replacing with the feature’s mean or median, a modelling approach was chosen. Package <code>fancyimpute</code> provides an iterative imputation algorithm for this purpose. Features are ordered by the fraction of missing values.</p>
</div>
<div id="feature-selection" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Feature Selection</h3>
<p>One of the biggest caveats in machine learning is the infamous “Curse of Dimensionality” coined by <span class="citation">(<span class="citeproc-not-found" data-reference-id="bellman1957dynamic"><strong>???</strong></span>)</span>. The curse comes from the fact that with an increasing number of dimensions, the required number of examples grows exponentially. In the area of machine learning, high dimensionality in short frequently leads to an overfitting of the training data, meaning that the generalisation error is unacceptably big (see <span class="citation">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span>).</p>
<p>It is therefore beneficial to reducde the data set dimensionality while preserving as much relevant information as possible. A method to deal with the problem is called boruta, introduced by <span class="citation">Kursa, Rudnicki, and others (<a href="#ref-kursa2010boruta">2010</a>)</span>. It works sequentially and removes features found to be less relevant at each iteration. By doing so, it solves the so-called all-relevant feature problem.
The algorithm is acutally a wrapper function around a random forest classifier. A random forest classifier is fast, can usually be run without parameters and returns an importance measure for each feature.</p>
<p>In short, the alogrithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>The input matrix <span class="math inline">\(\mathbf{X}\)</span> of dimension <span class="math inline">\(n\text{ x }p\)</span> is extended with <span class="math inline">\(p\)</span> so-called <em>shadow features</em>. The shadow features are permuted copies of the features in <span class="math inline">\(\mathbf{X}\)</span>. They are therefore decorrelated with the target.</li>
<li>On the resulting matrix <span class="math inline">\(\mathbf{X^*}\)</span>, a random forest classifier is trained and the Z-scores (<span class="math inline">\(\frac{\bar{loss}}{sd}\)</span>) for each of the <span class="math inline">\(2p\)</span> features calculated.</li>
<li>The highest Z-score among the shadow features <span class="math inline">\(MZSA\)</span> is determined.</li>
<li>All original features are compared against <span class="math inline">\(MZSA\)</span> and those features with a higher score selected as important.</li>
<li>With the remaining features, a two-sided test for equality of the Z-scores with <span class="math inline">\(MZSA\)</span> is performed and all features with significantly lower score are deemed unimportant.</li>
<li>All shadow copies are removed, go to step 1.</li>
</ol>
<p>The algorithm terminates when all attributes are marked as either important or not important or when the maximum number of iterations is reached.</p>
<p>For this thesis, a python implementation<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> was used. In effect, it is a port of the original R package by <span class="citation">Kursa, Rudnicki, and others (<a href="#ref-kursa2010boruta">2010</a>)</span> which plugs into scikit-learn.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-census-gazet">
<p>United States Census Bureau. 2018. “2018 U.s. Gazetteer Files.” 2018. <a href="https://www.census.gov/geographies/reference-files/2018/geo/gazetter-file.html">https://www.census.gov/geographies/reference-files/2018/geo/gazetter-file.html</a>.</p>
</div>
<div id="ref-Goodfellow-et-al-2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.</p>
</div>
<div id="ref-kursa2010boruta">
<p>Kursa, Miron B, Witold R Rudnicki, and others. 2010. “Feature Selection with the Boruta Package.” <em>J Stat Softw</em> 36 (11): 1–13.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>see <a href="https://github.com/datarian/master-thesis-code/tree/master/kdd98">github.com/datarian/master-thesis-code/kdd98</a><a href="data-preparation.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>see <a href="https://github.com/datarian/master-thesis-code/tree/master/notebooks/1_Preprocessing.ipynb">github.com/datarian/master-thesis-code/notebooks/1_Preprocessing.ipynb</a><a href="data-preparation.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>see <a href="https://github.com/datarian/master-thesis-code/tree/master/notebooks/2_Feature%20Engineering.ipynb">github.com/datarian/master-thesis-code/notebooks/2_Feature Engineering.ipynb</a><a href="data-preparation.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p><a href="https://developer.here.com/products/geocoding-and-search">HERE geocoding</a><a href="data-preparation.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>see <a href="https://github.com/scikit-learn-contrib/boruta_py">scikit-learn-contrib/boruta_py</a><a href="data-preparation.html#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tools-used.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-evaluation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Master_Thesis_Florian_Hochstrasser.pdf", "Master_Thesis_Florian_Hochstrasser.epub"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
