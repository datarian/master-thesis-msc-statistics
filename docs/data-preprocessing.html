<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.3 Data Preprocessing | Profit maximization for direct marketing campaigns</title>
  <meta name="description" content="Master Thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Statistics" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="3.3 Data Preprocessing | Profit maximization for direct marketing campaigns" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Master Thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.3 Data Preprocessing | Profit maximization for direct marketing campaigns" />
  
  <meta name="twitter:description" content="Master Thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Statistics" />
  

<meta name="author" content="Florian Hochstrasser" />


<meta name="date" content="2019-06-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-handling.html">
<link rel="next" href="methods-prediction.html">
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="assets/kePrint-0.0.1/kePrint.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="task-background.html"><a href="task-background.html"><i class="fa fa-check"></i><b>1.1</b> Task Background</a></li>
<li class="chapter" data-level="1.2" data-path="goal.html"><a href="goal.html"><i class="fa fa-check"></i><b>1.2</b> Goal</a></li>
<li class="chapter" data-level="1.3" data-path="conventions-and-notes.html"><a href="conventions-and-notes.html"><i class="fa fa-check"></i><b>1.3</b> Conventions and Notes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>2</b> Data</a><ul>
<li class="chapter" data-level="2.1" data-path="general-structure.html"><a href="general-structure.html"><i class="fa fa-check"></i><b>2.1</b> General Structure</a></li>
<li class="chapter" data-level="2.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>2.2</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="2.2.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-types"><i class="fa fa-check"></i><b>2.2.1</b> Data Types</a></li>
<li class="chapter" data-level="2.2.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#targets"><i class="fa fa-check"></i><b>2.2.2</b> Targets</a></li>
<li class="chapter" data-level="2.2.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#skewness"><i class="fa fa-check"></i><b>2.2.3</b> Skewness</a></li>
<li class="chapter" data-level="2.2.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlations"><i class="fa fa-check"></i><b>2.2.4</b> Correlations</a></li>
<li class="chapter" data-level="2.2.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#donation-patterns"><i class="fa fa-check"></i><b>2.2.5</b> Donation Patterns</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experimental-setup-and-methods.html"><a href="experimental-setup-and-methods.html"><i class="fa fa-check"></i><b>3</b> Experimental Setup and Methods</a><ul>
<li class="chapter" data-level="3.1" data-path="tools-used.html"><a href="tools-used.html"><i class="fa fa-check"></i><b>3.1</b> Tools Used</a></li>
<li class="chapter" data-level="3.2" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>3.2</b> Data Handling</a></li>
<li class="chapter" data-level="3.3" data-path="data-preprocessing.html"><a href="data-preprocessing.html"><i class="fa fa-check"></i><b>3.3</b> Data Preprocessing</a><ul>
<li class="chapter" data-level="3.3.1" data-path="data-preprocessing.html"><a href="data-preprocessing.html#cleaning"><i class="fa fa-check"></i><b>3.3.1</b> Cleaning</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-preprocessing.html"><a href="data-preprocessing.html#methods-feature-engineering"><i class="fa fa-check"></i><b>3.3.2</b> Feature Engineering</a></li>
<li class="chapter" data-level="3.3.3" data-path="data-preprocessing.html"><a href="data-preprocessing.html#imputation"><i class="fa fa-check"></i><b>3.3.3</b> Imputation</a></li>
<li class="chapter" data-level="3.3.4" data-path="data-preprocessing.html"><a href="data-preprocessing.html#methods-feature-selection"><i class="fa fa-check"></i><b>3.3.4</b> Feature Selection</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="methods-prediction.html"><a href="methods-prediction.html"><i class="fa fa-check"></i><b>3.4</b> Prediction</a><ul>
<li class="chapter" data-level="3.4.1" data-path="methods-prediction.html"><a href="methods-prediction.html#optimization-of-alpha"><i class="fa fa-check"></i><b>3.4.1</b> Optimization of <span class="math inline">\(\alpha^*\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="eval-and-select.html"><a href="eval-and-select.html"><i class="fa fa-check"></i><b>3.5</b> Model Evaluation and -Selection</a><ul>
<li class="chapter" data-level="3.5.1" data-path="eval-and-select.html"><a href="eval-and-select.html#evaluation"><i class="fa fa-check"></i><b>3.5.1</b> Evaluation</a></li>
<li class="chapter" data-level="3.5.2" data-path="eval-and-select.html"><a href="eval-and-select.html#dealing-with-imbalanced-data"><i class="fa fa-check"></i><b>3.5.2</b> Dealing With Imbalanced Data</a></li>
<li class="chapter" data-level="3.5.3" data-path="eval-and-select.html"><a href="eval-and-select.html#algorithms"><i class="fa fa-check"></i><b>3.5.3</b> Algorithms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="results-and-discussion.html"><a href="results-and-discussion.html"><i class="fa fa-check"></i><b>4</b> Results and Discussion</a><ul>
<li class="chapter" data-level="4.1" data-path="preprocessing-with-package-kdd98.html"><a href="preprocessing-with-package-kdd98.html"><i class="fa fa-check"></i><b>4.1</b> Preprocessing With Package kdd98</a></li>
<li class="chapter" data-level="4.2" data-path="imputation-1.html"><a href="imputation-1.html"><i class="fa fa-check"></i><b>4.2</b> Imputation</a></li>
<li class="chapter" data-level="4.3" data-path="feature-selection.html"><a href="feature-selection.html"><i class="fa fa-check"></i><b>4.3</b> Feature Selection</a></li>
<li class="chapter" data-level="4.4" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>4.4</b> Classifiers</a></li>
<li class="chapter" data-level="4.5" data-path="regressors.html"><a href="regressors.html"><i class="fa fa-check"></i><b>4.5</b> Regressors</a></li>
<li class="chapter" data-level="4.6" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>4.6</b> Prediction</a><ul>
<li class="chapter" data-level="4.6.1" data-path="prediction.html"><a href="prediction.html#conditional-prediction-of-the-donation-amount"><i class="fa fa-check"></i><b>4.6.1</b> Conditional Prediction of the Donation Amount</a></li>
<li class="chapter" data-level="4.6.2" data-path="prediction.html"><a href="prediction.html#profit-optimization"><i class="fa fa-check"></i><b>4.6.2</b> Profit Optimization</a></li>
<li class="chapter" data-level="4.6.3" data-path="prediction.html"><a href="prediction.html#final-prediction"><i class="fa fa-check"></i><b>4.6.3</b> Final Prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>5</b> Conclusions</a><ul>
<li class="chapter" data-level="5.1" data-path="comparison-with-cup-winners.html"><a href="comparison-with-cup-winners.html"><i class="fa fa-check"></i><b>5.1</b> Comparison With Cup Winners</a></li>
<li class="chapter" data-level="5.2" data-path="biggest-problems-remaining.html"><a href="biggest-problems-remaining.html"><i class="fa fa-check"></i><b>5.2</b> Biggest Problems Remaining</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="appendix-appendix.html"><a href="appendix-appendix.html"><i class="fa fa-check"></i>(APPENDIX) Appendix</a><ul>
<li class="chapter" data-level="5.3" data-path="python-environment.html"><a href="python-environment.html"><i class="fa fa-check"></i><b>5.3</b> Python Environment</a></li>
<li class="chapter" data-level="5.4" data-path="data-set-dictionary.html"><a href="data-set-dictionary.html"><i class="fa fa-check"></i><b>5.4</b> Data Set Dictionary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Profit maximization for direct marketing campaigns</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-preprocessing" class="section level2">
<h2><span class="header-section-number">3.3</span> Data Preprocessing</h2>
<p>The necessary preprocessing was guided by practical necessity (input errors, inconsistent categories), the requirements of the algorithms that were examined (Section @eval-and-select) and the requirements set out in the cup documentation:</p>
<ul>
<li>Only numeric features (required by some algorithms)</li>
<li>Imputation of missing values (required by cup documentation)</li>
<li>Removal of <em>sparse</em> features (required by cup documentation)</li>
</ul>
<p>The transformations were established interactively in Jupyter notebooks. Once finalized, transformations were implemented in the python package <code>kdd98</code><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>.</p>
<div id="cleaning" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Cleaning</h3>
<p>The transformations applied can be studied in the Jupyter notebook <em>1_Preprocessing.ipynb</em><a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>.</p>
<p>The cleaning stage of preprocessing encompassed the following transformations:</p>
<ul>
<li>Removing <em>noise</em>: Input errors, inconsistent encoding of binary / categorical features</li>
<li>Dropping constant and sparse (i.e. those where only few examples have a value set) features</li>
<li>Imputation of values missing at random (MAR)</li>
</ul>
<p>MAR values in the sense of <span class="citation">Rubin (<a href="references.html#ref-rubin1976inference">1976</a>)</span> are missing conditionally on other features in the data. For example, there are three related features from the promotion and giving history: <em>ADATE</em>, the date of mailing a promotion, <em>RDATE</em>, the date of receiving a donation in response to the promotion and <em>RAMOUNT</em>, the amount received. For missing <em>RAMOUNT</em> values, we can check if <em>RDATE</em> is non-missing. If <em>RDATE</em> is missing, then the example most likely has not donated and we can set <em>RAMOUNT</em> to zero. If, on the other hand, both date features have a value, <em>RAMOUNT</em> is truly missing.</p>
</div>
<div id="methods-feature-engineering" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Feature Engineering</h3>
<p>During feature engineering, all non-numeric (i.e. categorical) features were encoded into numeric values. Also, several features were transformed to better usable representations. Care was taken to keep the dimensionality of the data set as low as possible. The result of this transformation step was an all-numeric data set usable for downstream learning. The transformations applied in feature engineering are described in detail in the Jupyter notebook <em>2_Feature_Engineering.ipynb</em><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<p>For ordinal features, manual mappings from alphanumeric levels to integer numbers were specified.</p>
<p>For nominal features, two encoding techniques were employed, depending on the number of levels:</p>
<ul>
<li><p>One-hot encoding for <span class="math inline">\(\leq 10\)</span> levels: For each level of a categorical feature, a new feature is created. An additional feature may be added to indicate missing values. Exactly one of these new features is set to <span class="math inline">\(1\)</span>, indicating the original level.</p></li>
<li><p>Binary encoding, <span class="math inline">\(&gt; 10\)</span> levels: The levels of the categorical are first transformed ordinally (i.e. to a sequence of integer numbers). Then, these numbers are taken to the base of 2. (A <span class="math inline">\(5\)</span>, for example, becomes <code>101</code>). According to the number of levels, new features for the binary digits are created. As an example: To represent 60 levels, 6 features are required (<span class="math inline">\(2^6=64\)</span>).</p></li>
</ul>
</div>
<div id="imputation" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Imputation</h3>
<p>Three different approaches were evaluated. The details are shown in Jupyter notebook <em>4_Imputation.ipynb</em><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>.</p>
<div id="k-nearest-neighbors" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> K-Nearest Neighbors</h4>
<p>The kNN algorithm by <span class="citation">Troyanskaya et al. (<a href="references.html#ref-troyanskaya2001missing">2001</a>)</span> works approximately as follows:</p>
<ol style="list-style-type: decimal">
<li>Construct the distance matrix <span class="math inline">\(D\)</span> with distances between examples</li>
<li>Order all features with missing values descending by number of missing values</li>
<li>Starting with the feature with most missing, use the <span class="math inline">\(k\)</span> nearest neighbors of each example with a missing value that have a value for the current feature to impute using either the mean or median.</li>
</ol>
<p>The algorithm runs until all values are imputed.</p>
<p>While very attractive because of the intuitive approach and because it preserves data types in the features, the distance matrix is very memory-intensive for large data sets. Also, all features with more than 80 % missing values have to be removed from the data first.</p>
</div>
<div id="iterative-imputation" class="section level4">
<h4><span class="header-section-number">3.3.3.2</span> Iterative imputation</h4>
<p>Iterative imputation, implemented in package <code>fancyimpute</code><a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>, works similar to the R-package <code>mice</code> (see <span class="citation">van Buuren and Groothuis-Oudshoorn (<a href="references.html#ref-buuren2011mice">2011</a>)</span>). Before imputation, all features have to be transformed to numerical data types. Categorical features were therefore encoded first, using a one-hot or binary encoding and preserving missing values.</p>
<ol style="list-style-type: decimal">
<li>Features are ordered by the fraction of missing values</li>
<li>Starting with the feature with most missing values, use the other features to build a model, using the current feature as the dependent variable and predict missing values.</li>
<li>Repeat step 2 until all features are complete</li>
<li>Repeat steps 2 – 3 <span class="math inline">\(n\)</span> times, <span class="math inline">\(n=5\)</span> was chosen</li>
</ol>
</div>
<div id="median-imputation-and-categorical-indicator" class="section level4">
<h4><span class="header-section-number">3.3.3.3</span> Median Imputation and Categorical Indicator</h4>
<p>The ‘sklearn.impute.SimpleImputer’ was used on all numerical features. Since many features are skewed and have outliers, the median strategy was used. The missing values in each feature are imputed by the feature’s median value.</p>
<p>As this implementation only supports numerical data types, categorical features were treated separately during feature engineering (Section <a href="data-preprocessing.html#methods-feature-engineering">3.3.2</a>): The one-hot or binary encoded categoricals had one more feature added, indicating missing values.</p>
</div>
</div>
<div id="methods-feature-selection" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Feature Selection</h3>
<p>One of the biggest caveats in machine learning is the infamous “Curse of Dimensionality” coined by <span class="citation">Bellman (<a href="references.html#ref-bellman1966dynamic">1966</a>)</span>. The curse comes from the fact that with an increasing number of dimensions of the feature space, the number of possible combinations grows exponentially. In order to cover all possible combinations with several examples, a huge amount of examples would be required as a result. <span class="citation">Hughes (<a href="references.html#ref-hughes1968mean">1968</a>)</span> showed that for a fixed number of examples, model performance first increases with increasing number of dimensions but then decreases again. In the area of machine learning, high dimensionality frequently manifests in the form of overfitting, which leads to an unacceptably big generalization error <span class="citation">Goodfellow, Bengio, and Courville (<a href="references.html#ref-Goodfellow-et-al-2016">2016</a>)</span>.</p>
<p>It is therefore beneficial to reduce the data set dimensionality while preserving as much relevant information as possible. A method to deal with the problem is called boruta, introduced by <span class="citation">Kursa, Rudnicki, and others (<a href="references.html#ref-kursa2010boruta">2010</a>)</span>. The algorithm was found to perform very well regarding selection of relevant features in <span class="citation">Kursa and Rudnicki (<a href="references.html#ref-kursa2011boruta">2011</a>)</span>. It works sequentially and removes features found to be less relevant at each iteration. By doing so, it solves the so-called all-relevant feature problem.
The algorithm is actually a wrapper function around a random forest classifier. A random forest classifier is fast, can usually be run without parameters and returns an importance measure for each feature.</p>
<p>In short, the alogrithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>The input matrix <span class="math inline">\(\mathbf{X}\)</span> of dimension <span class="math inline">\(n \times p\)</span> is extended with <span class="math inline">\(p\)</span> so-called <em>shadow features</em>. The shadow features are permuted copies of the features in <span class="math inline">\(\mathbf{X}\)</span>. They are therefore decorrelated with the target.</li>
<li>On the resulting matrix <span class="math inline">\(\mathbf{X^*}\)</span>, a random forest classifier is trained and the Z-scores (<span class="math inline">\(\frac{\bar{loss}}{sd}\)</span>) for each of the <span class="math inline">\(2p\)</span> features calculated.</li>
<li>The highest Z-score among the shadow features <span class="math inline">\(MZSA\)</span> is determined.</li>
<li>All original features are compared against <span class="math inline">\(MZSA\)</span> and those features with a higher score selected as important.</li>
<li>With the remaining features, a two-sided test for equality of the Z-scores with <span class="math inline">\(MZSA\)</span> is performed and all features with significantly lower score are deemed unimportant.</li>
<li>All shadow copies are removed, go to step 1.</li>
</ol>
<p>The algorithm terminates when all attributes are marked as either important or not important or when the maximum number of iterations is reached.</p>
<p>For this thesis, a python implementation<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> was used. In effect, it is a port of the original R package by <span class="citation">Kursa, Rudnicki, and others (<a href="references.html#ref-kursa2010boruta">2010</a>)</span> which conveniently implements <code>scikit-learn</code>’s API.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>Available from <a href="https://github.com/datarian/master-thesis-code/tree/master/kdd98">github.com/datarian/master-thesis-code/kdd98</a><a href="data-preprocessing.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>see <a href="https://github.com/datarian/master-thesis-code/tree/master/notebooks/1_Preprocessing.ipynb">github.com/datarian/master-thesis-code/notebooks/1_Preprocessing.ipynb</a><a href="data-preprocessing.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>see <a href="https://github.com/datarian/master-thesis-code/tree/master/notebooks/2_Feature_Engineering.ipynb">github.com/datarian/master-thesis-code/notebooks/2_Feature_Engineering.ipynb</a><a href="data-preprocessing.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>see <a href="https://github.com/datarian/master-thesis-code/tree/master/notebooks/4_Imputation.ipynb">github.com/datarian/master-thesis-code/notebooks/4_Imputation.ipynb</a><a href="data-preprocessing.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>Available at: <a href="https://pypi.org/project/fancyimpute/">https://pypi.org/project/fancyimpute/</a>, accessed on 30.06.2019<a href="data-preprocessing.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>see <a href="https://github.com/scikit-learn-contrib/boruta_py">scikit-learn-contrib/boruta_py</a><a href="data-preprocessing.html#fnref11" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-handling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methods-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Master_Thesis_Florian_Hochstrasser.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
