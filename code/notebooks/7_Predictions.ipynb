{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-data-and-estimators\" data-toc-modified-id=\"Loading-data-and-estimators-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading data and estimators</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Loading-Models\" data-toc-modified-id=\"Loading-Models-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Loading Models</a></span></li><li><span><a href=\"#Splitting-into-learning-and-validation-sets\" data-toc-modified-id=\"Splitting-into-learning-and-validation-sets-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Splitting into learning and validation sets</a></span></li></ul></li><li><span><a href=\"#Implementation-of-the-two-stage-estimation\" data-toc-modified-id=\"Implementation-of-the-two-stage-estimation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implementation of the two-stage estimation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predicting-donors\" data-toc-modified-id=\"Predicting-donors-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Predicting donors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-performance,-retrain-on-$X_{train}$\" data-toc-modified-id=\"Check-performance,-retrain-on-$X_{train}$-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Check performance, retrain on $X_{train}$</a></span></li></ul></li><li><span><a href=\"#Estimating-conditional-donation-amounts\" data-toc-modified-id=\"Estimating-conditional-donation-amounts-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Estimating conditional donation amounts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-$\\mathbf{X}_d,-y_d$\" data-toc-modified-id=\"Creating-$\\mathbf{X}_d,-y_d$-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Creating $\\mathbf{X}_d, y_d$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-$y_d$\" data-toc-modified-id=\"Transforming-$y_d$-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Transforming $y_d$</a></span></li></ul></li><li><span><a href=\"#Distribution-of-target-before-and-after-transformation\" data-toc-modified-id=\"Distribution-of-target-before-and-after-transformation-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Distribution of target before and after transformation</a></span></li></ul></li><li><span><a href=\"#Predicting-Profit\" data-toc-modified-id=\"Predicting-Profit-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Predicting Profit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Assessing-regressor-performance\" data-toc-modified-id=\"Assessing-regressor-performance-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Assessing regressor performance</a></span></li><li><span><a href=\"#Predicted-conditional-donation-amount-using-the-regression-model\" data-toc-modified-id=\"Predicted-conditional-donation-amount-using-the-regression-model-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Predicted conditional donation amount using the regression model</a></span></li><li><span><a href=\"#Unit-cost\" data-toc-modified-id=\"Unit-cost-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Unit cost</a></span></li><li><span><a href=\"#The-profit-function-$\\Pi_{alpha}$\" data-toc-modified-id=\"The-profit-function-$\\Pi_{alpha}$-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>The profit function $\\Pi_{alpha}$</a></span></li><li><span><a href=\"#Optimizing-$\\alpha$\" data-toc-modified-id=\"Optimizing-$\\alpha$-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>Optimizing $\\alpha$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estimation-of-profit-for-a-grid-of-$\\alpha$-values\" data-toc-modified-id=\"Estimation-of-profit-for-a-grid-of-$\\alpha$-values-2.3.5.1\"><span class=\"toc-item-num\">2.3.5.1&nbsp;&nbsp;</span>Estimation of profit for a grid of $\\alpha$-values</a></span></li><li><span><a href=\"#Fitting-a-model\" data-toc-modified-id=\"Fitting-a-model-2.3.5.2\"><span class=\"toc-item-num\">2.3.5.2&nbsp;&nbsp;</span>Fitting a model</a></span></li></ul></li><li><span><a href=\"#Prediction-with-$\\alpha^*$\" data-toc-modified-id=\"Prediction-with-$\\alpha^*$-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span>Prediction with $\\alpha^*$</a></span></li></ul></li><li><span><a href=\"#Putting-the-pieces-together\" data-toc-modified-id=\"Putting-the-pieces-together-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Putting the pieces together</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fitting-the-profit-estimator\" data-toc-modified-id=\"Fitting-the-profit-estimator-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Fitting the profit estimator</a></span></li><li><span><a href=\"#Prediction-on-the-learning-data\" data-toc-modified-id=\"Prediction-on-the-learning-data-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Prediction on the learning data</a></span></li><li><span><a href=\"#Prediction-on-the-test-data\" data-toc-modified-id=\"Prediction-on-the-test-data-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Prediction on the test data</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run /home/datarian/git/master-thesis-code/notebooks/common_init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run /home/datarian/git/master-thesis-code/notebooks/learning_init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load custom code\n",
    "import kdd98.data_handler as dh\n",
    "from kdd98.config import Config\n",
    "from kdd98.transformers import Rescaler\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, recall_score, r2_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from glmnet import LogitNet\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "\n",
    "CHAPTER_ID = \"predictions\"\n",
    "# Where to save the figures\n",
    "IMAGES_PATH = pathlib.Path(figure_output, CHAPTER_ID)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(Config.get(\"qual_palette\"))\n",
    "plt.rcParams['figure.figsize'] = (1.618*10, 10)\n",
    "\n",
    "pathlib.Path(IMAGES_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=[\"pdf\", \"png\"], resolution=300):\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    [plt.savefig(pathlib.Path(IMAGES_PATH, fig_id + \".\" + f), \n",
    "                 format=f,\n",
    "                 dpi=resolution,\n",
    "                 transparent=True,\n",
    "                 bbox_inches='tight') for f in fig_extension]\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10*1.618,10)\n",
    "sns.set_palette(Config.get(\"qual_palette\"))\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "setup_file_logger(\"predictions.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Loading data and estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "learning_provider = dh.KDD98DataProvider(\"cup98LRN.txt\")\n",
    "test_provider = dh.KDD98DataProvider(\"cup98VAL.txt\")\n",
    "learning = learning_provider.all_relevant_data\n",
    "test = test_provider.all_relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = learning[\"data\"]\n",
    "y_train = learning[\"targets\"]\n",
    "\n",
    "X_test = test[\"data\"]\n",
    "y_test = test[\"targets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path(Config.get(\"model_store\"),\"best_classifier.pkl\"), \"rb\") as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "with open(pathlib.Path(Config.get(\"model_store\"),\"best_regressor.pkl\"), \"rb\") as f:\n",
    "    regressor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into learning and validation sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to preserve this ratio in the split datasets. scikit-learn provides a stratified sampler for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "seed = Config.get(\"random_seed\")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8, random_state=seed)\n",
    "for learn_index, val_index in splitter.split(X_train, y_train.TARGET_B.astype('int')):\n",
    "    X_learn = X_train.iloc[learn_index]\n",
    "    y_learn = y_train.iloc[learn_index]\n",
    "    X_val = X_train.iloc[val_index]\n",
    "    y_val = y_train.iloc[val_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the two-stage estimation\n",
    "\n",
    "Since we predict the probability of donating through a model, we introduce bias in the estimation of the (conditional) donation amount and hence the final prediciton of net profit.\n",
    "\n",
    "To correct this bias, a correction in the spirit of the two-stage Heckman correction is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting donors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "sampler = BorderlineSMOTE(random_state=Config.get(\"random_seed\"))\n",
    "scaler = PowerTransformer(method=\"yeo-johnson\", standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b_predict = classifier.predict(X_val.values)\n",
    "y_b_predict_proba = classifier.predict_proba(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check performance, retrain on $X_{train}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the results. Recall for class $1$ is 0.56."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val.TARGET_B.astype(\"int\").values, y_b_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_b_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_b_predict_proba[:,1])\n",
    "ax.set(xlabel=\"Predicted Donation Probability\")\n",
    "plt.xlim(-0.01, 1.01)\n",
    "save_fig(\"y_b_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating conditional donation amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating $\\mathbf{X}_d, y_d$\n",
    "\n",
    "We select all examples who did donate (where $TARGET\\_B = 1$) and create $X_d, y_d$. This sample is then used to build a model that predicts donation amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y_train.TARGET_B.astype(\"int\").astype(\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d = X_train.loc[mask, :].values\n",
    "X_d_transformed = scaler.fit_transform(X_d)\n",
    "X_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = y_train.loc[mask, \"TARGET_D\"].values\n",
    "y_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming $y_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rgression models perform better if the target is close to a normal distribution. We therefore transform the target before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path(Config.get(\"model_store\"), \"target_d_transformer.pkl\"), \"rb\") as f:\n",
    "        target_transformer = pickle.load(f)\n",
    "y_d_transformed = target_transformer.transform(np.array(y_d).reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box-Cox param:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transformer.lambdas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of target before and after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "ax1 = sns.distplot(y_d, bins=40)\n",
    "ax1.set(xlabel='Donation amount, $ US', ylabel=\"Density\")\n",
    "plt.subplot(1,2,2)\n",
    "ax2 = sns.distplot(y_d_transformed, bins=20)\n",
    "ax2.set(xlabel='Transformed donation amount, $ US')\n",
    "save_fig(\"target_d-distributions-before-after-transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing regressor performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.score(X_d,y_d_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted conditional donation amount using the regression model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the donation amount predictions  shows that we are missing the largest donation amounts. Nevertheless, the distribution is similar to the original one.\n",
    "This estimate is biased because we drew a non-random sample (only donors) to fit the regressor above. We will therefore have to correct our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d_predict = regressor.predict(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(target_transformer.inverse_transform(np.array(y_d_predict).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(target_transformer.inverse_transform(np.array(y_d_predict).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "ax1 = sns.distplot(y_d_predict, bins=40)\n",
    "ax1.set(xlabel=\"Predicted donation amount (transformed), $ US\", ylabel=\"Density\")\n",
    "plt.subplot(1,2,2)\n",
    "ax2 = sns.distplot(target_transformer.inverse_transform(np.array(y_d_predict).reshape(-1,1)))\n",
    "ax2.set(xlabel=\"Predicted donation amount, $ US\")\n",
    "save_fig(\"hat_y_d-distributions-before-after-transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit cost\n",
    "\n",
    "One promotion letter sent out costs the organisation 0.68 \\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The profit function $\\Pi_{alpha}$\n",
    "\n",
    "The profit function corrects for the introduced bias due to the non-random sampling of donors to generate the sample for training the regressor.\n",
    "\n",
    "We maximize profit using parameter $\\alpha$, which adjusts the predicted donation amount for comparison with the unit cost.\n",
    "\n",
    "We want to include the fewest possible number of non-donors, therefore maximizing profit.\n",
    "\n",
    "The parameter $\\alpha^*$ is then applied to predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_alpha(y_b_predict_proba, y_d_predict, y_d_true, transformer, alpha = 1.0, u=0.68):\n",
    "    \"\"\"\n",
    "    Calculates the net profit for a given optimization parameter alpha.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    y_b_predict_proba: Predicted donation probability\n",
    "    y_d_predict:       Predicted donation amount, transformed with a power-transform\n",
    "    y_d_true:          True donation amount\n",
    "    transformer:       Transformation applied to y_true to normalize distribution\n",
    "    alpha:             Optimization parameter\n",
    "    u:                 Utility cost, defaults to 0.68 $ US\n",
    "    \"\"\"\n",
    "    \n",
    "    u_trans = transformer.transform(np.array(u).reshape(-1,1)).ravel()[0]\n",
    "    \n",
    "    # The indicator function used to determine if an example is predicted to yield a profit.\n",
    "    indicator = (y_b_predict_proba * np.exp(y_d_predict) * alpha > np.exp(u_trans)).ravel()\n",
    "    \n",
    "    true_profit = y_d_true - u\n",
    "    \n",
    "    profit = np.dot(indicator, true_profit)\n",
    "    \n",
    "    return profit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of profit for a grid of $\\alpha$-values\n",
    "\n",
    "The maximum profit achievable: Only mail those members who donate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_train.TARGET_D.values[y_train.TARGET_D.values > 0.0] - u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = list()\n",
    "alpha_grid = np.linspace(0.0, 1., 10000)\n",
    "for a in alpha_grid:\n",
    "    prof.append(pi_alpha(y_b_predict_proba[:,1], y_d_predict, y_train.TARGET_D.values, target_transformer, alpha=a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_curve = pd.DataFrame((alpha_grid, prof)).T\n",
    "profit_curve.columns = [\"alpha\", \"profit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate data for the whole range of $[0,1]$. We can see the profit stays constant from a value of about 0.15, which means that at this point, all examples are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(profit_curve.profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"alpha\",y=\"profit\", data=profit_curve)\n",
    "ax.set(xlabel=\"alpha\", ylabel=\"Profit, $ US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's *zoom* in on the interesting area and generate new data, stopping once profit stays constant. It is now evident that there is a very small range of *good* $\\alpha$ values that we need to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_profit_for_alphas(y_b_predict_proba, y_d_predict, y_d_true, transformer, u=0.68, n_iter_no_change=200):\n",
    "    # Generates profit for a grid of alpha values.\n",
    "    # Stops once there's no change (all predicted examples are selected)\n",
    "    alpha_grid = np.linspace(0.0, 1., 100000)\n",
    "    data = []\n",
    "    profits = []\n",
    "    for a in alpha_grid:\n",
    "        if len(profits) > n_iter_no_change and profits[-1] > 0.0:\n",
    "            if len(set(profits[-n_iter_no_change:])) == 1:\n",
    "                break\n",
    "        profits.append(pi_alpha(y_b_predict_proba, y_d_predict, y_d_true, target_transformer, alpha=a))\n",
    "        data.append({\"alpha\": a, \"profit\": profits[-1]})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomed_data = generate_profit_for_alphas(y_b_predict_proba[:,1], y_d_predict, y_train.TARGET_D.values, target_transformer)\n",
    "zoomed_data = pd.DataFrame(zoomed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"alpha\", y=\"profit\", data = zoomed_data)\n",
    "ax.set(title = \"Profit with $\\hat{y}_{i,b}$ in indicator function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first apply a lowess filter to smooth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "lowess_filtered = lowess(profit_curve.profit, profit_curve.alpha, is_sorted=True, frac=0.03, it=0)\n",
    "lowess_filtered = pd.DataFrame(lowess_filtered, columns=[\"alpha\", \"profit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"alpha\", y=\"profit\", data=lowess_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.polyfit(lowess_filtered.alpha, lowess_filtered.profit,degree)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_model = np.poly1d(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial = pd.DataFrame([profit_curve.alpha, polynomial_model(profit_curve.alpha)]).T\n",
    "polynomial.columns = [\"alpha\", \"polynomial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"alpha\", y=\"polynomial\", data=polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cubic Spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "s= CubicSpline(profit_curve.alpha.values, profit_curve.profit.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(profit_curve.alpha.values, s(profit_curve.alpha.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = s.derivative()\n",
    "roots = ds.roots()\n",
    "candidates = roots[~np.isnan(roots)] # filter out discontinuities\n",
    "\n",
    "alpha_star = candidates[np.argmax(s(candidates))]\n",
    "max_profit = pi_alpha(y_b_predict_proba[:,1], y_d_predict, y_train.TARGET_D.values, target_transformer, alpha=alpha_star)\n",
    "alpha_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic_spline = pd.DataFrame((profit_curve.alpha, s(profit_curve.alpha.values))).T\n",
    "cubic_spline.columns =[\"alpha\", \"cubic spline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_model = pd.melt(profit_curve.merge(cubic_spline, on=\"alpha\").merge(polynomial, on=\"alpha\"),\n",
    "                         id_vars=[\"alpha\"], value_vars=[\"profit\",\"cubic spline\", \"polynomial\"],\n",
    "                        var_name=\"curve\", value_name=\"profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_profit = pd.DataFrame({\"alpha\": [alpha_star], \"curve\": [\"max at alpha*\"], \"profit\": [max_profit]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"alpha\", y=\"profit\", hue=\"curve\", data = data_and_model, style=\"curve\", legend=False)\n",
    "sns.scatterplot(x=\"alpha\", y=\"profit\", color=Config.get(\"qual_palette\")[4], data=max_profit, ax=ax, legend=False)\n",
    "plt.legend(title=None, loc='upper right', labels=['Predicted', 'Cubic Spline', \"Polynomial\", r\"Max. at $\\alpha^*$\"])\n",
    "plt.plot((alpha_star, alpha_star), (0, max_profit.profit.max()), \"--\", color=Config.get(\"qual_palette\")[4], linewidth=2, )\n",
    "ax.set(xlabel=r\"$\\alpha$\", ylabel=\"Expected profit, $ US\")\n",
    "plt.xlim((-0.01,0.3))\n",
    "save_fig(\"comparison-alpha-profit-models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimisation for the polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_p(x, coefs):\n",
    "    deg = len(coefs) - 1\n",
    "    pred = 0\n",
    "    for c in coefs:\n",
    "        pred += x**deg*c\n",
    "        deg -= 1\n",
    "    return -pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(f_p, x0=0.02, args=(weights)).x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_star_poly = minimize(f_p, x0=0.02, args=(weights)).x[0]\n",
    "max_profit_poly = -f_p(a_star_poly, weights)\n",
    "xp = np.linspace(0, .3, 100)\n",
    "ax = plt.plot(profit_curve.alpha, profit_curve.profit, '.', xp, polynomial_model(xp), '-')\n",
    "plt.plot((a_star_poly, a_star_poly), (0, max_profit_poly), \"--\", color=Config.get(\"qual_palette\")[3], linewidth=2)\n",
    "plt.xlim(-0.001,0.3)\n",
    "plt.legend(loc=\"upper right\", labels=[\"Prediction\", \"Polynomial\", \"alpha*\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are quite far from the maximum in the predicted data. At this $\\alpha^*$, we would include more false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with $\\alpha^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_alpha(y_b_predict_proba[:,1],y_d_predict, y_train.TARGET_D.values, target_transformer, alpha = alpha_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the pieces together\n",
    "\n",
    "We can now use the classifier and the regressor together with the Kdd98ProfitEstimator class, which bundles all above steps and makes prediction easier.\n",
    "\n",
    "We predict for the learning data set here, with a maximum obttainable profit of 58395 $\n",
    "\n",
    "Kdd98ProfitEstimator returns both the boolean vector indicating which examples were predicted to donate and the net profit achieved with that prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdd98.prediction import Kdd98ProfitEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = Kdd98ProfitEstimator(classifier, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the profit estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_learning = pe.predict(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of members mailed: {}, net profit: {}\".format(np.sum(prediction_learning[0]), prediction_learning[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_train.TARGET_D.values[y_train.TARGET_D.values > 0.0] - u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the learnig data set, we predict 15218 $, which is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(15218/72375*100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent of the theoretical maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_unseen = pe.predict(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of members mailed: {}, net profit: {}\".format(np.sum(prediction_unseen[0]), prediction_unseen[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.alpha_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test.TARGET_D.values[y_test.TARGET_D.values > 0.0] - u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the predicted donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_predictions = y_test.TARGET_D[prediction_unseen[0]]-0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_predictions.describe()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
