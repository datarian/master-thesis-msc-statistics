{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup logging to file: out.log\n",
      "Figure output directory saved in figure_output at /home/datarian/OneDrive/unine/Master_Thesis/ma-thesis-report/figures\n",
      " cwd: /data/home/datarian/git/master-thesis-msc-statistics/code\n"
     ]
    }
   ],
   "source": [
    "%run ./common_init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, FunctionTransformer, maxabs_scale\n",
    "\n",
    "from kdd98.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.set(\"model_store\", \"/data/home/datarian/OneDrive/unine/Master_Thesis/ma-thesis-report/models\")\n",
    "Config.set(\"data_dir\", \"/data/home/datarian/OneDrive/unine/Master_Thesis/ma-thesis-report/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None):\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=Config.get(\"seq_color_map\"))\n",
    "    if normalize:\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           ylabel='True',\n",
    "           xlabel='Predicted')\n",
    "    if title:\n",
    "        ax.set(title=title)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"black\" if cm[i, j] < thresh else \"white\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_result_store(file):\n",
    "    result_store = pathlib.Path(\n",
    "        Config.get(\"model_store\"), file)\n",
    "    if result_store.is_file():\n",
    "        with open(result_store, \"rb\") as f:\n",
    "            gridsearch_results = pickle.load(f)\n",
    "    else:\n",
    "        gridsearch_results = {m: {\n",
    "                \"best_estimator\": None,\n",
    "                \"best_score\": -1.0,\n",
    "                \"best_params\": None,\n",
    "                \"cv_results\": None\n",
    "            } for m in [\"GBM\", \"RF\", \"GLMnet\", \"NNet\", \"SVM\"]}\n",
    "        with open(result_store, \"wb\") as f:\n",
    "            pickle.dump(gridsearch_results, f)\n",
    "    return gridsearch_results\n",
    "\n",
    "def update_result(model, gridsearch, results_file, ignore_score=False):\n",
    "    if not model in [\"GBM\", \"RF\", \"GLMnet\", \"NNet\", \"SVM\"]:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    gridsearch_results = prepare_result_store(results_file)\n",
    "    \n",
    "    previous_score = gridsearch_results[model][\"best_score\"]\n",
    "    if gridsearch.best_score_ > previous_score or ignore_score:\n",
    "        log(\"Storing result. Score change: {}\".format(gridsearch.best_score_-previous_score))\n",
    "        log(\"Best params: {}\".format(gridsearch.best_params_))\n",
    "        gridsearch_results[model][\"best_estimator\"] = gridsearch.best_estimator_\n",
    "        gridsearch_results[model][\"best_params\"] = gridsearch.best_params_\n",
    "        gridsearch_results[model][\"best_score\"] = gridsearch.best_score_\n",
    "        gridsearch_results[model][\"cv_results\"] = pd.DataFrame(gridsearch.cv_results_)\n",
    "        with open(pathlib.Path(Config.get(\"model_store\"),results_file), \"wb\") as f:\n",
    "            pickle.dump(gridsearch_results, f)\n",
    "    else:\n",
    "        log(\"Best params: {}\".format(gridsearch.best_params_))\n",
    "        log(\"No improvement over previous search for {}. Not storing results.\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_cv_results(refit):\n",
    "    results_file = \"classifiers_refit_{}.pkl\".format(refit)\n",
    "    with open(pathlib.Path(Config.get(\"model_store\"), results_file), \"rb\") as f:\n",
    "        gridsearch_results = pickle.load(f)\n",
    "    for m in gridsearch_results:\n",
    "        if gridsearch_results[m][\"best_estimator\"]:\n",
    "            cv_results = gridsearch_results[m]\n",
    "            print(\"Model {}\".format(m))\n",
    "            print(classification_report_imbalanced(y_val.TARGET_B.values,cv_results[\"best_estimator\"].predict(X_val.values)))\n",
    "            results = cv_results[\"cv_results\"].sort_values(by='mean_test_{}'.format(refit), ascending=False)\n",
    "            print(\"Mean scores\")\n",
    "            print(results[[k for k, v in results.iteritems() if k.startswith(\"mean_test_\")]].round(3))\n",
    "            #print(results[['mean_test_f1', 'mean_test_recall', 'mean_test_auc']].round(3))\n",
    "            if \"best_params\" in gridsearch_results[m].keys():\n",
    "                print(\"Best params\")\n",
    "                print(gridsearch_results[m][\"best_params\"])\n",
    "            print(\"*******************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrices(refit):\n",
    "    results_file = \"classifiers_refit_{}.pkl\".format(refit)\n",
    "    with open(pathlib.Path(Config.get(\"model_store\"), results_file), \"rb\") as f:\n",
    "        gridsearch_results = pickle.load(f)\n",
    "\n",
    "    for m in gridsearch_results:\n",
    "        if gridsearch_results[m][\"best_estimator\"]:\n",
    "            y_predict = gridsearch_results[m][\"best_estimator\"].predict(X_val.values)\n",
    "            gridsearch_results[m][\"y_pred\"] = y_predict\n",
    "            print(\"Confusion matrix for model {}\".format(m))\n",
    "            plot_confusion_matrix(y_val.TARGET_B.values, y_predict, [1,0], title=m)\n",
    "            save_fig(\"confusion_matrix_model_{}_refit_{}\".format(m, refit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_roc_auc_curve(refit, print_title=True):\n",
    "    results_file = \"classifiers_refit_{}.pkl\".format(refit)\n",
    "    with open(pathlib.Path(Config.get(\"model_store\"), results_file), \"rb\") as f:\n",
    "        gridsearch_results = pickle.load(f)\n",
    "\n",
    "    def roc_curve_data(estimator):\n",
    "        try:\n",
    "            y_score = estimator.score(X_val.values)\n",
    "        except Exception:\n",
    "            y_score = estimator.predict_proba(X_val.values)[:,1]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_val.TARGET_B.values, y_score, pos_label=1)\n",
    "        return (fpr, tpr, thresholds)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.xlim(0.0,1.0)\n",
    "    plt.ylim(0.0,1.0)\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=1.5, linestyle='--')\n",
    "\n",
    "    for m in gridsearch_results:\n",
    "        if gridsearch_results[m][\"best_estimator\"]:\n",
    "            fpr, tpr, thresholds = roc_curve_data(gridsearch_results[m][\"best_estimator\"])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=1, label=\"{}, area = {:.2}\".format(m, roc_auc))\n",
    "    plt.legend(loc=\"lower right\", title=\"Metric: {}\".format(refit))\n",
    "    set_size(8,8)\n",
    "    save_fig(\"roc_auc_compared_refit_{}\".format(refit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "\n",
    "def print_precision_recall_curve(refit, print_title=True):\n",
    "    \n",
    "    results_file = \"classifiers_refit_{}.pkl\".format(refit)\n",
    "    with open(pathlib.Path(Config.get(\"model_store\"), results_file), \"rb\") as f:\n",
    "        gridsearch_results = pickle.load(f)\n",
    "        \n",
    "    def p_r_data(estimator):\n",
    "        try:\n",
    "            y_score = estimator.score(X_val.values)\n",
    "        except Exception:\n",
    "            y_score = estimator.predict_proba(X_val.values)[:,1]\n",
    "        p, r, _ = precision_recall_curve(y_val.TARGET_B.values, y_score)\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_val.TARGET_B.values, y_score, pos_label=1)\n",
    "        return (p, r)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "\n",
    "    for m in gridsearch_results:\n",
    "        if gridsearch_results[m][\"best_estimator\"]:\n",
    "            p, r = p_r_data(gridsearch_results[m][\"best_estimator\"])\n",
    "            plt.plot(r, p, lw=1.5, label=\"{}\".format(m))\n",
    "    plt.legend(loc=\"upper right\", title=\"Metric: {}\".format(refit))\n",
    "    set_size(8,8)\n",
    "    save_fig(\"prec_rec_compared_refit_{}\".format(refit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
