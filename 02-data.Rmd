# Data

The data at hand originates from a U.S. veterans organisation and is freely available online ^[See [UCI Machine Learning Repository: KDD Cup 1998 Data](https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data)]. It contains data from the organisation's members and the turnout of a direct mailing addressed to 3.5 million members in the scope of a fundraising effort that was conducted in 1997.

In the following section, the data will be characterized.

...

## General structure

The dataset is provided pre-cut into a learning and validation set. The features are identical between the two except for the target that has been stripped from the validation set.

The dataset is aggregated from several sources.

- Internal database of members
- US census 1990: 286 features
- Promotion history file: 97 features
- Giving history file: 13 features

**Learning dataset**
In the learning dataset, there are 95412 observations and 481 features, of which two are the targets.

**Validation dataset**
The validation dataset contains 96367 observations and 479 features.


## Features
So and so many features, blablabla.

**Datatypes**
An analysis of the dataset dictionary (see \@ref(dataset-dictionary)) reveals the following datatypes for features:

- Index: CONTROLN, unique record identifier
- Dates: 48 features in yymm format.
- Boolean: 30 features
- Categorical: 90 features
- Numeric: 286

## Targets

- TARGET_B: Categorical -> Classification
- TARGET_D: Numeric -> Regression


## Preprocessing

### Boolean features

Boolean features were all recoded to $\{\text{True},\text{False}\}$. Missing values and those that were not mentioned in the dictionary but present in the data were coded as $\text{False}$.

### Categorical features

Several methods exist to transform categorical (string-) values into a usable format. These include:

- One-hot tranformation: Creating dummy variables for each category level.
This approach greatly increases dimensionality, which is both more resource-intensive and prone to overfitting.
- Ordinal encoding: The categorical labels are transformed to ordinal values (integer numbers). This preserves dimensionality, but the algorithm chosen to assign the ordinal levels can introduce unwanted effects (an implied similarity based on closeness of the ordinal variables)
    Feature hashing: The individual values are hashed into a value of fixed length.
- Hashing: [DEFINITION]

### Missing values

Two possible strategies:

    - Exclusion of rows/columnns
    - Imputation
    
## Summary Statistics

    
## Feature Extraction

-> Dimensionality reduction and so on


## Feature Selection
