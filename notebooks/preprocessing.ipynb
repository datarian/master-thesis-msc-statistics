{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This notebook contains all code for the preprocessing of the KDD Cup 98 datasets.\n",
    "* Splits into learning and test\n",
    "* Learns the transformation pipeline on the learning dataset for future use\n",
    "* Prepares the data for model fitting\n",
    "\n",
    "This will be done with scikit-learn's transforming framework in order to ensure all transformations are applied identically on training, test and validation datasets.\n",
    "\n",
    "\n",
    "The transformations are 'learned' on the training dataset and then applied to the test dataset and new data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Change to project root\n",
    "if not os.getcwd()[-4:] == 'code':\n",
    "    os.chdir(\"../code\")\n",
    "    \n",
    "# Load custom code\n",
    "import kdd98.data_loader as dl\n",
    "import kdd98.utils_transformer as ut\n",
    "from kdd98.transformers import *\n",
    "from kdd98.config import App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# seaborn config\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style('ticks')\n",
    "sns.axes_style({'spines.right': False,\n",
    "                'axes.spines.top': False})\n",
    "sns.set_palette(App.config(\"color_palette\"))\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# figures:\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \"../../\"\n",
    "CHAPTER_ID = \"preprocessing\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"figures\", CHAPTER_ID)\n",
    "\n",
    "if not os.path.exists(IMAGES_PATH):\n",
    "    os.makedirs(IMAGES_PATH)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the learning dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set working directory to main code folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = dl.KDD98DataLoader(\"cup98LRN.txt\")\n",
    "learning_raw = data_loader.get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training- and test dataset\n",
    "\n",
    "Before applying *any* transformations, the dataset will be split 80/20 into a learning and test set.\n",
    "\n",
    "Let's look at feature TARGET_B, which describes whether a person has donated or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.949241\n",
       "1    0.050759\n",
       "Name: TARGET_B, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_raw.TARGET_B.value_counts(normalize=True) # 5 % of recipients have donated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to preserve this ratio in the split datasets. scikit-learn provides a method for achieving this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import App\n",
    "seed = App.config(\"random_seed\")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8, random_state=seed)\n",
    "for learn_index, test_index in splitter.split(learning_raw, learning_raw.TARGET_B.astype('int')):\n",
    "    l_i = learn_index\n",
    "    t_i = test_index\n",
    "    kdd_learn = learning_raw.iloc[learn_index]\n",
    "    kdd_test = learning_raw.iloc[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check that the two sets are really disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(kdd_learn.index).intersection(kdd_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the frequencies of the donors in the sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.949246\n",
       "1    0.050754\n",
       "Name: TARGET_B, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_learn['TARGET_B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.949222\n",
       "1    0.050778\n",
       "Name: TARGET_B, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_test['TARGET_B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating features and label\n",
    "\n",
    "First, we separate the features from the labels. We will also remove the label \"TARGET_B\", which is an indicator variable for donors that is no longer of interest\n",
    "\n",
    "**All preprocessing is performed on *kdd_learn_feat***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_learn_feat = kdd_learn.drop(['TARGET_B', 'TARGET_D'],axis=1).copy()\n",
    "kdd_learn_labels = kdd_learn[['TARGET_B','TARGET_D']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning pipeline\n",
    "\n",
    "The cleaning pipeline results in a dataset with numerical (binary features encoded correclty), categorial and string date features.\n",
    "\n",
    "Following this step, feature extraction, imputation, dropping of constant and sparse features and ensuring all data is numerical can be tackled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating cleaning and feature extraction\n",
    "\n",
    "The function below allows for a one-step cleaning and feature extraction, returning a pandas dataframe with all features correctly labelled.\n",
    "\n",
    "As has been seen in EDA, some features are redundant. These will not be processed by the pipeline here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe = Pipeline([\n",
    "    (\"binary_features\",\n",
    "     ColumnTransformer([\n",
    "        (\"binary_x_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'X', 'false': ' '}, correct_noisy=False),\n",
    "         ['PEPSTRFL', 'NOEXCH', 'MAJOR', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP']\n",
    "         ),\n",
    "        (\"binary_y_n\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'Y', 'false': 'N'}, correct_noisy=False),\n",
    "         ['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO',\n",
    "          'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN',  'BOATS', 'WALKER', 'KIDSTUFF',\n",
    "          'CARDS', 'PLATES']\n",
    "         ),\n",
    "        (\"binary_e_i\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"E\", 'false': 'I'}, correct_noisy=False),\n",
    "         ['AGEFLAG']\n",
    "         ),\n",
    "        (\"binary_h_u\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"H\", 'false': 'U'}, correct_noisy=False),\n",
    "         ['HOMEOWNR']),\n",
    "        (\"binary_b_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'B', 'false': ' '}, correct_noisy=False),\n",
    "         ['MAILCODE']\n",
    "         ),\n",
    "        (\"binary_1_0\",\n",
    "         BinaryFeatureRecode(value_map={'true': '1', 'false': '0'}, correct_noisy=False),\n",
    "         ['HPHONE_D']\n",
    "         )\n",
    "        ])\n",
    "    ),\n",
    "    (\"date_features\",\n",
    "     ColumnTransformer([\n",
    "        (\"months_to_donation\", MonthsToDonation(), dl.promo_history_dates+dl.giving_history_dates),\n",
    "         (\"time_last_donation\", DeltaTime(unit='months'), ['LASTDATE','MINRDATE','MAXRDATE','MAXADATE']),\n",
    "        (\"membership_years\", DeltaTime(unit='years'),['ODATEDW'])\n",
    "        ])\n",
    "    ),\n",
    "    (\"osource\",\n",
    "      ColumnTransformer([(\"hash_osource\", HashingEncoder(), ['OSOURCE'])])\n",
    "    ),\n",
    "    (\"tcode\",\n",
    "      ColumnTransformer([(\"hash_tcode\", HashingEncoder(), ['TCODE'])])\n",
    "    ),\n",
    "    (\"zip\",\n",
    "      ColumnTransformer([(\"hash_zip\", HashingEncoder(), ['ZIP'])])\n",
    "    ),\n",
    "    (\"rfa\",\n",
    "      Pipeline([\n",
    "        (\"spread_rfa\", ColumnTransformer([('spread', MultiByteExtract([\"R\", \"F\", \"A\"]), dl.nominal_features[2:])])),\n",
    "        (\"order_multibytes\", OrdinalEncoder(mapping=dl.ordinal_mapping_rfa,handle_unknown='ignore'))\n",
    "      ])\n",
    "    ),\n",
    "    (\"domain\",\n",
    "     Pipeline([\n",
    "         (\"spread_domain\", ColumnTransformer([(\"spread\",MultiByteExtract([\"Urbanicity\", \"SocioEconomic\"]),[\"DOMAIN\"])])),\n",
    "         (\"recode_socioecon\", RecodeUrbanSocioEconomic())\n",
    "     ])\n",
    "    ),\n",
    "    (\"mdmaud\",\n",
    "     ColumnTransformer([\n",
    "         (\"mdmaud\",\n",
    "         OrdinalEncoder(mapping=dl.ordinal_mapping_mdmaud,handle_unknown='ignore'),\n",
    "         ['MDMAUD_R','MDMAUD_A'])\n",
    "     ])\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-dff4257d1bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreproc_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkdd_learn_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             raise ValueError(\"Specifying the columns using strings is only \"\n\u001b[0m\u001b[1;32m    634\u001b[0m                              \"supported for pandas DataFrames\")\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "preproc_pipe.fit(kdd_learn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cleaning_preprocessing(dataset, fit=False):\n",
    "    \n",
    "    # Cleaning stage\n",
    "    \n",
    "    # Binary features\n",
    "    binary_transformers = ColumnTransformer([\n",
    "        (\"binary_x_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'X', 'false': ' '}, correct_noisy=False),\n",
    "         ['PEPSTRFL', 'NOEXCH', 'MAJOR', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP']\n",
    "         ),\n",
    "        (\"binary_y_n\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'Y', 'false': 'N'}, correct_noisy=False),\n",
    "         ['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO',\n",
    "          'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN',  'BOATS', 'WALKER', 'KIDSTUFF',\n",
    "          'CARDS', 'PLATES']\n",
    "         ),\n",
    "        (\"binary_e_i\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"E\", 'false': 'I'}, correct_noisy=False),\n",
    "         ['AGEFLAG']\n",
    "         ),\n",
    "        (\"binary_h_u\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"H\", 'false': 'U'}, correct_noisy=False),\n",
    "         ['HOMEOWNR']),\n",
    "        (\"binary_b_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'B', 'false': ' '}, correct_noisy=False),\n",
    "         ['MAILCODE']\n",
    "         ),\n",
    "        (\"binary_1_0\",\n",
    "         BinaryFeatureRecode(value_map={'true': '1', 'false': '0'}, correct_noisy=False),\n",
    "         ['HPHONE_D']\n",
    "         )\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Dates\n",
    "    don_hist_transformer = ColumnTransformer([\n",
    "        (\"months_to_donation\",\n",
    "         MonthsToDonation(),\n",
    "         dl.promo_history_dates+dl.giving_history_dates\n",
    "         )\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    timedelta_transformer = ColumnTransformer([\n",
    "        (\"time_last_donation\", DeltaTime(unit='months'), ['LASTDATE','MINRDATE','MAXRDATE','MAXADATE']),\n",
    "        (\"membership_years\", DeltaTime(unit='years'),['ODATEDW'])\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Categorical Features\n",
    "    \n",
    "    # Nominals\n",
    "    osource_transformer = ColumnTransformer([\n",
    "        (\"hash_osource\", HashingEncoder(), ['OSOURCE'])\n",
    "    ])\n",
    "    \n",
    "    tcode_transformer = ColumnTransformer([\n",
    "        (\"hash_tcode\", HashingEncoder(), ['TCODE'])\n",
    "    ])\n",
    "    \n",
    "    # Ordinals\n",
    "    multibyte_transformer = ColumnTransformer([\n",
    "        (\"spread\",\n",
    "         MultiByteExtract([\"R\", \"F\", \"A\"]),\n",
    "         dl.nominal_features[2:])\n",
    "    ])\n",
    "    \n",
    "    domain_transformer = Pipeline([\n",
    "        (\"spread\", ColumnTransformer([\n",
    "            (\"spread_domain\",\n",
    "            MultiByteExtract([\"Urbanicity\", \"SocioEconomic\"]),\n",
    "            [\"DOMAIN\"])\n",
    "        ])),\n",
    "        (\"recode_socioecon\", RecodeUrbanSocioEconomic())\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Remaining ordinals\n",
    "    ordinal_transformer = ColumnTransformer([\n",
    "        (\"order_ordinals\",\n",
    "        OrdinalEncoder(mapping=dl.ordinal_mapping_mdmaud,handle_unknown='ignore'),\n",
    "        ['MDMAUD_R','MDMAUD_A']),\n",
    "        (\"order_multibytes\",\n",
    "        OrdinalEncoder(mapping=dl.ordinal_mapping_rfa,handle_unknown='ignore'),\n",
    "         list(dataset.filter(like=\"RFA_\",axis=1).columns))\n",
    "    ])\n",
    "    \n",
    "    # Transforming the data (possibly fitting first) and rebuilding the pandas dataframe\n",
    "    \n",
    "    binarys = binary_transformers.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, binarys, binary_transformers)\n",
    "    donation_responses = don_hist_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, donation_responses, don_hist_transformer)\n",
    "    timedeltas = timedelta_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, timedeltas, timedelta_transformer, drop=dl.date_features)\n",
    "    osource = osource_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, osource, osource_transformer)\n",
    "    tcode = tcode_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, tcode, osource_transformer)\n",
    "    multibytes = multibyte_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, multibytes, multibyte_transformer, drop=dl.nominal_features, new_dtype=\"category\")\n",
    "    domains = domain_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, domains, domain_transformer)\n",
    "    ordinals = ordinal_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, ordinals, ordinal_transformer)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipe = Pipeline([\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/git/master-thesis-code/code/utils/transformer.py\u001b[0m in \u001b[0;36mget_feature_names_from_transformer_collection\u001b[0;34m(collection)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# We have a pure column transformer. Concatenate the feature names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'get_feature_names'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ffbf7376ba48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_cleaning_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkdd_learn_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-a21d238f663c>\u001b[0m in \u001b[0;36mdo_cleaning_preprocessing\u001b[0;34m(dataset, fit)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_df_with_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultibytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultibyte_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnominal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mdomains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomain_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_df_with_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_transformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mordinals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordinal_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_df_with_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordinals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordinal_transformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/master-thesis-code/code/utils/transformer.py\u001b[0m in \u001b[0;36mupdate_df_with_transformed\u001b[0;34m(df_old, new_features, transformer, drop, new_dtype)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_df_with_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mfeat_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_names_from_transformer_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     transformed_df = pd.DataFrame(data=new_features, columns=feat_names,\n\u001b[1;32m     34\u001b[0m                      index=df_old.index)\n",
      "\u001b[0;32m~/git/master-thesis-code/code/utils/transformer.py\u001b[0m in \u001b[0;36mget_feature_names_from_transformer_collection\u001b[0;34m(collection)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# We have at least one pipeline in the collection. So do it manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = do_cleaning_preprocessing(kdd_learn_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to one-hot encode categorical features. This results in an all-numeric dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = pd.get_dummies(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing values\n",
    "\n",
    "https://github.com/epsilon-machine/missingpy\n",
    "\n",
    "Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17 no. 6, 2001 Pages 520-525\n",
    "\n",
    "This step requires that we first drop features with more than 80% missing values for the KNNImputer to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in dataset2.columns if dataset2[c].count() / len(dataset2.index) <= 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.drop([c for c in dataset2.columns if dataset2[c].count() / len(dataset2.index) <= 0.2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set weights to distance so that binary and categorical features get an integer value:\n",
    "https://www.queryxchange.com/q/27_52658127/imputing-missing-values-with-knn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "kdd_learn_feat_imputed = imputer.fit_transform(dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing constant features\n",
    "\n",
    "As per the documentation, features with either low variance or very few non-NA examples are to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in kdd_learn_feat_imputed.columns if kdd_learn_feat_imputed[c].var() <= 1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_variance_cols(df=None, cols=None,\n",
    "                             skip_cols=[], thresh=1e-5,\n",
    "                             autoremove=False):\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn VarianceThreshold for use on pandas dataframes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # get list of all the original df cols\n",
    "        all_cols = df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # remove `skip_cols`\n",
    "        remaining_cols = all_cols.drop(skip_cols)\n",
    "\n",
    "        # get length of new index\n",
    "        max_index = len(remaining_cols) - 1\n",
    "\n",
    "        # get indices for `skip_cols`\n",
    "        skipped_idx = [all_cols.get_loc(column)\n",
    "                       for column\n",
    "                       in skip_cols]\n",
    "\n",
    "        # adjust insert location by the number of cols removed\n",
    "        # (for non-zero insertion locations) to keep relative\n",
    "        # locations intact\n",
    "        for idx, item in enumerate(skipped_idx):\n",
    "            if item > max_index:\n",
    "                diff = item - max_index\n",
    "                skipped_idx[idx] -= diff\n",
    "            if item == max_index:\n",
    "                diff = item - len(skip_cols)\n",
    "                skipped_idx[idx] -= diff\n",
    "            if idx == 0:\n",
    "                skipped_idx[idx] = item\n",
    "\n",
    "        # get values of `skip_cols`\n",
    "        skipped_values = df.iloc[:, skipped_idx].values\n",
    "\n",
    "        # get dataframe values\n",
    "        X = df.loc[:, remaining_cols].values\n",
    "\n",
    "        # instantiate VarianceThreshold object\n",
    "        vt = VarianceThreshold(threshold=thresh)\n",
    "\n",
    "        # fit vt to data\n",
    "        vt.fit(X)\n",
    "\n",
    "        # get the indices of the features that are being kept\n",
    "        feature_indices = vt.get_support(indices=True)\n",
    "\n",
    "        # remove low-variance cols from index\n",
    "        feature_names = [remaining_cols[idx]\n",
    "                         for idx, _\n",
    "                         in enumerate(remaining_cols)\n",
    "                         if idx\n",
    "                         in feature_indices]\n",
    "\n",
    "        # get the cols to be removed\n",
    "        removed_features = list(np.setdiff1d(remaining_cols,\n",
    "                                             feature_names))\n",
    "        print(\"Found {0} low-variance cols.\"\n",
    "              .format(len(removed_features)))\n",
    "\n",
    "        # remove the cols\n",
    "        if autoremove:\n",
    "            print(\"Removing low-variance features.\")\n",
    "            # remove the low-variance cols\n",
    "            X_removed = vt.transform(X)\n",
    "\n",
    "            print(\"Reassembling the dataframe (with low-variance \"\n",
    "                  \"features removed).\")\n",
    "            # re-assemble the dataframe\n",
    "            df = pd.DataFrame(data=X_removed,\n",
    "                                  cols=feature_names)\n",
    "\n",
    "            # add back the `skip_cols`\n",
    "            for idx, index in enumerate(skipped_idx):\n",
    "                df.insert(loc=index,\n",
    "                              column=skip_cols[idx],\n",
    "                              value=skipped_values[:, idx])\n",
    "            print(\"Succesfully removed low-variance cols.\")\n",
    "\n",
    "        # do not remove cols\n",
    "        else:\n",
    "            print(\"No changes have been made to the dataframe.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not remove low-variance features. Something \"\n",
    "              \"went wrong.\")\n",
    "        pass\n",
    "\n",
    "    return df, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, removed = get_low_variance_cols(kdd_learn_feat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring strategies for specific feature types\n",
    "\n",
    "* Noisy data: Correction of data entry / formatting errors\n",
    "    - These errors must be corrected without excluding the records in question\n",
    "* Missing data: Has to be inferred from known values\n",
    "    - (e.g., mean, median, mode, a modeled value).\n",
    "    - One exception to this rule is the attributes containing 99.5 percent or more missings. These are to be dropped\n",
    "* Sparse data: Events actually represented in given data make only a very small subset of the event space are to be dropped\n",
    "* Constant values are to be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant and Sparse Features\n",
    "\n",
    "Features where only one value is present and those where the majority is empty are to be dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_sparse_transformer = DropSparseLowVar(keep_anyways=[\"RAMNT_\\d{1,2}\", \"MONTHS_TO_DONATION_\\d{1,2}\"])\n",
    "cs = const_sparse_transformer.fit(learning)\n",
    "cs = const_sparse_transformer.fit_transform(learning)\n",
    "set(cs.columns)\n",
    "const_sparse_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = learning_raw.select_dtypes(include='object').columns\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in objects:\n",
    "    print(f+\": \"+learning_raw[f].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are two types:\n",
    "\n",
    "* ZIP: Malformed zip codes. Some have a dash at the end, which has to be removed.\n",
    "* Multibyte values. These can be extracted into separate features bytewise. However, this is done in feature extraction later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "It is now time to construct the preprocessing pipeline. A set of transforming operations is concatenated to a sequence of operations. This pipeline is the learned on the learning dataset. All transformations to the learning dataset will then later be applied to the test dataset and to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_feats = list(kdd_learn_feat.select_dtypes(include=np.number).columns)\n",
    "categorical_feats = list(kdd_learn_feat.select_dtypes(include=np.number).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all categories now properly formatted, it is time for one-hot encoding. The sklearn pipeline also has an impute transformation. NaN's get their own level, \"missing\". This step results in a huge increase in the dimension of the feature space. It is also heavy on computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"one_hot\",  OneHotEncoder(impute_missing=True,use_cat_names=True,return_df=True))\n",
    "])\n",
    "\n",
    "categories_transformer = ColumnTransformer([\n",
    "    (\"cat_encoder\",\n",
    "     cat_pipe,\n",
    "     list(kdd_learn_feat.select_dtypes(include=\"category\").columns))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interests and donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = learning_raw.loc[:,dl.interest_features+[\"TARGET_D\"]].fillna(0)\n",
    "interests = pd.melt(data,value_vars=dl.interest_features, value_name=\"Interest\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with constant values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Meant to reduce dimensionality by selecting only features that are 'interesting enough' to be considered in order to boost performance of calculations / improve accuracy of the estimator\n",
    "- By variance threshold\n",
    "- Recursive Feature Elimination by Cross-Validation\n",
    "- L1-based feature selection (Logistic Regression, Lasso, SVM)\n",
    "- Tree-based feature selection\n",
    "\n",
    "See [scikit-learn: feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing constant features (zero variance)\n",
    "\n",
    "sklearn.feature_selection_variance_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in learning.columns:\n",
    "        if len(learning[column].unique()) == 1:\n",
    "            print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = []\n",
    "for column in learning:\n",
    "    top_freq = learning[column].value_counts(normalize=True).iloc[0]\n",
    "    if top_freq > 0.995:\n",
    "        sparse_features.append(column)\n",
    "        print(column+\" has a top frequency of: \" + str(top_freq))\n",
    "        print(learning[column].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced approaches\n",
    "\n",
    "* If overfitting is a problem, ensemble-learning or tree learning can be used to find important features, then apply SelectFromModel before the actual estimator. See http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "All explanatory fields have to be numerical for the subsequent operations with scikit-learn. Here, the necessary feature extractions are performed.\n",
    "\n",
    "See [scikit-learn: feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_features = []\n",
    "symbolic_features.append(tds.SymbolicFeatureSpreader(\n",
    "    \"DOMAIN\", [\"U\", \"S\"])) #Urbanicity, SocioEconomicStatus\n",
    "# RFA_2 is already spread out\n",
    "for i in range(3, 25):\n",
    "    feature = \"_\".join([\"RFA\", str(i)])\n",
    "    symbolic_features.append(tds.SymbolicFeatureSpreader(\n",
    "        feature, [\"R\", \"F\", \"A\"])) # Recency, Frequency, Amount\n",
    "\n",
    "spread_multibyte = pd.DataFrame(index=learning_raw.index)\n",
    "for f in symbolic_features:\n",
    "    f.set_tidy_dataset_ref(learning_raw)\n",
    "    spread_multibyte = pd.concat([spread_multibyte,f.spread(inplace=False)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spread_multibyte.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "A first look at important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = learning.drop([\"TARGET_B\",\"TARGET_D\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 3\n",
    "pca = decomposition.PCA(n_components = n_comp)\n",
    "pca.fit(X)\n",
    "result = pd.DataFrame(pca.transform(X), columns=[\"PCA%i\" % i for i in range(n_comp)], index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "domain_spreader = tds.SymbolicFieldToDummies(learning,\"RFA_24\",[\"Recency\", \"Frequency\", \"Amount\"])\n",
    "cProfile.run('domain_spreader.spread()', sort='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "os.getcwd()\n",
    "proj_dir = os.path.split(os.getcwd())[0]\n",
    "if proj_dir not in sys.path:\n",
    "    sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eda.tidy_dataset as tds\n",
    "tidy = tds.TidyDataset(\"cup98LRN.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = tidy.get_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreader = tds.SymbolicFieldToDummies(\n",
    "    raw, \"RFA_24\", [\"Recency\", \"Frequency\", \"Amount\"])\n",
    "spreader.spread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
