{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-learning-dataset\" data-toc-modified-id=\"Loading-the-learning-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the learning dataset</a></span></li><li><span><a href=\"#Splitting-into-training--and-test-dataset\" data-toc-modified-id=\"Splitting-into-training--and-test-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Splitting into training- and test dataset</a></span></li><li><span><a href=\"#Separating-features-and-label\" data-toc-modified-id=\"Separating-features-and-label-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Separating features and label</a></span></li><li><span><a href=\"#Cleaning-pipeline\" data-toc-modified-id=\"Cleaning-pipeline-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Cleaning pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automating-cleaning-and-feature-extraction\" data-toc-modified-id=\"Automating-cleaning-and-feature-extraction-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Automating cleaning and feature extraction</a></span></li></ul></li><li><span><a href=\"#Imputation-of-missing-values\" data-toc-modified-id=\"Imputation-of-missing-values-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Imputation of missing values</a></span></li><li><span><a href=\"#Removing-constant-features\" data-toc-modified-id=\"Removing-constant-features-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Removing constant features</a></span></li><li><span><a href=\"#Exploring-strategies-for-specific-feature-types\" data-toc-modified-id=\"Exploring-strategies-for-specific-feature-types-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Exploring strategies for specific feature types</a></span><ul class=\"toc-item\"><li><span><a href=\"#Constant-and-Sparse-Features\" data-toc-modified-id=\"Constant-and-Sparse-Features-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Constant and Sparse Features</a></span></li><li><span><a href=\"#Numerical-features\" data-toc-modified-id=\"Numerical-features-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Numerical features</a></span></li><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Categorical features</a></span></li><li><span><a href=\"#Remaining-object-features\" data-toc-modified-id=\"Remaining-object-features-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Remaining object features</a></span></li></ul></li><li><span><a href=\"#Preprocessing-Pipeline\" data-toc-modified-id=\"Preprocessing-Pipeline-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Preprocessing Pipeline</a></span></li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Feature Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Removing-constant-features-(zero-variance)\" data-toc-modified-id=\"Removing-constant-features-(zero-variance)-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Removing constant features (zero variance)</a></span></li><li><span><a href=\"#Sparse-Features\" data-toc-modified-id=\"Sparse-Features-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Sparse Features</a></span></li><li><span><a href=\"#Advanced-approaches\" data-toc-modified-id=\"Advanced-approaches-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Advanced approaches</a></span></li></ul></li><li><span><a href=\"#Feature-Extraction\" data-toc-modified-id=\"Feature-Extraction-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Feature Extraction</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This notebook contains all code for the preprocessing of the KDD Cup 98 datasets.\n",
    "* Splits into learning and test\n",
    "* Learns the transformation pipeline on the learning dataset for future use\n",
    "* Prepares the data for model fitting\n",
    "\n",
    "This will be done with scikit-learn's transforming framework in order to ensure all transformations are applied identically on training, test and validation datasets.\n",
    "\n",
    "\n",
    "The transformations are 'learned' on the training dataset and then applied to the test dataset and new data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup logging to file: out.log\n",
      "Figure output directory saved in figure_output at /home/datarian/OneDrive/unine/Master_Thesis/figures\n"
     ]
    }
   ],
   "source": [
    "%run ./common_init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import HashingEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Load custom code\n",
    "import kdd98.data_loader as dl\n",
    "import kdd98.utils_transformer as ut\n",
    "from kdd98.transformers import *\n",
    "from kdd98.config import App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "IMAGES_PATH = pathlib.Path(figure_output/'preprocessing')\n",
    "\n",
    "pathlib.Path(IMAGES_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = pathlib.Path(IMAGES_PATH/fig_id + \".\" + fig_extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the learning dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set working directory to main code folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = dl.KDD98DataLoader(\"cup98LRN.txt\")\n",
    "learning_raw = data_loader.get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first, general look at the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95412 entries, 95515 to 185114\n",
      "Columns: 480 entries, ODATEDW to GEOCODE2\n",
      "dtypes: category(25), datetime64[ns](53), float64(48), int64(298), object(56)\n",
      "memory usage: 334.2+ MB\n"
     ]
    }
   ],
   "source": [
    "learning_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 481 features (of which one is the index)\n",
    "* A total of 95412 examples\n",
    "* 24 categorical features, 53 datetime features, 48 numerical features with missing values, 297 integer features without missing values and 56 string features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ODATEDW</th>\n",
       "      <th>OSOURCE</th>\n",
       "      <th>TCODE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>MAILCODE</th>\n",
       "      <th>PVASTATE</th>\n",
       "      <th>DOB</th>\n",
       "      <th>NOEXCH</th>\n",
       "      <th>RECINHSE</th>\n",
       "      <th>...</th>\n",
       "      <th>TARGET_D</th>\n",
       "      <th>HPHONE_D</th>\n",
       "      <th>RFA_2R</th>\n",
       "      <th>RFA_2F</th>\n",
       "      <th>RFA_2A</th>\n",
       "      <th>MDMAUD_R</th>\n",
       "      <th>MDMAUD_F</th>\n",
       "      <th>MDMAUD_A</th>\n",
       "      <th>CLUSTER2</th>\n",
       "      <th>GEOCODE2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONTROLN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95515</th>\n",
       "      <td>1989-01-01</td>\n",
       "      <td>GRI</td>\n",
       "      <td>0</td>\n",
       "      <td>IL</td>\n",
       "      <td>61081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1937-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148535</th>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>BOA</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>91326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1952-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>L</td>\n",
       "      <td>2</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15078</th>\n",
       "      <td>1990-01-01</td>\n",
       "      <td>AMH</td>\n",
       "      <td>1</td>\n",
       "      <td>NC</td>\n",
       "      <td>27017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172556</th>\n",
       "      <td>1987-01-01</td>\n",
       "      <td>BRY</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>95953</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1928-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7112</th>\n",
       "      <td>1986-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FL</td>\n",
       "      <td>33176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1920-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 480 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ODATEDW OSOURCE TCODE STATE    ZIP MAILCODE PVASTATE        DOB  \\\n",
       "CONTROLN                                                                      \n",
       "95515    1989-01-01     GRI     0    IL  61081      NaN      NaN 1937-12-01   \n",
       "148535   1994-01-01     BOA     1    CA  91326      NaN      NaN 1952-02-01   \n",
       "15078    1990-01-01     AMH     1    NC  27017      NaN      NaN        NaT   \n",
       "172556   1987-01-01     BRY     0    CA  95953      NaN      NaN 1928-01-01   \n",
       "7112     1986-01-01     NaN     0    FL  33176      NaN      NaN 1920-01-01   \n",
       "\n",
       "         NOEXCH RECINHSE   ...    TARGET_D HPHONE_D RFA_2R RFA_2F RFA_2A  \\\n",
       "CONTROLN                   ...                                             \n",
       "95515         0      NaN   ...           0        0      L      4      E   \n",
       "148535        0      NaN   ...           0        0      L      2      G   \n",
       "15078         0      NaN   ...           0        1      L      4      E   \n",
       "172556        0      NaN   ...           0        1      L      4      E   \n",
       "7112          0        X   ...           0        1      L      2      F   \n",
       "\n",
       "         MDMAUD_R  MDMAUD_F MDMAUD_A CLUSTER2 GEOCODE2  \n",
       "CONTROLN                                                \n",
       "95515         NaN       NaN      NaN     39.0        C  \n",
       "148535        NaN       NaN      NaN      1.0        A  \n",
       "15078         NaN       NaN      NaN     60.0        C  \n",
       "172556        NaN       NaN      NaN     41.0        C  \n",
       "7112          NaN       NaN      NaN     26.0        A  \n",
       "\n",
       "[5 rows x 480 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 346 numerical features\n"
     ]
    }
   ],
   "source": [
    "numerical = learning_raw.select_dtypes(include=np.number).columns\n",
    "print(\"There are {:1} numerical features\".format(len(numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "Categories were defined on import of the csv data. The categories were identified in the dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['STATE', 'PVASTATE', 'DOMAIN', 'CLUSTER', 'CHILD03', 'CHILD07',\n",
      "       'CHILD12', 'CHILD18', 'INCOME', 'GENDER', 'WEALTH1', 'DATASRCE',\n",
      "       'SOLP3', 'SOLIH', 'WEALTH2', 'GEOCODE', 'LIFESRC', 'TARGET_D', 'RFA_2R',\n",
      "       'RFA_2F', 'RFA_2A', 'MDMAUD_R', 'MDMAUD_F', 'MDMAUD_A', 'GEOCODE2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "categories = learning_raw.select_dtypes(include='category').columns\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STATE</th>\n",
       "      <td>95412</td>\n",
       "      <td>57</td>\n",
       "      <td>CA</td>\n",
       "      <td>17343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PVASTATE</th>\n",
       "      <td>1458</td>\n",
       "      <td>2</td>\n",
       "      <td>P</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOMAIN</th>\n",
       "      <td>93096</td>\n",
       "      <td>16</td>\n",
       "      <td>R2</td>\n",
       "      <td>13623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLUSTER</th>\n",
       "      <td>93096</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>3979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHILD03</th>\n",
       "      <td>1146</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHILD07</th>\n",
       "      <td>1566</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>1061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHILD12</th>\n",
       "      <td>1811</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHILD18</th>\n",
       "      <td>2847</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>1442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCOME</th>\n",
       "      <td>74126</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>15451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GENDER</th>\n",
       "      <td>92455</td>\n",
       "      <td>6</td>\n",
       "      <td>F</td>\n",
       "      <td>51277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEALTH1</th>\n",
       "      <td>50680</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATASRCE</th>\n",
       "      <td>74132</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>43549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLP3</th>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "      <td>00</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLIH</th>\n",
       "      <td>6200</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEALTH2</th>\n",
       "      <td>51589</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOCODE</th>\n",
       "      <td>15244</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>3914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIFESRC</th>\n",
       "      <td>41380</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TARGET_D</th>\n",
       "      <td>95412</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>90569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFA_2R</th>\n",
       "      <td>95412</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>95412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFA_2F</th>\n",
       "      <td>95412</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>47675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFA_2A</th>\n",
       "      <td>95412</td>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>46964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MDMAUD_R</th>\n",
       "      <td>294</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MDMAUD_F</th>\n",
       "      <td>294</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MDMAUD_A</th>\n",
       "      <td>294</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEOCODE2</th>\n",
       "      <td>95093</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>34484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count unique top   freq\n",
       "STATE     95412     57  CA  17343\n",
       "PVASTATE   1458      2   P   1453\n",
       "DOMAIN    93096     16  R2  13623\n",
       "CLUSTER   93096     53  40   3979\n",
       "CHILD03    1146      3   M    869\n",
       "CHILD07    1566      3   M   1061\n",
       "CHILD12    1811      3   M   1149\n",
       "CHILD18    2847      3   M   1442\n",
       "INCOME    74126      7   5  15451\n",
       "GENDER    92455      6   F  51277\n",
       "WEALTH1   50680     10   9   7585\n",
       "DATASRCE  74132      3   3  43549\n",
       "SOLP3       180      4  00     80\n",
       "SOLIH      6200      7  12   5693\n",
       "WEALTH2   51589     10   9   6523\n",
       "GEOCODE   15244      7  12   3914\n",
       "LIFESRC   41380      3   2  20027\n",
       "TARGET_D  95412     71   0  90569\n",
       "RFA_2R    95412      1   L  95412\n",
       "RFA_2F    95412      4   1  47675\n",
       "RFA_2A    95412      4   F  46964\n",
       "MDMAUD_R    294      4   C    109\n",
       "MDMAUD_F    294      3   1    187\n",
       "MDMAUD_A    294      4   C    249\n",
       "GEOCODE2  95093      4   A  34484"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_raw[categories].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MAILCODE', 'NOEXCH', 'RECSWEEP', 'RECINHSE', 'RECP3', 'RECPGVG', 'AGEFLAG', 'HOMEOWNR', 'MAJOR', 'COLLECT1', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO', 'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN', 'BOATS', 'WALKER', 'KIDSTUFF', 'CARDS', 'PLATES', 'PEPSTRFL', 'TARGET_B', 'HPHONE_D', 'VETERANS']\n"
     ]
    }
   ],
   "source": [
    "print(dl.binary_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several binary features have only one value set. These are features where a blank represents false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_raw[dl.binary_features].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Features\n",
    "\n",
    "These features have mixed datatypes and are encoded as strings. This hints at noisy data and features that will have to be transformed before becoming usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = learning_raw.select_dtypes(include='object').columns\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw[objects].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features\n",
    "Dates are parsed into datetime64 by pandas on reading the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = learning_raw[dl.date_features]\n",
    "dates.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training- and test dataset\n",
    "\n",
    "Before applying *any* transformations, the dataset will be split 80/20 into a learning and test set.\n",
    "\n",
    "Let's look at feature TARGET_B, which describes whether a person has donated or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.949241\n",
       "1    0.050759\n",
       "Name: TARGET_B, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_raw.TARGET_B.value_counts(normalize=True) # 5 % of recipients have donated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to preserve this ratio in the split datasets. scikit-learn provides a method for achieving this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = App.config(\"random_seed\")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8, random_state=seed)\n",
    "for learn_index, test_index in splitter.split(learning_raw, learning_raw.TARGET_B.astype('int')):\n",
    "    l_i = learn_index\n",
    "    t_i = test_index\n",
    "    kdd_learn = learning_raw.iloc[learn_index]\n",
    "    kdd_test = learning_raw.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check that the two sets are really disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(kdd_learn.index).intersection(kdd_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the frequencies of the donors in the sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.949246\n",
       "1    0.050754\n",
       "Name: TARGET_B, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_learn['TARGET_B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.949222\n",
       "1    0.050778\n",
       "Name: TARGET_B, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdd_test['TARGET_B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating features and label\n",
    "\n",
    "First, we separate the features from the labels. We will also remove the label \"TARGET_B\", which is an indicator variable for donors that is no longer of interest\n",
    "\n",
    "**All preprocessing is performed on *kdd_learn_feat***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_learn_feat = kdd_learn.drop(['TARGET_B', 'TARGET_D'],axis=1).copy()\n",
    "kdd_learn_labels = kdd_learn[['TARGET_B','TARGET_D']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline\n",
    "\n",
    "The preprocessing pipeline results in a dataset with numerical (binary features encoded correclty), categorial and string date features.\n",
    "\n",
    "Following this step, feature extraction, imputation, dropping of constant and sparse features and ensuring all data is numerical can be tackled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating cleaning and feature extraction\n",
    "\n",
    "The function below allows for a one-step cleaning and feature extraction, returning a pandas dataframe with all features correctly labelled.\n",
    "\n",
    "As has been seen in EDA, some features are redundant. These will not be processed by the pipeline here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/bigdatarepublic/integrating-pandas-and-scikit-learn-with-pipelines-f70eb6183696\n",
    "https://ramhiser.com/post/2018-04-16-building-scikit-learn-pipeline-with-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe = FeatureUnion(n_jobs=1, transformer_list= [\n",
    "    (\"binary_features\",\n",
    "     # Binary features need recoding. All of them will be True / False afterwards, encoded as 1 / 0\n",
    "     ColumnTransformer([\n",
    "        (\"binary_x_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'X', 'false': ' '}, correct_noisy=False),\n",
    "         ['PEPSTRFL', 'NOEXCH', 'MAJOR', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP']\n",
    "         ),\n",
    "        (\"binary_y_n\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'Y', 'false': 'N'}, correct_noisy=False),\n",
    "         ['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO',\n",
    "          'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN',  'BOATS', 'WALKER', 'KIDSTUFF',\n",
    "          'CARDS', 'PLATES']\n",
    "         ),\n",
    "        (\"binary_e_i\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"E\", 'false': 'I'}, correct_noisy=False),\n",
    "         ['AGEFLAG']\n",
    "         ),\n",
    "        (\"binary_h_u\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"H\", 'false': 'U'}, correct_noisy=False),\n",
    "         ['HOMEOWNR']),\n",
    "        (\"binary_b_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'B', 'false': ' '}, correct_noisy=False),\n",
    "         ['MAILCODE']\n",
    "         ),\n",
    "        (\"binary_1_0\",\n",
    "         BinaryFeatureRecode(value_map={'true': '1', 'false': '0'}, correct_noisy=False),\n",
    "         ['HPHONE_D']\n",
    "         )\n",
    "        ])\n",
    "    ),\n",
    "    (\"date_features\",\n",
    "     # Date features are converted to time deltas.\n",
    "     ColumnTransformer([\n",
    "        (\"months_to_donation\", MonthsToDonation(), dl.promo_history_dates+dl.giving_history_dates),\n",
    "         (\"time_last_donation\", DeltaTime(unit='months'), ['LASTDATE','MINRDATE','MAXRDATE','MAXADATE']),\n",
    "        (\"membership_years\", DeltaTime(unit='years'),['ODATEDW'])\n",
    "        ])\n",
    "    ),\n",
    "    (\"osource\",\n",
    "      ColumnTransformer([(\"hash_osource\", HashingEncoder(), ['OSOURCE'])])\n",
    "    ),\n",
    "    (\"tcode\",\n",
    "      ColumnTransformer([(\"hash_tcode\", HashingEncoder(), ['TCODE'])])\n",
    "    ),\n",
    "    (\"zip\",\n",
    "      ColumnTransformer([(\"hash_zip\", HashingEncoder(), ['ZIP'])])\n",
    "    ),\n",
    "    (\"rfa\",\n",
    "      Pipeline([\n",
    "        # Recency / Frequency / Amount featrues are spread out into individual features, then ordinally encoded\n",
    "        (\"spread_rfa\", ColumnTransformer([('spread', MultiByteExtract([\"R\", \"F\", \"A\"]), dl.nominal_features[2:])])),\n",
    "        (\"order_multibytes\", OrdinalEncoder(mapping=dl.ordinal_mapping_rfa,handle_unknown='ignore'))\n",
    "      ])\n",
    "    ),\n",
    "    (\"domain\",\n",
    "     Pipeline([\n",
    "         # The domain feature holds a code for urbanicity and socio economic status of an area. It is split into two\n",
    "         # and then the socio economic status is recoded to an ordinal feature\n",
    "         (\"spread_domain\", ColumnTransformer([(\"spread\",MultiByteExtract([\"Urbanicity\", \"SocioEconomic\"]),[\"DOMAIN\"])])),\n",
    "         (\"recode_socioecon\", RecodeUrbanSocioEconomic())\n",
    "     ])\n",
    "    ),\n",
    "    (\"mdmaud\",\n",
    "     ColumnTransformer([\n",
    "         (\"mdmaud\",\n",
    "         OrdinalEncoder(mapping=dl.ordinal_mapping_mdmaud,handle_unknown='ignore'),\n",
    "         ['MDMAUD_R','MDMAUD_A'])\n",
    "     ]),\n",
    "     remainder = 'passthrough'\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RFA_3A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RFA_3A'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-dff4257d1bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreproc_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkdd_learn_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    766\u001b[0m         transformers = Parallel(n_jobs=self.n_jobs)(\n\u001b[1;32m    767\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fit_one_transformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             for _, trans, _ in self._iter())\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_transformer_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_one_transformer\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;31m#  factorize the code in ColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_one_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/category_encoders-1.3.0-py3.6.egg/category_encoders/ordinal.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mhandle_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         )\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/category_encoders-1.3.0-py3.6.egg/category_encoders/ordinal.py\u001b[0m in \u001b[0;36mordinal_encoding\u001b[0;34m(X_in, mapping, cols, handle_unknown, handle_missing)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mswitch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'col'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswitch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mapping'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RFA_3A'"
     ]
    }
   ],
   "source": [
    "preproc_pipe.fit(kdd_learn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = preproc_pipe.transform(kdd_learn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe.fit(kdd_learn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = preproc_pipe.transform(kdd_learn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cleaning_preprocessing(dataset, fit=False):\n",
    "    \n",
    "    # Cleaning stage\n",
    "    \n",
    "    # Binary features\n",
    "    binary_transformers = ColumnTransformer([\n",
    "        (\"binary_x_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'X', 'false': ' '}, correct_noisy=False),\n",
    "         ['PEPSTRFL', 'NOEXCH', 'MAJOR', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP']\n",
    "         ),\n",
    "        (\"binary_y_n\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'Y', 'false': 'N'}, correct_noisy=False),\n",
    "         ['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO',\n",
    "          'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN',  'BOATS', 'WALKER', 'KIDSTUFF',\n",
    "          'CARDS', 'PLATES']\n",
    "         ),\n",
    "        (\"binary_e_i\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"E\", 'false': 'I'}, correct_noisy=False),\n",
    "         ['AGEFLAG']\n",
    "         ),\n",
    "        (\"binary_h_u\",\n",
    "         BinaryFeatureRecode(value_map={'true': \"H\", 'false': 'U'}, correct_noisy=False),\n",
    "         ['HOMEOWNR']),\n",
    "        (\"binary_b_bl\",\n",
    "         BinaryFeatureRecode(value_map={'true': 'B', 'false': ' '}, correct_noisy=False),\n",
    "         ['MAILCODE']\n",
    "         ),\n",
    "        (\"binary_1_0\",\n",
    "         BinaryFeatureRecode(value_map={'true': '1', 'false': '0'}, correct_noisy=False),\n",
    "         ['HPHONE_D']\n",
    "         )\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Dates\n",
    "    don_hist_transformer = ColumnTransformer([\n",
    "        (\"months_to_donation\",\n",
    "         MonthsToDonation(),\n",
    "         dl.promo_history_dates+dl.giving_history_dates\n",
    "         )\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    timedelta_transformer = ColumnTransformer([\n",
    "        (\"time_last_donation\", DeltaTime(unit='months'), ['LASTDATE','MINRDATE','MAXRDATE','MAXADATE']),\n",
    "        (\"membership_years\", DeltaTime(unit='years'),['ODATEDW'])\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Categorical Features\n",
    "    \n",
    "    # Nominals\n",
    "    osource_transformer = ColumnTransformer([\n",
    "        (\"hash_osource\", HashingEncoder(), ['OSOURCE'])\n",
    "    ])\n",
    "    \n",
    "    tcode_transformer = ColumnTransformer([\n",
    "        (\"hash_tcode\", HashingEncoder(), ['TCODE'])\n",
    "    ])\n",
    "    \n",
    "    # Ordinals\n",
    "    multibyte_transformer = ColumnTransformer([\n",
    "        (\"spread\",\n",
    "         MultiByteExtract([\"R\", \"F\", \"A\"]),\n",
    "         dl.nominal_features[2:])\n",
    "    ])\n",
    "    \n",
    "    domain_transformer = Pipeline([\n",
    "        (\"spread\", ColumnTransformer([\n",
    "            (\"spread_domain\",\n",
    "            MultiByteExtract([\"Urbanicity\", \"SocioEconomic\"]),\n",
    "            [\"DOMAIN\"])\n",
    "        ])),\n",
    "        (\"recode_socioecon\", RecodeUrbanSocioEconomic())\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Remaining ordinals\n",
    "    ordinal_transformer = ColumnTransformer([\n",
    "        (\"order_ordinals\",\n",
    "        OrdinalEncoder(mapping=dl.ordinal_mapping_mdmaud,handle_unknown='ignore'),\n",
    "        ['MDMAUD_R','MDMAUD_A']),\n",
    "        (\"order_multibytes\",\n",
    "        OrdinalEncoder(mapping=dl.ordinal_mapping_rfa,handle_unknown='ignore'),\n",
    "         list(dataset.filter(like=\"RFA_\",axis=1).columns))\n",
    "    ])\n",
    "    \n",
    "    # Transforming the data (possibly fitting first) and rebuilding the pandas dataframe\n",
    "    \n",
    "    binarys = binary_transformers.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, binarys, binary_transformers)\n",
    "    donation_responses = don_hist_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, donation_responses, don_hist_transformer)\n",
    "    timedeltas = timedelta_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, timedeltas, timedelta_transformer, drop=dl.date_features)\n",
    "    osource = osource_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, osource, osource_transformer)\n",
    "    tcode = tcode_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, tcode, osource_transformer)\n",
    "    multibytes = multibyte_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, multibytes, multibyte_transformer, drop=dl.nominal_features, new_dtype=\"category\")\n",
    "    domains = domain_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, domains, domain_transformer)\n",
    "    ordinals = ordinal_transformer.fit_transform(dataset)\n",
    "    dataset = ut.update_df_with_transformed(dataset, ordinals, ordinal_transformer)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = do_cleaning_preprocessing(kdd_learn_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to one-hot encode categorical features. This results in an all-numeric dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = pd.get_dummies(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing values\n",
    "\n",
    "https://github.com/epsilon-machine/missingpy\n",
    "\n",
    "Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17 no. 6, 2001 Pages 520-525\n",
    "\n",
    "This step requires that we first drop features with more than 80% missing values for the KNNImputer to work.\n",
    "\n",
    "Best results with k=3: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959387/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in dataset2.columns if dataset2[c].count() / len(dataset2.index) <= 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.drop([c for c in dataset2.columns if dataset2[c].count() / len(dataset2.index) <= 0.2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set weights to distance so that binary and categorical features get an integer value:\n",
    "https://www.queryxchange.com/q/27_52658127/imputing-missing-values-with-knn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
    "kdd_learn_feat_imputed = imputer.fit_transform(dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing constant features\n",
    "\n",
    "As per the documentation, features with either low variance or very few non-NA examples are to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in kdd_learn_feat_imputed.columns if kdd_learn_feat_imputed[c].var() <= 1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_variance_cols(df=None, cols=None,\n",
    "                             skip_cols=[], thresh=1e-5,\n",
    "                             autoremove=False):\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn VarianceThreshold for use on pandas dataframes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # get list of all the original df cols\n",
    "        all_cols = df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # remove `skip_cols`\n",
    "        remaining_cols = all_cols.drop(skip_cols)\n",
    "\n",
    "        # get length of new index\n",
    "        max_index = len(remaining_cols) - 1\n",
    "\n",
    "        # get indices for `skip_cols`\n",
    "        skipped_idx = [all_cols.get_loc(column)\n",
    "                       for column\n",
    "                       in skip_cols]\n",
    "\n",
    "        # adjust insert location by the number of cols removed\n",
    "        # (for non-zero insertion locations) to keep relative\n",
    "        # locations intact\n",
    "        for idx, item in enumerate(skipped_idx):\n",
    "            if item > max_index:\n",
    "                diff = item - max_index\n",
    "                skipped_idx[idx] -= diff\n",
    "            if item == max_index:\n",
    "                diff = item - len(skip_cols)\n",
    "                skipped_idx[idx] -= diff\n",
    "            if idx == 0:\n",
    "                skipped_idx[idx] = item\n",
    "\n",
    "        # get values of `skip_cols`\n",
    "        skipped_values = df.iloc[:, skipped_idx].values\n",
    "\n",
    "        # get dataframe values\n",
    "        X = df.loc[:, remaining_cols].values\n",
    "\n",
    "        # instantiate VarianceThreshold object\n",
    "        vt = VarianceThreshold(threshold=thresh)\n",
    "\n",
    "        # fit vt to data\n",
    "        vt.fit(X)\n",
    "\n",
    "        # get the indices of the features that are being kept\n",
    "        feature_indices = vt.get_support(indices=True)\n",
    "\n",
    "        # remove low-variance cols from index\n",
    "        feature_names = [remaining_cols[idx]\n",
    "                         for idx, _\n",
    "                         in enumerate(remaining_cols)\n",
    "                         if idx\n",
    "                         in feature_indices]\n",
    "\n",
    "        # get the cols to be removed\n",
    "        removed_features = list(np.setdiff1d(remaining_cols,\n",
    "                                             feature_names))\n",
    "        print(\"Found {0} low-variance cols.\"\n",
    "              .format(len(removed_features)))\n",
    "\n",
    "        # remove the cols\n",
    "        if autoremove:\n",
    "            print(\"Removing low-variance features.\")\n",
    "            # remove the low-variance cols\n",
    "            X_removed = vt.transform(X)\n",
    "\n",
    "            print(\"Reassembling the dataframe (with low-variance \"\n",
    "                  \"features removed).\")\n",
    "            # re-assemble the dataframe\n",
    "            df = pd.DataFrame(data=X_removed,\n",
    "                                  cols=feature_names)\n",
    "\n",
    "            # add back the `skip_cols`\n",
    "            for idx, index in enumerate(skipped_idx):\n",
    "                df.insert(loc=index,\n",
    "                              column=skip_cols[idx],\n",
    "                              value=skipped_values[:, idx])\n",
    "            print(\"Succesfully removed low-variance cols.\")\n",
    "\n",
    "        # do not remove cols\n",
    "        else:\n",
    "            print(\"No changes have been made to the dataframe.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not remove low-variance features. Something \"\n",
    "              \"went wrong.\")\n",
    "        pass\n",
    "\n",
    "    return df, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, removed = get_low_variance_cols(kdd_learn_feat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring strategies for specific feature types\n",
    "\n",
    "* Noisy data: Correction of data entry / formatting errors\n",
    "    - These errors must be corrected without excluding the records in question\n",
    "* Missing data: Has to be inferred from known values\n",
    "    - (e.g., mean, median, mode, a modeled value).\n",
    "    - One exception to this rule is the attributes containing 99.5 percent or more missings. These are to be dropped\n",
    "* Sparse data: Events actually represented in given data make only a very small subset of the event space are to be dropped\n",
    "* Constant values are to be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant and Sparse Features\n",
    "\n",
    "Features where only one value is present and those where the majority is empty are to be dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_sparse_transformer = DropSparseLowVar(keep_anyways=[\"RAMNT_\\d{1,2}\", \"MONTHS_TO_DONATION_\\d{1,2}\"])\n",
    "cs = const_sparse_transformer.fit(learning)\n",
    "cs = const_sparse_transformer.fit_transform(learning)\n",
    "set(cs.columns)\n",
    "const_sparse_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = learning_raw.select_dtypes(include='object').columns\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in objects:\n",
    "    print(f+\": \"+learning_raw[f].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are two types:\n",
    "\n",
    "* ZIP: Malformed zip codes. Some have a dash at the end, which has to be removed.\n",
    "* Multibyte values. These can be extracted into separate features bytewise. However, this is done in feature extraction later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "It is now time to construct the preprocessing pipeline. A set of transforming operations is concatenated to a sequence of operations. This pipeline is the learned on the learning dataset. All transformations to the learning dataset will then later be applied to the test dataset and to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_feats = list(kdd_learn_feat.select_dtypes(include=np.number).columns)\n",
    "categorical_feats = list(kdd_learn_feat.select_dtypes(include=np.number).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all categories now properly formatted, it is time for one-hot encoding. The sklearn pipeline also has an impute transformation. NaN's get their own level, \"missing\". This step results in a huge increase in the dimension of the feature space. It is also heavy on computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"one_hot\",  OneHotEncoder(impute_missing=True,use_cat_names=True,return_df=True))\n",
    "])\n",
    "\n",
    "categories_transformer = ColumnTransformer([\n",
    "    (\"cat_encoder\",\n",
    "     cat_pipe,\n",
    "     list(kdd_learn_feat.select_dtypes(include=\"category\").columns))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interests and donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = learning_raw.loc[:,dl.interest_features+[\"TARGET_D\"]].fillna(0)\n",
    "interests = pd.melt(data,value_vars=dl.interest_features, value_name=\"Interest\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with constant values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Meant to reduce dimensionality by selecting only features that are 'interesting enough' to be considered in order to boost performance of calculations / improve accuracy of the estimator\n",
    "- By variance threshold\n",
    "- Recursive Feature Elimination by Cross-Validation\n",
    "- L1-based feature selection (Logistic Regression, Lasso, SVM)\n",
    "- Tree-based feature selection\n",
    "\n",
    "See [scikit-learn: feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing constant features (zero variance)\n",
    "\n",
    "sklearn.feature_selection_variance_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in learning.columns:\n",
    "        if len(learning[column].unique()) == 1:\n",
    "            print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = []\n",
    "for column in learning:\n",
    "    top_freq = learning[column].value_counts(normalize=True).iloc[0]\n",
    "    if top_freq > 0.995:\n",
    "        sparse_features.append(column)\n",
    "        print(column+\" has a top frequency of: \" + str(top_freq))\n",
    "        print(learning[column].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced approaches\n",
    "\n",
    "* If overfitting is a problem, ensemble-learning or tree learning can be used to find important features, then apply SelectFromModel before the actual estimator. See http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "All explanatory fields have to be numerical for the subsequent operations with scikit-learn. Here, the necessary feature extractions are performed.\n",
    "\n",
    "See [scikit-learn: feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_features = []\n",
    "symbolic_features.append(tds.SymbolicFeatureSpreader(\n",
    "    \"DOMAIN\", [\"U\", \"S\"])) #Urbanicity, SocioEconomicStatus\n",
    "# RFA_2 is already spread out\n",
    "for i in range(3, 25):\n",
    "    feature = \"_\".join([\"RFA\", str(i)])\n",
    "    symbolic_features.append(tds.SymbolicFeatureSpreader(\n",
    "        feature, [\"R\", \"F\", \"A\"])) # Recency, Frequency, Amount\n",
    "\n",
    "spread_multibyte = pd.DataFrame(index=learning_raw.index)\n",
    "for f in symbolic_features:\n",
    "    f.set_tidy_dataset_ref(learning_raw)\n",
    "    spread_multibyte = pd.concat([spread_multibyte,f.spread(inplace=False)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spread_multibyte.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "A first look at important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = learning.drop([\"TARGET_B\",\"TARGET_D\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 3\n",
    "pca = decomposition.PCA(n_components = n_comp)\n",
    "pca.fit(X)\n",
    "result = pd.DataFrame(pca.transform(X), columns=[\"PCA%i\" % i for i in range(n_comp)], index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "domain_spreader = tds.SymbolicFieldToDummies(learning,\"RFA_24\",[\"Recency\", \"Frequency\", \"Amount\"])\n",
    "cProfile.run('domain_spreader.spread()', sort='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "os.getcwd()\n",
    "proj_dir = os.path.split(os.getcwd())[0]\n",
    "if proj_dir not in sys.path:\n",
    "    sys.path.append(proj_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eda.tidy_dataset as tds\n",
    "tidy = tds.TidyDataset(\"cup98LRN.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = tidy.get_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreader = tds.SymbolicFieldToDummies(\n",
    "    raw, \"RFA_24\", [\"Recency\", \"Frequency\", \"Amount\"])\n",
    "spreader.spread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
