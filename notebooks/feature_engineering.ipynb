{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./common_init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import HashingEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Load custom code\n",
    "import kdd98.data_handler as dh\n",
    "import kdd98.utils_transformer as ut\n",
    "from kdd98.transformers import *\n",
    "from kdd98.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "IMAGES_PATH = pathlib.Path(figure_output/'preprocessing')\n",
    "\n",
    "pathlib.Path(IMAGES_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = pathlib.Path(IMAGES_PATH/fig_id + \".\" + fig_extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates\n",
    "\n",
    "There are several date features. ODATEDW is the date the record was added, DOB the birth date. ADATE_* and RDATE_* are from the promotion history. ADATE_* is the date of a mailing, RDATE_* the date the donation for the corresponding mailing was received. While these dates are not of particular interest (very low variance), the time it took to respond might be.\n",
    "Furthermore, there are the features MINRDATE, MAXRDATE, MAXADATE, FISTDATE, NEXTDATE and LASTDATE coming from the giving history file.\n",
    "\n",
    "Three different transformations are applied:\n",
    "\n",
    "1. ODATEDW, DOB: Years before 1997 -> membership duration, age\n",
    "2. Giving history features: Relative time in months to 1997/06/01\n",
    "3. For the promotion history, as specified above, the time for response in months\n",
    "\n",
    "There are redundant features which can be safely removed, as is shown below:\n",
    "\n",
    "1. FISTDATE and NEXTDATE are contained in TIMELAG, the number of months between first and second donation\n",
    "2. DOB, the date of birth, is contained in the feature AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dh.date_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we transform the dates from the giving history. First, we create two dataframes with the sending dates of the mailings and the dates when the gift (donation) for these was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_hist_transformer = ColumnTransformer([\n",
    "    (\"months_to_donation\",\n",
    "     MonthsToDonation(),\n",
    "     dh.PROMO_HISTORY_DATES+dh.GIVING_HISTORY_DATES\n",
    "     )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donation_responses = don_hist_transformer.fit_transform(learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_hist_feature_names = [n[n.find('__')+2:]\n",
    "                 for n in don_hist_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donation_responses = pd.DataFrame(\n",
    "    donation_responses, index=learning.index, columns=don_hist_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = learning.merge(donation_responses, on=learning.index.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time delta computation of the remaining features with either a specific reference or the date of the most recent mailing as a reference:\n",
    "\n",
    "* Time since last donation, minimum- and maximum donation and receiving most recent promotion\n",
    "* Delta between first and next donation\n",
    "* Age, years of membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_transformer = ColumnTransformer([\n",
    "    (\"time_last_donation\", DeltaTime(unit='months'), ['LASTDATE','MINRDATE','MAXRDATE','MAXADATE']),\n",
    "    (\"delta_first_next\", DeltaTime(reference_date=learning.NEXTDATE), ['FISTDATE']),\n",
    "    (\"membership_years\", DeltaTime(unit='years'),['ODATEDW', 'DOB'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas = timedelta_transformer.fit_transform(learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_feature_names = [n[n.find('__')+2:]\n",
    "                 for n in timedelta_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas = pd.DataFrame(timedeltas, index=learning.index,columns=timedelta_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = learning.merge(timedeltas, on=learning.index.name)\n",
    "learning.drop(dh.date_features, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying redundance of DOB <-> AGE and \\[FISTDATE, NEXTDATE\\] <-> TIMELAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = pd.DataFrame([learning.AGE, timedeltas.DOB_DELTA_YEARS]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ages.loc[ages.AGE != ages.DOB_DELTA_YEARS,:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = pd.DataFrame([learning.TIMELAG, timedeltas.FISTDATE_NEXTDATE_DELTA_MONTHS]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags.loc[lags.TIMELAG != lags.FISTDATE_NEXTDATE_DELTA_MONTHS,:].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed feature DOB is represented in the feature AGE already. So we can drop DOB_DELTA_YEARS. TIMELAG already holds the difference in months between FISTDATE and NEXTDATE, so this delta can also be safely removed together with the original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.drop(['DOB_DELTA_YEARS', 'FISTDATE_NEXTDATE_DELTA_MONTHS'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline\n",
    "\n",
    "The preprocessing pipeline results in a dataset with numerical (binary features encoded correclty), categorial and string date features.\n",
    "\n",
    "Following this step, feature extraction, imputation, dropping of constant and sparse features and ensuring all data is numerical can be tackled.\n",
    "\n",
    "https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087\n",
    "\n",
    "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159\n",
    "\n",
    "The hashing transformer hashes the nominal feature values into an 8 bit representation. If more than one feature is passed in, they all get encoded into the same 8 bits, therefore in effect reducing the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = dh.KDD98DataLoader(\"cup98LRN.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = data_loader.clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_transformer = ColumnTransformer([\n",
    "            (\"hash_osource\", HashingEncoder(), ['OSOURCE']),\n",
    "            (\"hash_tcode\", HashingEncoder(), ['TCODE']),\n",
    "            (\"hash_zip\", HashingEncoder(), ['ZIP'])\n",
    "        ])\n",
    "hashes = hashing_transformer.fit_transform(learning)\n",
    "data = ut.update_df_with_transformed(learning,hashes,hashing_transformer)\n",
    "data = data.drop(['OSOURCE', 'TCODE', 'ZIP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_names = hashing_transformer.get_feature_names()\n",
    "data_df = pd.DataFrame(data = hashes, columns = hashing_names, index=learning.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\"date_features\",\n",
    "     # Date features are converted to time deltas.\n",
    "     ColumnTransformer([\n",
    "        (\"months_to_donation\", MonthsToDonation(), dh.promo_history_dates+dh.giving_history_dates),\n",
    "         (\"time_last_donation\", DeltaTime(unit='months'), ['LASTDATE','MINRDATE','MAXRDATE','MAXADATE']),\n",
    "        (\"membership_years\", DeltaTime(unit='years'),['ODATEDW'])\n",
    "        ])\n",
    "    ),\n",
    "    (\"osource\",\n",
    "      ColumnTransformer([(\"hash_osource\", HashingEncoder(), ['OSOURCE'])])\n",
    "    ),\n",
    "    (\"tcode\",\n",
    "      ColumnTransformer([(\"hash_tcode\", HashingEncoder(), ['TCODE'])])\n",
    "    ),\n",
    "    (\"zip\",\n",
    "      ColumnTransformer([(\"hash_zip\", HashingEncoder(), ['ZIP'])])\n",
    "    ),\n",
    "    (\"rfa\",\n",
    "      Pipeline([\n",
    "        # Recency / Frequency / Amount featrues are spread out into individual features, then ordinally encoded\n",
    "        (\"spread_rfa\", ColumnTransformer([('spread', MultiByteExtract([\"R\", \"F\", \"A\"]), dh.nominal_features[2:])])),\n",
    "        (\"order_multibytes\", OrdinalEncoder(mapping=dh.ordinal_mapping_rfa,handle_unknown='ignore'))\n",
    "      ])\n",
    "    ),\n",
    "    (\"domain\",\n",
    "     Pipeline([\n",
    "         # The domain feature holds a code for urbanicity and socio economic status of an area. It is split into two\n",
    "         # and then the socio economic status is recoded to an ordinal feature\n",
    "         (\"spread_domain\", ColumnTransformer([(\"spread\",MultiByteExtract([\"Urbanicity\", \"SocioEconomic\"]),[\"DOMAIN\"])])),\n",
    "         (\"recode_socioecon\", RecodeUrbanSocioEconomic())\n",
    "     ])\n",
    "    ),\n",
    "    (\"mdmaud\",\n",
    "     ColumnTransformer([\n",
    "         (\"mdmaud\",\n",
    "         OrdinalEncoder(mapping=dh.ordinal_mapping_mdmaud,handle_unknown='ignore'),\n",
    "         ['MDMAUD_R','MDMAUD_A'])\n",
    "     ]),\n",
    "     remainder = 'passthrough'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing values\n",
    "\n",
    "https://github.com/epsilon-machine/missingpy\n",
    "\n",
    "Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman, Missing value estimation methods for DNA microarrays, BIOINFORMATICS Vol. 17 no. 6, 2001 Pages 520-525\n",
    "\n",
    "This step requires that we first drop features with more than 80% missing values for the KNNImputer to work.\n",
    "\n",
    "Best results with k=3: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959387/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in dataset2.columns if dataset2[c].count() / len(dataset2.index) <= 0.2]\n",
    "dataset2.drop([c for c in dataset2.columns if dataset2[c].count() / len(dataset2.index) <= 0.2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.drop([c for c in dataset2.columns if dataset2[c].count() / len(dataset2.index) <= 0.2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set weights to distance so that binary and categorical features get an integer value:\n",
    "https://www.queryxchange.com/q/27_52658127/imputing-missing-values-with-knn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
    "kdd_learn_feat_imputed = imputer.fit_transform(dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing constant features\n",
    "\n",
    "As per the documentation, features with either low variance or very few non-NA examples are to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in kdd_learn_feat_imputed.columns if kdd_learn_feat_imputed[c].var() <= 1e-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing constant features (zero variance)\n",
    "\n",
    "sklearn.feature_selection_variance_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in learning.columns:\n",
    "        if len(learning[column].unique()) == 1:\n",
    "            print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = []\n",
    "for column in learning:\n",
    "    top_freq = learning[column].value_counts(normalize=True).iloc[0]\n",
    "    if top_freq > 0.995:\n",
    "        sparse_features.append(column)\n",
    "        print(column+\" has a top frequency of: \" + str(top_freq))\n",
    "        print(learning[column].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced approaches\n",
    "\n",
    "* If overfitting is a problem, ensemble-learning or tree learning can be used to find important features, then apply SelectFromModel before the actual estimator. See http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_variance_cols(df=None, cols=None,\n",
    "                             skip_cols=[], thresh=1e-5,\n",
    "                             autoremove=False):\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn VarianceThreshold for use on pandas dataframes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # get list of all the original df cols\n",
    "        all_cols = df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # remove `skip_cols`\n",
    "        remaining_cols = all_cols.drop(skip_cols)\n",
    "\n",
    "        # get length of new index\n",
    "        max_index = len(remaining_cols) - 1\n",
    "\n",
    "        # get indices for `skip_cols`\n",
    "        skipped_idx = [all_cols.get_loc(column)\n",
    "                       for column\n",
    "                       in skip_cols]\n",
    "\n",
    "        # adjust insert location by the number of cols removed\n",
    "        # (for non-zero insertion locations) to keep relative\n",
    "        # locations intact\n",
    "        for idx, item in enumerate(skipped_idx):\n",
    "            if item > max_index:\n",
    "                diff = item - max_index\n",
    "                skipped_idx[idx] -= diff\n",
    "            if item == max_index:\n",
    "                diff = item - len(skip_cols)\n",
    "                skipped_idx[idx] -= diff\n",
    "            if idx == 0:\n",
    "                skipped_idx[idx] = item\n",
    "\n",
    "        # get values of `skip_cols`\n",
    "        skipped_values = df.iloc[:, skipped_idx].values\n",
    "\n",
    "        # get dataframe values\n",
    "        X = df.loc[:, remaining_cols].values\n",
    "\n",
    "        # instantiate VarianceThreshold object\n",
    "        vt = VarianceThreshold(threshold=thresh)\n",
    "\n",
    "        # fit vt to data\n",
    "        vt.fit(X)\n",
    "\n",
    "        # get the indices of the features that are being kept\n",
    "        feature_indices = vt.get_support(indices=True)\n",
    "\n",
    "        # remove low-variance cols from index\n",
    "        feature_names = [remaining_cols[idx]\n",
    "                         for idx, _\n",
    "                         in enumerate(remaining_cols)\n",
    "                         if idx\n",
    "                         in feature_indices]\n",
    "\n",
    "        # get the cols to be removed\n",
    "        removed_features = list(np.setdiff1d(remaining_cols,\n",
    "                                             feature_names))\n",
    "        print(\"Found {0} low-variance cols.\"\n",
    "              .format(len(removed_features)))\n",
    "\n",
    "        # remove the cols\n",
    "        if autoremove:\n",
    "            print(\"Removing low-variance features.\")\n",
    "            # remove the low-variance cols\n",
    "            X_removed = vt.transform(X)\n",
    "\n",
    "            print(\"Reassembling the dataframe (with low-variance \"\n",
    "                  \"features removed).\")\n",
    "            # re-assemble the dataframe\n",
    "            df = pd.DataFrame(data=X_removed,\n",
    "                                  cols=feature_names)\n",
    "\n",
    "            # add back the `skip_cols`\n",
    "            for idx, index in enumerate(skipped_idx):\n",
    "                df.insert(loc=index,\n",
    "                              column=skip_cols[idx],\n",
    "                              value=skipped_values[:, idx])\n",
    "            print(\"Succesfully removed low-variance cols.\")\n",
    "\n",
    "        # do not remove cols\n",
    "        else:\n",
    "            print(\"No changes have been made to the dataframe.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not remove low-variance features. Something \"\n",
    "              \"went wrong.\")\n",
    "        pass\n",
    "\n",
    "    return df, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, removed = get_low_variance_cols(kdd_learn_feat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring strategies for specific feature types\n",
    "\n",
    "* Noisy data: Correction of data entry / formatting errors\n",
    "    - These errors must be corrected without excluding the records in question\n",
    "* Missing data: Has to be inferred from known values\n",
    "    - (e.g., mean, median, mode, a modeled value).\n",
    "    - One exception to this rule is the attributes containing 99.5 percent or more missings. These are to be dropped\n",
    "* Sparse data: Events actually represented in given data make only a very small subset of the event space are to be dropped\n",
    "* Constant values are to be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant and Sparse Features\n",
    "\n",
    "Features where only one value is present and those where the majority is empty are to be dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_sparse_transformer = DropSparseLowVar(keep_anyways=[\"RAMNT_\\d{1,2}\", \"MONTHS_TO_DONATION_\\d{1,2}\"])\n",
    "cs = const_sparse_transformer.fit(learning)\n",
    "cs = const_sparse_transformer.fit_transform(learning)\n",
    "set(cs.columns)\n",
    "const_sparse_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = learning_raw.select_dtypes(include='object').columns\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in objects:\n",
    "    print(f+\": \"+learning_raw[f].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are two types:\n",
    "\n",
    "* ZIP: Malformed zip codes. Some have a dash at the end, which has to be removed.\n",
    "* Multibyte values. These can be extracted into separate features bytewise. However, this is done in feature extraction later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "It is now time to construct the preprocessing pipeline. A set of transforming operations is concatenated to a sequence of operations. This pipeline is the learned on the learning dataset. All transformations to the learning dataset will then later be applied to the test dataset and to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_feats = list(kdd_learn_feat.select_dtypes(include=np.number).columns)\n",
    "categorical_feats = list(kdd_learn_feat.select_dtypes(include=np.number).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all categories now properly formatted, it is time for one-hot encoding. The sklearn pipeline also has an impute transformation. NaN's get their own level, \"missing\". This step results in a huge increase in the dimension of the feature space. It is also heavy on computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"one_hot\",  OneHotEncoder(impute_missing=True,use_cat_names=True,return_df=True))\n",
    "])\n",
    "\n",
    "categories_transformer = ColumnTransformer([\n",
    "    (\"cat_encoder\",\n",
    "     cat_pipe,\n",
    "     list(kdd_learn_feat.select_dtypes(include=\"category\").columns))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interests and donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = learning_raw.loc[:,dh.interest_features+[\"TARGET_D\"]].fillna(0)\n",
    "interests = pd.melt(data,value_vars=dh.interest_features, value_name=\"Interest\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with constant values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training- and test dataset\n",
    "\n",
    "Before applying *any* transformations, the dataset will be split 80/20 into a learning and test set.\n",
    "\n",
    "Let's look at feature TARGET_B, which describes whether a person has donated or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.TARGET_B.value_counts(normalize=True) # 5 % of recipients have donated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to preserve this ratio in the split datasets. scikit-learn provides a method for achieving this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = Config.get.config(\"random_seed\")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8, random_state=seed)\n",
    "for learn_index, test_index in splitter.split(learning_raw, learning_raw.TARGET_B.astype('int')):\n",
    "    l_i = learn_index\n",
    "    t_i = test_index\n",
    "    kdd_learn = learning_raw.iloc[learn_index]\n",
    "    kdd_test = learning_raw.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check that the two sets are really disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(kdd_learn.index).intersection(kdd_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the frequencies of the donors in the sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_learn['TARGET_B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_test['TARGET_B'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating features and label\n",
    "\n",
    "First, we separate the features from the labels. We will also remove the label \"TARGET_B\", which is an indicator variable for donors that is no longer of interest\n",
    "\n",
    "**All preprocessing is performed on *kdd_learn_feat***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd_learn_feat = kdd_learn.drop(['TARGET_B', 'TARGET_D'],axis=1).copy()\n",
    "kdd_learn_labels = kdd_learn[['TARGET_B','TARGET_D']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "All explanatory fields have to be numerical for the subsequent operations with scikit-learn. Here, the necessary feature extractions are performed.\n",
    "\n",
    "See [scikit-learn: feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Meant to reduce dimensionality by selecting only features that are 'interesting enough' to be considered in order to boost performance of calculations / improve accuracy of the estimator\n",
    "- By variance threshold\n",
    "- Recursive Feature Elimination by Cross-Validation\n",
    "- L1-based feature selection (Logistic Regression, Lasso, SVM)\n",
    "- Tree-based feature selection\n",
    "\n",
    "See [scikit-learn: feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
