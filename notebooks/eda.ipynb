{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-learning-dataset\" data-toc-modified-id=\"Loading-the-learning-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the learning dataset</a></span></li><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-Features\" data-toc-modified-id=\"Numerical-Features-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Numerical Features</a></span></li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Categorical Features</a></span></li><li><span><a href=\"#Object-Features\" data-toc-modified-id=\"Object-Features-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Object Features</a></span></li><li><span><a href=\"#Date-features\" data-toc-modified-id=\"Date-features-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Date features</a></span></li></ul></li><li><span><a href=\"#Cleaning\" data-toc-modified-id=\"Cleaning-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-features\" data-toc-modified-id=\"Binary-features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Binary features</a></span></li><li><span><a href=\"#Dates\" data-toc-modified-id=\"Dates-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Dates</a></span></li><li><span><a href=\"#Categories\" data-toc-modified-id=\"Categories-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Categories</a></span><ul class=\"toc-item\"><li><span><a href=\"#Treating-multibyte-features,-OSOURCE-and-TCODE:\" data-toc-modified-id=\"Treating-multibyte-features,-OSOURCE-and-TCODE:-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Treating multibyte features, OSOURCE and TCODE:</a></span></li><li><span><a href=\"#Ordinal-features\" data-toc-modified-id=\"Ordinal-features-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Ordinal features</a></span></li></ul></li></ul></li><li><span><a href=\"#Actual-EDA\" data-toc-modified-id=\"Actual-EDA-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Actual EDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Socio-economic-environment-and-label\" data-toc-modified-id=\"Socio-economic-environment-and-label-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Socio-economic environment and label</a></span></li><li><span><a href=\"#Correlations\" data-toc-modified-id=\"Correlations-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Correlations</a></span></li><li><span><a href=\"#Correlations-between-numerical-features,-excluding-US-census-data\" data-toc-modified-id=\"Correlations-between-numerical-features,-excluding-US-census-data-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Correlations between numerical features, excluding US census data</a></span></li><li><span><a href=\"#Promotion-history-correlations\" data-toc-modified-id=\"Promotion-history-correlations-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Promotion history correlations</a></span></li><li><span><a href=\"#Giving-history-correlations\" data-toc-modified-id=\"Giving-history-correlations-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Giving history correlations</a></span></li><li><span><a href=\"#Puttting-donors-on-a-map\" data-toc-modified-id=\"Puttting-donors-on-a-map-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Puttting donors on a map</a></span></li><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Categorical features</a></span></li><li><span><a href=\"#The-US-census-data\" data-toc-modified-id=\"The-US-census-data-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>The US census data</a></span></li><li><span><a href=\"#Income,-Wealth-and-donations\" data-toc-modified-id=\"Income,-Wealth-and-donations-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Income, Wealth and donations</a></span></li><li><span><a href=\"#Some-promising-fetures-and-their-impact-on-the-label\" data-toc-modified-id=\"Some-promising-fetures-and-their-impact-on-the-label-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>Some promising fetures and their impact on the label</a></span></li><li><span><a href=\"#Boruta\" data-toc-modified-id=\"Boruta-4.11\"><span class=\"toc-item-num\">4.11&nbsp;&nbsp;</span>Boruta</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "This notebook contains all code for the prelimiatory analysis of the KDD Cup 98 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup logging to file: out.log\n",
      "Figure output directory saved in figure_output at /home/datarian/OneDrive/unine/Master_Thesis/figures\n"
     ]
    }
   ],
   "source": [
    "# Set up logging and graphics defaults\n",
    "%run ./common_init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import kdd98.data_loader as dl\n",
    "import kdd98.utils_transformer as ut\n",
    "from kdd98.transformers import *\n",
    "from kdd98.config import App\n",
    "\n",
    "# Where to save the figures\n",
    "IMAGES_PATH = pathlib.Path(figure_output/'eda')\n",
    "\n",
    "pathlib.Path(IMAGES_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = pathlib.Path(IMAGES_PATH/fig_id + \".\" + fig_extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the learning dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set working directory to main code folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = dl.KDD98DataLoader(\"cup98LRN.txt\", pull_stored=False)\n",
    "learning = data_loader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kdd98.preprocessor\n",
    "preproc = Preprocessor(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "We will leverage scikit-learn's transformer classes, and add our own custom transformers. This might on first glance look as a tedious way to clean data. However, it will be very powerful later on. The transformer's parameters are actually hyperparameters in model selection. This means that a grid-search can be employed to evaluate several different strategies for i.e. imputation of missing values, cutoff thresholds for sparse features and so on and find the best preprocessing steps.\n",
    "\n",
    "sklearn doc:\n",
    "\n",
    "* http://scikit-learn.org/dev/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders.hashing import HashingEncoder  # Use custom edits in local file instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary features\n",
    "\n",
    "For these, we will convert the values specified as True and False as per the dataset dictionary into 1.0 and 0.0 respectively. Furthermore, input errors are also being treated. In the end, these features will be of dtype float64, having {1.0, 0.0 and NaN} as values.\n",
    "\n",
    "For features that either have a value representing True or are empty (as specified in the dataset dictionary), all empty cells will be considered False. For features specifically denoting True and False values, these will be coded appropriately and empty cells set to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dl.binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "binary_transformers = ColumnTransformer([\n",
    "    (\"binary_x_bl\",\n",
    "     BinaryFeatureRecode(value_map={'true': 'X', 'false': ' '}, correct_noisy=False),\n",
    "     ['PEPSTRFL', 'NOEXCH', 'MAJOR', 'RECINHSE', 'RECP3', 'RECPGVG', 'RECSWEEP']\n",
    "     ),\n",
    "    (\"binary_y_n\",\n",
    "     BinaryFeatureRecode(value_map={'true': 'Y', 'false': 'N'}, correct_noisy=False),\n",
    "     ['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO',\n",
    "      'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN',  'BOATS', 'WALKER', 'KIDSTUFF',\n",
    "      'CARDS', 'PLATES']\n",
    "     ),\n",
    "    (\"binary_e_i\",\n",
    "     BinaryFeatureRecode(value_map={'true': \"E\", 'false': 'I'}, correct_noisy=False),\n",
    "     ['AGEFLAG']\n",
    "     ),\n",
    "    (\"binary_h_u\",\n",
    "     BinaryFeatureRecode(value_map={'true': \"H\", 'false': 'U'}, correct_noisy=False),\n",
    "     ['HOMEOWNR']),\n",
    "    (\"binary_b_bl\",\n",
    "     BinaryFeatureRecode(value_map={'true': 'B', 'false': ' '}, correct_noisy=False),\n",
    "     ['MAILCODE']\n",
    "     ),\n",
    "    (\"binary_1_0\",\n",
    "     BinaryFeatureRecode(value_map={'true': '1', 'false': '0'}, correct_noisy=False),\n",
    "     ['HPHONE_D', 'TARGET_B']\n",
    "     )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarys = binary_transformers.fit_transform(learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_feature_names = [n[n.find('__')+2:]\n",
    "                 for n in binary_transformers.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarys = pd.DataFrame(data=binarys, columns=binary_feature_names,\n",
    "                     index=learning.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarys.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several features contain only very few actual feature values. These might get dropped by the sparsity transformer later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning[binary_feature_names] = binarys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories\n",
    "\n",
    "\n",
    "Some categories are already created on import of the data. Additionally, we will have to treat some special cases:\n",
    "\n",
    "* Multibyte features. These are features that group together several related nominal features. These are mainly the promotion history codes. Recency, Frequency and Amount as of a particular mailing are glued together in one feature. For RFA_2 and additionally MDMAUD, the major donor matrix, the features were already spread out by the supplier of the data. These two were dropped on import of the CSV file and their spread out features kept.\n",
    "\n",
    "* OSOURCE: It identifies the origin of the data for a particular record. However, it has so many levels that the feature space would get inflated heavily by one-hot encoding. For this feature, hasing is employed.\n",
    "\n",
    "* TCODE: Special treatment will also be necessary for the TCODE feature. It describes the title code (Ms., Hon., and so on) in an unfortunate integer coding ranging from 1e0 to 1e4. We will also use the hasing encoder for these features\n",
    "\n",
    "After having the categorical features ready, missing values are assigned their own category, 'missing'. Then, all non-hashed categorical features are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.select_dtypes(include=\"category\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating multibyte features, OSOURCE and TCODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dl.nominal_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087\n",
    "\n",
    "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159\n",
    "\n",
    "The hashing transformer hashes the nominal feature values into an 8 bit representation. If more than one feature is passed in, they all get encoded into the same 8 bits, therefore in effect reducing the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_transformer = ColumnTransformer([\n",
    "    (\"hash_osource\", HashingEncoder(), ['OSOURCE'])\n",
    "])\n",
    "\n",
    "multibyte_transformer = ColumnTransformer([\n",
    "    (\"promotion_history_spreader\",\n",
    "     MultiByteExtract([\"R\", \"F\", \"A\"]),\n",
    "     dl.nominal_features[2:]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transormations to all RFA_* features and the OSOURCE feature and extract the new feature names to build a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = hash_transformer.fit_transform(learning)\n",
    "feature_names_h = [n[n.find('__')+2:]\n",
    "                 for n in hash_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multibytes = multibyte_transformer.fit_transform(learning)\n",
    "feature_names_m = [n[n.find('__')+2:]\n",
    "                 for n in multibyte_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge learning and the new nominal features, then drop the originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multibytes = pd.DataFrame(data=multibytes, columns=feature_names_m,\n",
    "                   index=learning.index).astype(\"category\")\n",
    "learning = learning.merge(multibytes, on=learning.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = pd.DataFrame(data=hashes, columns=feature_names_h,\n",
    "                   index=learning.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = learning.merge(hashes, on=learning.index.name)\n",
    "learning = learning.drop(dl.nominal_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in learning.select_dtypes(include=\"category\").columns:\n",
    "    learning[cat] = learning[cat].cat.remove_unused_categories()\n",
    "    print(\"Feature: {}\\n{}\".format(cat, learning[cat].cat.categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal features\n",
    "\n",
    "Several ordinal features are present. We need to ensure to encode the levels correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the order is obvious, no order has to be passed in (i.e. 0 < 1 < 2 < 3 < ... and alphabetical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ordered(feature):\n",
    "    try:\n",
    "        learning[feature] = learning[feature].cat.as_ordered()\n",
    "    except AttributeError as e:\n",
    "         learning[feature] = learning[feature].astype(\"category\").cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['WEALTH1','WEALTH2','INCOME']+list(learning.filter(regex=\"RFA_\\d{1,2}F\").columns)+list(learning.filter(regex=\"RFA_\\d{1,2}A\").columns):\n",
    "    make_ordered(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a new level 'missing' to each category to encode NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = learning.select_dtypes(include=\"int64\").columns\n",
    "learning[int_cols] = learning[int_cols].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the label (amount donated in US dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.barplot(x = [0,1], y = learning.groupby('TARGET_B')['TARGET_B'].count()/len(learning.index),\n",
    "                  palette=App.config(\"color_palette_binary\"));\n",
    "fig.set_xticklabels([\"No\", \"Yes\"]);\n",
    "plt.xlabel(\"Donated\");\n",
    "plt.ylabel(\"Percentage of examples\");\n",
    "save_fig(fig_id=\"_ratio_binary\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.TARGET_D = learning.TARGET_D.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.distplot(learning.loc[learning.TARGET_D > 0, ('TARGET_D')], bins=50, hist_kws={'alpha': 0.9}, color=App.config(\"color_palette\")[0])\n",
    "plt.ylabel(\"Percentage of donors\");\n",
    "save_fig('target_distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.loc[learning.TARGET_D > 0.0, 'TARGET_D'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The label is imbalanced, with roughly 95% / 5%\n",
    "* Most donations are below 20 dollars. The median is 13 \\$\n",
    "* Spikes are visible for 5, 10, 15, 20, 25, 50, 100 and 200 $\n",
    "* The distribution is right-skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the claim from the documentation that donations are positively correlated with the time since the last donation. We plot the duration since the last gift against the donation amount for the current campaign. The marker size indicates the total amount an example has donated so far.\n",
    "\n",
    "It is evident that from a lag of &geq; 15 months, donations increase indeed, and over the whole spectrum of amounts. We see a marked difference in 100- and 50 $ donations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='LASTDATE_DELTA_MONTHS',y='TARGET_D', size='RAMNTALL', alpha=0.6, data=learning.loc[learning.TARGET_D > 0,:],\n",
    "                palette=App.config(\"color_palette_binary\"))\n",
    "plt.xlabel(\"Months passed without a donation before the current donation\");\n",
    "plt.ylabel(\"Amount donated, $\");\n",
    "save_fig(fig_id=\"donations_vs_time_since_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Socio-economic environment and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donations by living environment (C=City, R=Rural, S=Suburban, T=Town,U=Urban; lowest numbers represent highest socio-economic ranking). Major donors versus non-major donors.\n",
    "\n",
    "Surprisingly, one of the top donations came from a rural region of low socio-economic status. Major donors that donated this time are not present in the lowest socio-economic environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(y=\"TARGET_D\", x=\"DOMAIN\", hue='MAJOR',cut=0, data=learning.loc[learning.TARGET_D > 0,:],\n",
    "               palette=App.config(\"color_palette_binary\"))\n",
    "plt.xlabel(\"Living environment and socio-economic status\");\n",
    "plt.ylabel(\"Amount donated, $\");\n",
    "save_fig(fig_id=\"donations_vs_living_environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All-time donations by environment. The y- axis is in log scale. We see now that each socio-economic environment also harbours major donors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=sns.boxplot(y=\"RAMNTALL\", x=\"DOMAIN\", hue='MAJOR', data=learning,palette=App.config(\"color_palette_binary\"))\n",
    "fig.set_yscale('log')\n",
    "plt.xlabel(\"Living environment and socio-economic status\");\n",
    "plt.ylabel(\"Lifetime amount donated, $\");\n",
    "save_fig(fig_id=\"donations_vs_living_environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first, general look at the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 481 features (of which one is the index)\n",
    "* A total of 95412 examples\n",
    "* 24 categorical features, 53 datetime features, 48 numerical features with missing values, 297 integer features without missing values and 56 string features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numerical = learning.select_dtypes(include=np.number).columns\n",
    "print(\"There are {:1} numerical features\".format(len(numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = learning.select_dtypes(include=np.float64)\n",
    "stat, p_agost = stats.normaltest(numeric_features,axis=0,nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agost_skew = pd.DataFrame({'Feature': learning.select_dtypes(include=np.float64), 'DAgostino_stat': stat, 'DAgostino_pval': p_agost})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = learning.select_dtypes(include=np.float64).apply(stats.anderson, axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.MBCRAFT.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skewtest = pd.DataFrame({'Feature': learning.select_dtypes(include=np.float64).columns, 'Statistic': results_skewtest.statistic, 'P_val': results_skewtest.pvalue})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_skewtest.loc[df_skewtest.Statistic >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "Categories were defined on import of the csv data. The categories were identified in the dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = learning.select_dtypes(include='category').columns\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.loc[:, categories].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Features\n",
    "\n",
    "These features have mixed datatypes and are encoded as strings. This hints at noisy data and features that will have to be transformed before becoming usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = learning.select_dtypes(include='object').columns\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.loc[:, objects].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features\n",
    "These are imported as strings and will have to be transformed later on to become useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = learning.loc[:, dl.date_features]\n",
    "dates.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many features, we will plot those who have a significant correlation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_all = learning.drop(['TARGET_B','TARGET_D'], axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_all = np.zeros_like(corr_all, dtype=np.bool)\n",
    "mask_all[np.triu_indices_from(mask_all)] = True\n",
    "\n",
    "sns.heatmap(corr_all,\n",
    "            cmap=App.config(\"color_map_diverging\"), mask=mask_all, vmax=1.0, center = 0.0, square=True,\n",
    "            linewidths = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between numerical features, excluding US census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_census_numeric = learning[learning.columns.difference(dl.us_census_features)].select_dtypes(include=[\"float64\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exclude_census_corr = data_exclude_census_numeric[data_exclude_census_numeric.columns.difference(['TARGET_B','TARGET_D'])].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_census = np.zeros_like(data_exclude_census_corr, dtype=np.bool)\n",
    "mask_census[np.triu_indices_from(mask_census)] = True\n",
    "\n",
    "sns.heatmap(data_exclude_census_corr, mask=mask_census, cmap=App.config(\"color_map_diverging\"), vmax=1.0, center=0,\n",
    "            square=True, linewidths=.1, cbar_kws={\"shrink\": .5}, xticklabels=True,yticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promotion history correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom_hist_f = list(donation_responses.columns)+list(multibytes.columns)+dl.promo_history_summary\n",
    "promotion_history_features = learning.reindex(columns=prom_hist_f)\n",
    "prom_hist_corr = promotion_history_features[promotion_history_features.columns.difference(['TARGET_B','TARGET_D'])].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_promo = np.zeros_like(prom_hist_corr, dtype=np.bool)\n",
    "mask_promo[np.triu_indices_from(mask_promo)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "sns.heatmap(prom_hist_corr, mask=mask_promo, cmap=App.config(\"color_map_diverging\"), vmax=1.0, center=0,\n",
    "            square=True, linewidths=.3, cbar_kws={\"shrink\": .5}, xticklabels=True,yticklabels=True)\n",
    "save_fig(fig_id=\"correlations_promotion_giving_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving history correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giving_hist_f = list(donation_responses.columns) + dl.giving_history + dl.giving_history_summary +['LASTDATE_DELTA_MONTHS', 'MINRDATE_DELTA_MONTHS',\n",
    "       'MAXRDATE_DELTA_MONTHS', 'MAXADATE_DELTA_MONTHS']\n",
    "giving_history_features = learning.loc[:,giving_hist_f]\n",
    "giving_corr = giving_history_features[giving_history_features.columns.difference(['TARGET_B','TARGET_D'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_giving = np.zeros_like(giving_corr, dtype=np.bool)\n",
    "mask_giving[np.triu_indices_from(mask_giving)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "sns.heatmap(giving_corr, mask=mask_giving, cmap=App.config(\"color_map_diverging\"), vmax=1.0, center=0,\n",
    "            square=True, linewidths=.1, cbar_kws={\"shrink\": .5}, xticklabels=True,yticklabels=True)\n",
    "save_fig(fig_id=\"correlations_giving_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puttting donors on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_donors_by_zip = learning[['ZIP', 'TARGET_B']].groupby('ZIP', as_index=False).agg('sum') # number of people who donated\n",
    "num_members_by_zip = learning[['ZIP', 'TARGET_B']].groupby('ZIP', as_index=False).agg('count') # number of people who are registered at that ZIP\n",
    "cum_donation_by_zip = learning[['ZIP', 'TARGET_D']].groupby('ZIP', as_index=False).agg('sum')\n",
    "zip_states = learning[['ZIP','STATE']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_zip = cum_donation_by_zip.merge(num_members_by_zip, on='ZIP').merge(zip_states, on='ZIP')\n",
    "data_by_zip.columns = [\"ZIP\", \"CumDonation\", \"MemberCount\", \"State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_donation(row):\n",
    "    if row.CumDonation != 0.0:\n",
    "        return row.CumDonation/(1.0 if row.MemberCount == 0.0 else row.MemberCount)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "data_by_zip['RelDonation'] = data_by_zip.apply(rel_donation,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Here\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "\n",
    "def do_geo_query(q):\n",
    "    geolocator = Here(app_id=\"ZJBxigwxa1QPHlWrtWH6\", app_code=\"OJBun02aepkFbuHmYn1bOg\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=0.01, max_retries=4)\n",
    "    try:\n",
    "        return geolocator.geocode(query=q, exactly_one=True)\n",
    "    except GeocoderTimedOut:\n",
    "        return do_geo_query(q)\n",
    "\n",
    "def get_loc(example):\n",
    "    if example.ZIP:\n",
    "        zip = str(int(example.ZIP)).rjust(5, '0')\n",
    "        q = {'postalcode': zip, 'state': example.State}\n",
    "        return do_geo_query(q)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_coords(location):\n",
    "    return [location.latitude, location.longitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "try:\n",
    "    zip_data = open(\"zip_data.pkl\", \"rb\")\n",
    "    locations = pickle.load(zip_data)\n",
    "    zip_data.close()\n",
    "except Exception as e:\n",
    "    locations = data_by_zip.progress_apply(get_loc, axis=1)\n",
    "    locations = pd.DataFrame(locations, columns=\"location\")\n",
    "    locations['ZIP'] = data_by_zip.ZIP\n",
    "    zip_data = open(\"zip_data.pkl\", \"wb\")\n",
    "    pickle.dump(locations, zip_data)\n",
    "    zip_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_zip = data_by_zip.merge(locations, on='ZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_zip.loc[:,'longitude'] = data_by_zip.location.apply(lambda l: l.longitude if l != None else None)\n",
    "data_by_zip.loc[:,'latitude'] = data_by_zip.location.apply(lambda l: l.latitude if l != None else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AA, AE and AP stand for armed services. ZIP codes don't work here, they point anywhere. Also, we only include locations where someone has actually donated by filtering on CumDonation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_zip1 = data_by_zip.loc[data_by_zip.State != ['AA','AE','AP'],:]\n",
    "data_by_zip2 = data_by_zip1.loc[data_by_zip1.CumDonation > 0.0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy.feature as cfeature\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "\n",
    "osm_terrain = cimgt.OSM()\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1, projection=osm_terrain.crs)\n",
    "\n",
    "ax.set_extent([-166, -65, 10, 65], crs=ccrs.PlateCarree())\n",
    "ax.add_image(osm_terrain, 6)\n",
    "\n",
    "lon = data_by_zip2.longitude\n",
    "lat = data_by_zip2.latitude\n",
    "mc = data_by_zip2.MemberCount\n",
    "cd = data_by_zip2.CumDonation\n",
    "rd = data_by_zip2.RelDonation\n",
    "\n",
    "data_by_zip2.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",ax=ax,\n",
    "                  s=cd, c=rd, label=\"Cumulative Donations\",\n",
    "                  legend=True, alpha=0.5, cmap=App.config(\"color_map\"),\n",
    "                  subplots=True, colorbar=True, transform=ccrs.PlateCarree())\n",
    "            \n",
    "save_fig(fig_id=\"donations_geographical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most donations come from the urban areas, especially San Francisco, Los Angeles, Miami, Chicago and Detroit. To a lesser extent, cities like Houston, Dallas, Minneapolis, Atlanta, Tampa, Seattle and Phoenix can be made out.\n",
    "* Interestingly, the East Coast has not donated, despite featuring some large metropolitan areas like New York, Boston, or Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = learning.select_dtypes(\"category\").copy()\n",
    "target = categories['TARGET_D']\n",
    "categories = categories.drop('TARGET_D', axis=1)\n",
    "#categories['TARGET_B'] = learning.TARGET_B.astype(\"category\")\n",
    "#categories['TARGET_D'] = learning.TARGET_D\n",
    "#categories_grouped = categories.groupby('TARGET_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "lm.fit(np.ndarray(categories),y=np.ndarray(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(categories.TARGET_D,[categories.INCOME],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The US census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = learning[dl.us_census_features]\n",
    "census_corr = census.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(census_corr, dtype=np.binary)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(census_corr, mask=mask, cmap=cmap, vmax=1.0, center=0,\n",
    "            square=True, linewidths=.2, cbar_kws={\"shrink\": .5})\n",
    "save_fig(fig_id=\"correlation_census\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.select_dtypes(include=\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income, Wealth and donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_targ = sns.violinplot(x=\"INCOME\", y=\"TARGET_D\", data=learning.loc[learning.TARGET_D > 0.0, [\"INCOME\",\"TARGET_D\"]])\n",
    "inc_targ.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weal1_targ = sns.violinplot(x=\"WEALTH1\", y=\"TARGET_D\", data=learning.loc[learning.TARGET_D > 0.0, [\"WEALTH1\",\"TARGET_D\"]])\n",
    "weal1_targ.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weal2_targ = sns.violinplot(x=\"WEALTH2\", y=\"TARGET_D\", data=learning.loc[learning.TARGET_D > 0.0, [\"WEALTH2\",\"TARGET_D\"]])\n",
    "weal2_targ.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"WEALTH2\", y=\"TARGET_D\", hue=\"MAJOR\",\n",
    "            kind=\"violin\", inner=\"stick\", split=True, data=learning.loc[learning.TARGET_D > 0.0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"CLUSTER\", y=\"TARGET_D\", kind=\"box\", data=learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(learning.loc[learning.TARGET_D > 0.0,\n",
    "                          'TARGET_D'], bins=50, kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning.select_dtypes(include=np.float).hist(bins=50, figsize=(50, 50))\n",
    "plt.show()\n",
    "save_fig(\"float_feature_histograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = learning.dropna(axis=1)\n",
    "y = X['TARGET_D'].values\n",
    "X = X.drop(['TARGET_B','TARGET_D'],axis=1)\n",
    "cats = X.select_dtypes(include='category').columns\n",
    "dummies = pd.get_dummies(X[cats])\n",
    "X = X.drop(cats, axis=1)\n",
    "X = X.merge(dummies,on=X.index).dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest classifier, with utilising all cores and\n",
    "# sampling in proportion to y labels\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='auto', max_depth=5)\n",
    "\n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "\n",
    "# find all relevant features - 5 features should be selected\n",
    "feat_selector.fit(X, y)\n",
    "\n",
    "# check selected features - first 5 features are selected\n",
    "feat_selector.support_\n",
    "\n",
    "# check ranking of features\n",
    "feat_selector.ranking_\n",
    "\n",
    "# call transform() on X to filter it down to selected features\n",
    "X_filtered = feat_selector.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "A first look at important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from kdd98.transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = learning.drop([\"TARGET_B\", \"TARGET_D\", \"TCODE\"], axis=1)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select_dtypes(include=\"category\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"one_hot\",  OneHotEncoder(impute_missing=True,use_cat_names=True,return_df=True))\n",
    "])\n",
    "\n",
    "categories_transformer = ColumnTransformer([\n",
    "    (\"cat_encoder\",\n",
    "     cat_pipe,\n",
    "     list(X.select_dtypes(include=\"category\").columns))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = categories_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(categories_transformer.named_transformers_.cat_encoder.named_steps.one_hot.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = pd.DataFrame(cats, columns = list(categories_transformer.named_transformers_.cat_encoder.named_steps.one_hot.get_feature_names()), index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.merge(cats, on=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(X.select_dtypes(include=\"category\").columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X_centered.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered[X.select_dtypes(include=\"object\").columns] = X_centered[X.select_dtypes(include=\"object\").columns].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_centered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.fit(X_centered,)\n",
    "result = pd.DataFrame(pca.transform(X_centered), columns=[\n",
    "                      \"PCA%i\" % i for i in range(n_comp)], index=X.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
