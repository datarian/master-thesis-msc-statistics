{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-learning-dataset\" data-toc-modified-id=\"Loading-the-learning-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading the learning dataset</a></span></li><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-Features\" data-toc-modified-id=\"Numerical-Features-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Numerical Features</a></span></li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Categorical Features</a></span></li></ul></li><li><span><a href=\"#Cleaning-data\" data-toc-modified-id=\"Cleaning-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Cleaning data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Treating-multibyte-features\" data-toc-modified-id=\"Treating-multibyte-features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Treating multibyte features</a></span></li><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Categorical features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ordinal\" data-toc-modified-id=\"Ordinal-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Ordinal</a></span></li></ul></li><li><span><a href=\"#Binary-features\" data-toc-modified-id=\"Binary-features-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Binary features</a></span></li><li><span><a href=\"#Object-Features\" data-toc-modified-id=\"Object-Features-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Object Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dates\" data-toc-modified-id=\"Dates-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Dates</a></span></li></ul></li><li><span><a href=\"#The-cleaning-process-put-together\" data-toc-modified-id=\"The-cleaning-process-put-together-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>The cleaning process put together</a></span></li></ul></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Low-variance-(constant)-and-sparse-feature-removal\" data-toc-modified-id=\"Low-variance-(constant)-and-sparse-feature-removal-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Low variance (constant) and sparse feature removal</a></span></li><li><span><a href=\"#Converting-dates\" data-toc-modified-id=\"Converting-dates-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Converting dates</a></span></li><li><span><a href=\"#Preprocessing-put-together\" data-toc-modified-id=\"Preprocessing-put-together-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Preprocessing put together</a></span></li></ul></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imputation-of-missing-values\" data-toc-modified-id=\"Imputation-of-missing-values-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Imputation of missing values</a></span><ul class=\"toc-item\"><li><span><a href=\"#Categoricals\" data-toc-modified-id=\"Categoricals-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Categoricals</a></span></li><li><span><a href=\"#missingpy\" data-toc-modified-id=\"missingpy-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>missingpy</a></span></li><li><span><a href=\"#fancyimpute\" data-toc-modified-id=\"fancyimpute-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>fancyimpute</a></span></li></ul></li><li><span><a href=\"#Nominal-features\" data-toc-modified-id=\"Nominal-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Nominal features</a></span></li><li><span><a href=\"#Removing-low-variance-features\" data-toc-modified-id=\"Removing-low-variance-features-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Removing low-variance features</a></span></li><li><span><a href=\"#Feature-engineering-combined\" data-toc-modified-id=\"Feature-engineering-combined-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Feature engineering combined</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook contains all code for the cleaning of the KDD Cup 98 datasets.\n",
    "\n",
    "* Splits into learning and test\n",
    "* Prepares the data for model fitting\n",
    "\n",
    "This will be done with scikit-learn's transforming framework in order to ensure all transformations are applied identically on training, test and validation datasets.\n",
    "\n",
    "First, the steps necessary are analysed, then the implemented cleaner is introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup logging to file: out.log\n",
      "Figure output directory saved in figure_output at /home/datarian/OneDrive/unine/Master_Thesis/figures\n"
     ]
    }
   ],
   "source": [
    "%run ./common_init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import HashingEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Load custom code\n",
    "import kdd98.data_handler as dh\n",
    "import kdd98.utils_transformer as ut\n",
    "from kdd98.transformers import *\n",
    "from kdd98.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "IMAGES_PATH = pathlib.Path(figure_output/'preprocessing')\n",
    "\n",
    "pathlib.Path(IMAGES_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = pathlib.Path(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the learning dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data_provider = dh.KDD98DataProvider(\"cup98LRN.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw = data_provider.raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first, general look at the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 481 features (of which one is the index)\n",
    "* A total of 95412 examples\n",
    "* 25 categorical features, 48 numerical features with missing values, 297 integer features without missing values and 110 object (string / date) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numerical = learning_raw.select_dtypes(include=np.number).columns\n",
    "print(\"There are {:1} numerical features\".format(len(numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ZIP code, which should be numerical, is missing from the list as it has some input errors. This is evident as it is a object feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.ZIP.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix formatting for ZIP feature\n",
    "learning_raw.ZIP = learning_raw.ZIP.str.replace(\n",
    "    '-', '').replace([' ', '.'], np.nan).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "Some categories are already created on import of the data. Additionally, we will have to treat some special cases:\n",
    "\n",
    "* Multibyte features. These are features that group together several related nominal features. These are mainly the promotion history codes. Recency, Frequency and Amount as of a particular mailing are glued together in one feature. For RFA_2 and additionally MDMAUD, the major donor matrix, the features were already spread out by the supplier of the data. These two were dropped on import of the CSV file and their spread out features kept.\n",
    "\n",
    "* OSOURCE: It identifies the origin of the data for a particular record. However, it has so many levels that the feature space would get inflated heavily by one-hot encoding. For this feature, hasing is employed.\n",
    "\n",
    "* TCODE: Special treatment will also be necessary for the TCODE feature. It describes the title code (Ms., Hon., and so on) in an unfortunate integer coding ranging from 1e0 to 1e4. We will also use the hasing encoder for these features\n",
    "\n",
    "After having the categorical features ready, missing values are assigned their own category, 'missing'. Then, all non-hashed categorical features are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = learning_raw.select_dtypes(include='category').columns\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw[categories].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating multibyte features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dh.NOMINAL_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cup documentation states that for the MDMAUD_* features, X is used as NA code. This is fixed now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw[['MDMAUD_R', 'MDMAUD_F', 'MDMAUD_A']] = learning_raw.loc[:, ['MDMAUD_R', 'MDMAUD_F', 'MDMAUD_A']].replace('X', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multibyte_transformer = ColumnTransformer([\n",
    "            (\"spread_rfa\",\n",
    "             MultiByteExtract([\"R\", \"F\", \"A\"]),\n",
    "             dh.NOMINAL_FEATURES[2:]),\n",
    "             (\"spread_domain\",\n",
    "             MultiByteExtract([\"Urbanicity\", \"SocioEconomic\"]),\n",
    "             [\"DOMAIN\"])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multibytes = multibyte_transformer.fit_transform(learning_raw)\n",
    "multibytes_names = [n[n.find('__')+2:]\n",
    "                 for n in multibyte_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge learning and the new nominal features, then drop the originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multibytes = pd.DataFrame(data=multibytes, columns=multibytes_names,\n",
    "                   index=learning_raw.index).astype(\"category\")\n",
    "learning_raw = learning_raw.merge(multibytes, on=learning_raw.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.drop(dh.NOMINAL_FEATURES[2:]+[\"DOMAIN\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in learning_raw.select_dtypes(include=\"category\").columns:\n",
    "    learning_raw[cat] = learning_raw[cat].cat.remove_unused_categories()\n",
    "    print(\"Feature: {}\\n{}\".format(cat, learning_raw[cat].cat.categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal\n",
    "\n",
    "Several ordinal features are present. We need to ensure to encode the levels correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_transformer = ColumnTransformer([\n",
    "            (\"order_mdmaud\",\n",
    "             OrdinalEncoder(mapping=dh.ORDINAL_MAPPING_MDMAUD,\n",
    "                            handle_unknown='ignore'),\n",
    "             ['MDMAUD_R', 'MDMAUD_A']),\n",
    "            (\"order_rfa\",\n",
    "             OrdinalEncoder(mapping=dh.ORDINAL_MAPPING_RFA,\n",
    "                            handle_unknown='ignore'),\n",
    "                            list(learning_raw.filter(regex=r\"RFA_\\d{1,2}A\", axis=1).columns.values)),\n",
    "            (\"recode_socioecon\", RecodeUrbanSocioEconomic(), [\"DOMAINUrbanicity\", \"DOMAINSocioEconomic\"])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinals = ordinal_transformer.fit_transform(learning_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_names = [n[n.find('__')+2:]\n",
    "                 for n in ordinal_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinals = pd.DataFrame(data=ordinals, columns=ordinal_names,\n",
    "                   index=learning_raw.index).astype(\"category\")\n",
    "learning_raw[ordinal_names] = ordinals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the order is obvious, no order has to be passed in (i.e. 0 < 1 < 2 < 3 < ... and alphabetical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw[\"WEALTH1\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_ordinals = ['WEALTH1','WEALTH2','INCOME']+learning_raw.filter(regex=r\"RFA_\\d{1,2}F\").columns.values.tolist()\n",
    "\n",
    "for f in learning_raw[remaining_ordinals]:\n",
    "    try:\n",
    "        learning_raw[f] = learning_raw[f].cat.as_ordered()\n",
    "    except AttributeError:\n",
    "        learning_raw[f] = learning_raw[f].astype(\"category\").cat.as_ordered()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary features\n",
    "\n",
    "For these, we will convert the values specified as True and False as per the dataset dictionary into 1.0 and 0.0 respectively. Furthermore, input errors are also being treated. In the end, these features will be of dtype float64, having {1.0, 0.0 and NaN} as values.\n",
    "\n",
    "For features that either have a value representing True or are empty (as specified in the dataset dictionary), all empty cells will be considered False. For features specifically denoting True and False values, these will be coded appropriately and empty cells set to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_raw[dh.BINARY_FEATURES].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOEXCH has X and 1 for True, 0 for False, which is not consistent with the documentation. It is therefore recoded to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.NOEXCH.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix binary encoding inconsistency for NOEXCH\n",
    "learning_raw.NOEXCH = learning_raw.NOEXCH.str.replace(\"X\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_transformer = ColumnTransformer([\n",
    "            (\"binary_x_bl\",\n",
    "             BinaryFeatureRecode(\n",
    "                 value_map={'true': 'X', 'false': ' '}, correct_noisy=False),\n",
    "             ['PEPSTRFL', 'MAJOR', 'RECINHSE',\n",
    "                 'RECP3', 'RECPGVG', 'RECSWEEP']\n",
    "             ),\n",
    "            (\"binary_y_n\",\n",
    "             BinaryFeatureRecode(\n",
    "                 value_map={'true': 'Y', 'false': 'N'}, correct_noisy=False),\n",
    "             ['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO',\n",
    "              'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN',  'BOATS', 'WALKER', 'KIDSTUFF',\n",
    "              'CARDS', 'PLATES']\n",
    "             ),\n",
    "            (\"binary_e_i\",\n",
    "             BinaryFeatureRecode(\n",
    "                 value_map={'true': \"E\", 'false': 'I'}, correct_noisy=False),\n",
    "             ['AGEFLAG']\n",
    "             ),\n",
    "            (\"binary_h_u\",\n",
    "             BinaryFeatureRecode(\n",
    "                 value_map={'true': \"H\", 'false': 'U'}, correct_noisy=False),\n",
    "             ['HOMEOWNR']),\n",
    "            (\"binary_b_bl\",\n",
    "             BinaryFeatureRecode(\n",
    "                 value_map={'true': 'B', 'false': ' '}, correct_noisy=False),\n",
    "             ['MAILCODE']\n",
    "             ),\n",
    "            (\"binary_1_0\",\n",
    "             BinaryFeatureRecode(\n",
    "                 value_map={'true': '1', 'false': '0'}, correct_noisy=False),\n",
    "             ['NOEXCH', 'HPHONE_D']\n",
    "             )\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.MAJOR.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaries = binary_transformer.fit_transform(learning_raw)\n",
    "binary_names = [n[n.find('__')+2:]\n",
    "                 for n in binary_transformer.get_feature_names()]\n",
    "binaries = pd.DataFrame(data=binaries, columns=binary_names, index=learning_raw.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaries.MAJOR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw[binary_names] = binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw.RECPGVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Features\n",
    "\n",
    "These features have mixed datatypes and are encoded as strings. This hints at noisy data and features that will have to be transformed before becoming usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = learning_raw.select_dtypes(include='object').columns\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw[objects].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_raw[objects] = learning_raw[objects].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates\n",
    "For dates, input errors are fixed. The dataset contains some dates of length 3, while they should be formatted YYMM. For the short dates, the leading 0 is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = learning_raw[dh.DATE_FEATURES]\n",
    "dates.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedigits = set()\n",
    "for f in dh.DATE_FEATURES:\n",
    "    s = dates.loc[:,f]\n",
    "    if len(s.loc[s.str.len() == 3].values) > 0:\n",
    "        out = s.loc[s.str.len() == 3]\n",
    "        print(out)\n",
    "        unq = out.unique()\n",
    "        unq.sort()\n",
    "        print(unq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have three-digit birth dates. All other dates were correctly entered as YYMM.\n",
    "\n",
    "Looking at these values, the only possibilities for a missing **trailing** digit would be where a **0** or **1** is at the end. For the cases that have a **1** at the end, all years would then be even decades. This cannot be a coincidence and therefore it is more likely that the **leading 0 was forgotten** for these examples.\n",
    "\n",
    "The only case with a **0** at the end is 410, which is assumed to be 0410 based on the observation for the **1**s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_format(d):\n",
    "    if not pd.isna(d):\n",
    "        if len(d) == 3:\n",
    "            d = '0'+d\n",
    "    return d\n",
    "\n",
    "data[DATE_FEATURES] = data.loc[:,DATE_FEATURES].applymap(fix_format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cleaning process put together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps highlighted above are conveniently wrapped in the class `Cleaner` in module `data_provider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean = data_provider.clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cleaned dataset, multibyte features were split. There are therefore more features present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of preprocessing is to:\n",
    "\n",
    "- Remove low variance and sparse features\n",
    "- Impute missing values\n",
    "\n",
    "In order to assess low-variance features, the data should be imputed.\n",
    "For imputation to work, all non-numeric data has to be converted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low variance (constant) and sparse feature removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the list of features that have less than 20% values. These will be removed before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ODATEDW',\n",
       " 'OSOURCE',\n",
       " 'TCODE',\n",
       " 'STATE',\n",
       " 'ZIP',\n",
       " 'MAILCODE',\n",
       " 'RECINHSE',\n",
       " 'RECP3',\n",
       " 'RECPGVG',\n",
       " 'RECSWEEP',\n",
       " 'CLUSTER',\n",
       " 'GENDER',\n",
       " 'HIT',\n",
       " 'MALEMILI',\n",
       " 'MALEVET',\n",
       " 'VIETVETS',\n",
       " 'WWIIVETS',\n",
       " 'LOCALGOV',\n",
       " 'STATEGOV',\n",
       " 'FEDGOV',\n",
       " 'MAJOR',\n",
       " 'PEPSTRFL',\n",
       " 'POP901',\n",
       " 'POP902',\n",
       " 'POP903',\n",
       " 'POP90C1',\n",
       " 'POP90C2',\n",
       " 'POP90C3',\n",
       " 'POP90C4',\n",
       " 'POP90C5',\n",
       " 'ETH1',\n",
       " 'ETH2',\n",
       " 'ETH3',\n",
       " 'ETH4',\n",
       " 'ETH5',\n",
       " 'ETH6',\n",
       " 'ETH7',\n",
       " 'ETH8',\n",
       " 'ETH9',\n",
       " 'ETH10',\n",
       " 'ETH11',\n",
       " 'ETH12',\n",
       " 'ETH13',\n",
       " 'ETH14',\n",
       " 'ETH15',\n",
       " 'ETH16',\n",
       " 'AGE901',\n",
       " 'AGE902',\n",
       " 'AGE903',\n",
       " 'AGE904',\n",
       " 'AGE905',\n",
       " 'AGE906',\n",
       " 'AGE907',\n",
       " 'CHIL1',\n",
       " 'CHIL2',\n",
       " 'CHIL3',\n",
       " 'AGEC1',\n",
       " 'AGEC2',\n",
       " 'AGEC3',\n",
       " 'AGEC4',\n",
       " 'AGEC5',\n",
       " 'AGEC6',\n",
       " 'AGEC7',\n",
       " 'CHILC1',\n",
       " 'CHILC2',\n",
       " 'CHILC3',\n",
       " 'CHILC4',\n",
       " 'CHILC5',\n",
       " 'HHAGE1',\n",
       " 'HHAGE2',\n",
       " 'HHAGE3',\n",
       " 'HHN1',\n",
       " 'HHN2',\n",
       " 'HHN3',\n",
       " 'HHN4',\n",
       " 'HHN5',\n",
       " 'HHN6',\n",
       " 'MARR1',\n",
       " 'MARR2',\n",
       " 'MARR3',\n",
       " 'MARR4',\n",
       " 'HHP1',\n",
       " 'HHP2',\n",
       " 'DW1',\n",
       " 'DW2',\n",
       " 'DW3',\n",
       " 'DW4',\n",
       " 'DW5',\n",
       " 'DW6',\n",
       " 'DW7',\n",
       " 'DW8',\n",
       " 'DW9',\n",
       " 'HV1',\n",
       " 'HV2',\n",
       " 'HV3',\n",
       " 'HV4',\n",
       " 'HU1',\n",
       " 'HU2',\n",
       " 'HU3',\n",
       " 'HU4',\n",
       " 'HU5',\n",
       " 'HHD1',\n",
       " 'HHD2',\n",
       " 'HHD3',\n",
       " 'HHD4',\n",
       " 'HHD5',\n",
       " 'HHD6',\n",
       " 'HHD7',\n",
       " 'HHD8',\n",
       " 'HHD9',\n",
       " 'HHD10',\n",
       " 'HHD11',\n",
       " 'HHD12',\n",
       " 'ETHC1',\n",
       " 'ETHC2',\n",
       " 'ETHC3',\n",
       " 'ETHC4',\n",
       " 'ETHC5',\n",
       " 'ETHC6',\n",
       " 'HVP1',\n",
       " 'HVP2',\n",
       " 'HVP3',\n",
       " 'HVP4',\n",
       " 'HVP5',\n",
       " 'HVP6',\n",
       " 'HUR1',\n",
       " 'HUR2',\n",
       " 'RHP1',\n",
       " 'RHP2',\n",
       " 'RHP3',\n",
       " 'RHP4',\n",
       " 'HUPA1',\n",
       " 'HUPA2',\n",
       " 'HUPA3',\n",
       " 'HUPA4',\n",
       " 'HUPA5',\n",
       " 'HUPA6',\n",
       " 'HUPA7',\n",
       " 'RP1',\n",
       " 'RP2',\n",
       " 'RP3',\n",
       " 'RP4',\n",
       " 'MSA',\n",
       " 'ADI',\n",
       " 'DMA',\n",
       " 'IC1',\n",
       " 'IC2',\n",
       " 'IC3',\n",
       " 'IC4',\n",
       " 'IC5',\n",
       " 'IC6',\n",
       " 'IC7',\n",
       " 'IC8',\n",
       " 'IC9',\n",
       " 'IC10',\n",
       " 'IC11',\n",
       " 'IC12',\n",
       " 'IC13',\n",
       " 'IC14',\n",
       " 'IC15',\n",
       " 'IC16',\n",
       " 'IC17',\n",
       " 'IC18',\n",
       " 'IC19',\n",
       " 'IC20',\n",
       " 'IC21',\n",
       " 'IC22',\n",
       " 'IC23',\n",
       " 'HHAS1',\n",
       " 'HHAS2',\n",
       " 'HHAS3',\n",
       " 'HHAS4',\n",
       " 'MC1',\n",
       " 'MC2',\n",
       " 'MC3',\n",
       " 'TPE1',\n",
       " 'TPE2',\n",
       " 'TPE3',\n",
       " 'TPE4',\n",
       " 'TPE5',\n",
       " 'TPE6',\n",
       " 'TPE7',\n",
       " 'TPE8',\n",
       " 'TPE9',\n",
       " 'PEC1',\n",
       " 'PEC2',\n",
       " 'TPE10',\n",
       " 'TPE11',\n",
       " 'TPE12',\n",
       " 'TPE13',\n",
       " 'LFC1',\n",
       " 'LFC2',\n",
       " 'LFC3',\n",
       " 'LFC4',\n",
       " 'LFC5',\n",
       " 'LFC6',\n",
       " 'LFC7',\n",
       " 'LFC8',\n",
       " 'LFC9',\n",
       " 'LFC10',\n",
       " 'OCC1',\n",
       " 'OCC2',\n",
       " 'OCC3',\n",
       " 'OCC4',\n",
       " 'OCC5',\n",
       " 'OCC6',\n",
       " 'OCC7',\n",
       " 'OCC8',\n",
       " 'OCC9',\n",
       " 'OCC10',\n",
       " 'OCC11',\n",
       " 'OCC12',\n",
       " 'OCC13',\n",
       " 'EIC1',\n",
       " 'EIC2',\n",
       " 'EIC3',\n",
       " 'EIC4',\n",
       " 'EIC5',\n",
       " 'EIC6',\n",
       " 'EIC7',\n",
       " 'EIC8',\n",
       " 'EIC9',\n",
       " 'EIC10',\n",
       " 'EIC11',\n",
       " 'EIC12',\n",
       " 'EIC13',\n",
       " 'EIC14',\n",
       " 'EIC15',\n",
       " 'EIC16',\n",
       " 'OEDC1',\n",
       " 'OEDC2',\n",
       " 'OEDC3',\n",
       " 'OEDC4',\n",
       " 'OEDC5',\n",
       " 'OEDC6',\n",
       " 'OEDC7',\n",
       " 'EC1',\n",
       " 'EC2',\n",
       " 'EC3',\n",
       " 'EC4',\n",
       " 'EC5',\n",
       " 'EC6',\n",
       " 'EC7',\n",
       " 'EC8',\n",
       " 'SEC1',\n",
       " 'SEC2',\n",
       " 'SEC3',\n",
       " 'SEC4',\n",
       " 'SEC5',\n",
       " 'AFC1',\n",
       " 'AFC2',\n",
       " 'AFC3',\n",
       " 'AFC4',\n",
       " 'AFC5',\n",
       " 'AFC6',\n",
       " 'VC1',\n",
       " 'VC2',\n",
       " 'VC3',\n",
       " 'VC4',\n",
       " 'ANC1',\n",
       " 'ANC2',\n",
       " 'ANC3',\n",
       " 'ANC4',\n",
       " 'ANC5',\n",
       " 'ANC6',\n",
       " 'ANC7',\n",
       " 'ANC8',\n",
       " 'ANC9',\n",
       " 'ANC10',\n",
       " 'ANC11',\n",
       " 'ANC12',\n",
       " 'ANC13',\n",
       " 'ANC14',\n",
       " 'ANC15',\n",
       " 'POBC1',\n",
       " 'POBC2',\n",
       " 'LSC1',\n",
       " 'LSC2',\n",
       " 'LSC3',\n",
       " 'LSC4',\n",
       " 'VOC1',\n",
       " 'VOC2',\n",
       " 'VOC3',\n",
       " 'HC1',\n",
       " 'HC2',\n",
       " 'HC3',\n",
       " 'HC4',\n",
       " 'HC5',\n",
       " 'HC6',\n",
       " 'HC7',\n",
       " 'HC8',\n",
       " 'HC9',\n",
       " 'HC10',\n",
       " 'HC11',\n",
       " 'HC12',\n",
       " 'HC13',\n",
       " 'HC14',\n",
       " 'HC15',\n",
       " 'HC16',\n",
       " 'HC17',\n",
       " 'HC18',\n",
       " 'HC19',\n",
       " 'HC20',\n",
       " 'HC21',\n",
       " 'MHUC1',\n",
       " 'MHUC2',\n",
       " 'AC1',\n",
       " 'AC2',\n",
       " 'ADATE_2',\n",
       " 'ADATE_3',\n",
       " 'ADATE_4',\n",
       " 'ADATE_5',\n",
       " 'ADATE_6',\n",
       " 'ADATE_7',\n",
       " 'ADATE_8',\n",
       " 'ADATE_9',\n",
       " 'ADATE_10',\n",
       " 'ADATE_11',\n",
       " 'ADATE_12',\n",
       " 'ADATE_13',\n",
       " 'ADATE_14',\n",
       " 'ADATE_15',\n",
       " 'ADATE_16',\n",
       " 'ADATE_17',\n",
       " 'ADATE_18',\n",
       " 'ADATE_19',\n",
       " 'ADATE_20',\n",
       " 'ADATE_21',\n",
       " 'ADATE_22',\n",
       " 'ADATE_23',\n",
       " 'ADATE_24',\n",
       " 'CARDPROM',\n",
       " 'MAXADATE',\n",
       " 'NUMPROM',\n",
       " 'CARDPM12',\n",
       " 'NUMPRM12',\n",
       " 'RDATE_3',\n",
       " 'RDATE_4',\n",
       " 'RDATE_5',\n",
       " 'RDATE_6',\n",
       " 'RDATE_7',\n",
       " 'RDATE_8',\n",
       " 'RDATE_9',\n",
       " 'RDATE_10',\n",
       " 'RDATE_11',\n",
       " 'RDATE_12',\n",
       " 'RDATE_13',\n",
       " 'RDATE_14',\n",
       " 'RDATE_15',\n",
       " 'RDATE_16',\n",
       " 'RDATE_17',\n",
       " 'RDATE_18',\n",
       " 'RDATE_19',\n",
       " 'RDATE_20',\n",
       " 'RDATE_21',\n",
       " 'RDATE_22',\n",
       " 'RDATE_23',\n",
       " 'RDATE_24',\n",
       " 'RAMNTALL',\n",
       " 'NGIFTALL',\n",
       " 'CARDGIFT',\n",
       " 'MINRAMNT',\n",
       " 'MINRDATE',\n",
       " 'MAXRAMNT',\n",
       " 'MAXRDATE',\n",
       " 'LASTGIFT',\n",
       " 'LASTDATE',\n",
       " 'TIMELAG',\n",
       " 'AVGGIFT',\n",
       " 'TARGET_D',\n",
       " 'HPHONE_D',\n",
       " 'RFA_2R',\n",
       " 'RFA_2F',\n",
       " 'RFA_2A',\n",
       " 'MDMAUD_R',\n",
       " 'MDMAUD_A',\n",
       " 'CLUSTER2',\n",
       " 'GEOCODE2',\n",
       " 'RFA_3R',\n",
       " 'RFA_3F',\n",
       " 'RFA_3A',\n",
       " 'RFA_4R',\n",
       " 'RFA_4F',\n",
       " 'RFA_4A',\n",
       " 'RFA_5A',\n",
       " 'RFA_6R',\n",
       " 'RFA_6F',\n",
       " 'RFA_6A',\n",
       " 'RFA_7R',\n",
       " 'RFA_7F',\n",
       " 'RFA_7A',\n",
       " 'RFA_8R',\n",
       " 'RFA_8F',\n",
       " 'RFA_8A',\n",
       " 'RFA_9R',\n",
       " 'RFA_9F',\n",
       " 'RFA_9A',\n",
       " 'RFA_10A',\n",
       " 'RFA_11R',\n",
       " 'RFA_11F',\n",
       " 'RFA_11A',\n",
       " 'RFA_12R',\n",
       " 'RFA_12F',\n",
       " 'RFA_12A',\n",
       " 'RFA_13A',\n",
       " 'RFA_14R',\n",
       " 'RFA_14F',\n",
       " 'RFA_14A',\n",
       " 'RFA_15A',\n",
       " 'RFA_16A',\n",
       " 'RFA_17A',\n",
       " 'RFA_18A',\n",
       " 'RFA_19A',\n",
       " 'RFA_20A',\n",
       " 'RFA_21A',\n",
       " 'RFA_22A',\n",
       " 'RFA_23A',\n",
       " 'RFA_24A',\n",
       " 'DOMAINUrbanicity',\n",
       " 'DOMAINSocioEconomic']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in learning_clean.columns if learning_clean[c].isna().sum() / len(learning_clean.index) <= 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    0.0\n",
       "mean     NaN\n",
       "std      NaN\n",
       "min      NaN\n",
       "25%      NaN\n",
       "50%      NaN\n",
       "75%      NaN\n",
       "max      NaN\n",
       "Name: TARGET_B, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_provider.clean_data.TARGET_B.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MDMAUD_F', 'TARGET_B'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to the low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "numeric_features = [c for c in learning_clean.columns if learning_clean[c].dtype in ['int64', 'Int64', 'float64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COLLECT1', 'VETERANS', 'BIBLE', 'CATLG', 'HOMEE', 'PETS', 'CDPLAY', 'STEREO', 'PCOWNERS', 'PHOTO', 'CRAFTS', 'FISHER', 'GARDENIN', 'BOATS', 'WALKER', 'KIDSTUFF', 'CARDS', 'PLATES']\n"
     ]
    }
   ],
   "source": [
    "lowvar_numeric = [c for c in numeric_features if learning_clean[c].var() < 1e-6]\n",
    "print(lowvar_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant categoricals have only one level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = learning_clean.select_dtypes(include=[\"category\", \"object\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowvar_categorical = [c for c in categorical_features.columns if len(set(categorical_features[c].unique())-set('nan')-set([np.nan])) == 1]\n",
    "print(lowvar_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowvar_features = lowvar_numeric + lowvar_categorical\n",
    "print(lowvar_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowvar_sparse_to_remove = set(lowvar_features + sparse_features)\n",
    "print(lowvar_sparse_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean.drop(lowvar_sparse_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several date features. ODATEDW is the date the record was added, DOB the birth date. ADATE_* and RDATE_* are from the promotion history. ADATE_* is the date of a mailing, RDATE_* the date the donation for the corresponding mailing was received. While these dates are not of particular interest (very low variance), the time it took to respond might be.\n",
    "Furthermore, there are the features MINRDATE, MAXRDATE, MAXADATE, FISTDATE, NEXTDATE and LASTDATE coming from the giving history file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dh.DATE_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean[dh.DATE_FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different transformations are applied:\n",
    "\n",
    "1. ODATEDW, DOB: Years before 1997 -> membership duration, age\n",
    "2. Giving history features: Relative time in months to 1997/06/01\n",
    "3. For the promotion history, as specified above, the time for response in months\n",
    "\n",
    "There are redundant features which can be safely removed, as is shown below:\n",
    "\n",
    "1. FISTDATE and NEXTDATE are contained in TIMELAG, the number of months between first and second donation\n",
    "2. DOB, the date of birth, is contained in the feature AGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we transform the dates from the giving history. First, we create two dataframes with the sending dates of the mailings and the dates when the gift (donation) for these was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_hist_transformer = ColumnTransformer([\n",
    "    (\"months_to_donation\",\n",
    "     MonthsToDonation(),\n",
    "     dh.PROMO_HISTORY_DATES+dh.GIVING_HISTORY_DATES\n",
    "     )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donation_responses = don_hist_transformer.fit_transform(learning_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_hist_feature_names = [n[n.find('__')+2:]\n",
    "                 for n in don_hist_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donation_responses = pd.DataFrame(\n",
    "    donation_responses, index=learning_clean.index, columns=don_hist_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean = learning_clean.merge(donation_responses, on=learning_clean.index.name)\n",
    "learning_clean.drop(dh.PROMO_HISTORY_DATES+dh.GIVING_HISTORY_DATES, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time delta computation of the remaining features with either a specific reference or the date of the most recent mailing as a reference:\n",
    "\n",
    "* Time since last donation, minimum- and maximum donation and receiving most recent promotion\n",
    "* Delta between first and next donation\n",
    "* Age, years of membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_transformer = ColumnTransformer([\n",
    "    (\"time_last_donation\", DeltaTime(unit='months'), ['LASTDATE','MINRDATE','MAXRDATE','MAXADATE']),\n",
    "    (\"delta_first_next\", DeltaTime(reference_date=learning.NEXTDATE), ['FISTDATE']),\n",
    "    (\"membership_years\", DeltaTime(unit='years'),['ODATEDW', 'DOB'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas = timedelta_transformer.fit_transform(learning_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta_feature_names = [n[n.find('__')+2:]\n",
    "                 for n in timedelta_transformer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas = pd.DataFrame(timedeltas, index=learning_clean.index,columns=timedelta_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean = learning_clean.merge(timedeltas, on=learning_clean.index.name)\n",
    "learning_clean.drop(dh.date_features, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying redundance of DOB <-> AGE and \\[FISTDATE, NEXTDATE\\] <-> TIMELAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = pd.DataFrame([learning_clean.AGE, timedeltas.DOB_DELTA_YEARS]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ages.loc[ages.AGE != ages.DOB_DELTA_YEARS,:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = pd.DataFrame([learning_clean.TIMELAG, timedeltas.FISTDATE_NEXTDATE_DELTA_MONTHS]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags.loc[lags.TIMELAG != lags.FISTDATE_NEXTDATE_DELTA_MONTHS,:].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed feature DOB is represented in the feature AGE already. So we can drop DOB_DELTA_YEARS. TIMELAG already holds the difference in months between FISTDATE and NEXTDATE, so this delta can also be safely removed together with the original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_clean.drop(['DOB_DELTA_YEARS', 'FISTDATE_NEXTDATE_DELTA_MONTHS'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing put together\n",
    "Again, the operations shown above are bundled together in `data_provider.Cleaner.preprocess()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_preprocessed = data_provider.preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_preprocessed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(learning_preprocessed)\n",
    "save_fig(\"missing_matrix\", tight_layout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(learning_preprocessed.drop(dh.US_CENSUS_FEATURES, axis=1))\n",
    "save_fig(\"missing_matrix_no_census\", tight_layout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal features cannot be imputed by sophisticated imputation methods. The nominal features are therefore first imputed using the mode of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = learning_preprocessed.select_dtypes(include=\"category\")\n",
    "categorical_features = categoricals.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_categoricals = categoricals.fillna(categoricals.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in imputed_categoricals[[c for c in imputed_categoricals.columns if imputed_categoricals[c].cat.categories.dtype == 'object']]:\n",
    "    print(\"{} has {} levels:\\n{}\".format(c,len(categoricals[c].cat.categories),categoricals[c].cat.categories))\n",
    "    print(\"Number of missing values left: {}\".format(imputed_categoricals[c].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_preprocessed[imputed_categoricals.columns] = imputed_categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_numerical = data_provider.numerical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3, weights=\"distance\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_numerical = learning_preprocessed.loc[:,learning_preprocessed.select_dtypes(\"number\").columns.values.tolist()]\n",
    "sparse_features = [c for c in learning_numerical.columns if learning_numerical[c].count() / len(learning_numerical.index) <= 0.2]\n",
    "print(sparse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_numerical.drop(sparse_features, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = imputer.fit_transform(learning_numerical.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fancyimpute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_numerical.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = IterativeImputer(n_iter=5,initial_strategy=\"median\", random_state=Config.get(\"random_seed\"),verbose=1).fit_transform(learning_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = pd.DataFrame(data=imputed, columns = learning_numerical.columns, index=learning_numerical.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the nominals (categorical features with string levels) are worked on. Those categoricals with high cardinality (many levels) are hashed so as to not increase dimensionality too much.\n",
    "The remaining features are one-hot encoded.\n",
    "https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing low-variance features\n",
    "Code: https://stackoverflow.com/questions/29298973/removing-features-with-low-variance-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_and_obj = learning_clean.select_dtypes(include=[\"category\", \"object\"]).columns.values.tolist()\n",
    "get_low_variance_columns(learning_clean, skip_columns=cat_and_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (95412, 421)\n",
      "[IterativeImputer] Ending imputation round 1/5, elapsed time 243.10\n",
      "[IterativeImputer] Ending imputation round 2/5, elapsed time 485.35\n",
      "[IterativeImputer] Ending imputation round 3/5, elapsed time 728.04\n",
      "[IterativeImputer] Ending imputation round 5/5, elapsed time 1213.38\n"
     ]
    }
   ],
   "source": [
    "learning_imputed = data_provider.numerical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95412 entries, 95515 to 185114\n",
      "Columns: 658 entries, MAILCODE to DOMAINUrbanicity_C\n",
      "dtypes: float64(420), int64(238)\n",
      "memory usage: 482.2 MB\n"
     ]
    }
   ],
   "source": [
    "learning_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_imputed.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
