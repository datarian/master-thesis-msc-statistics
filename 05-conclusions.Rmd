# Conclusions

## Comparison With Cup Winner

The winner, Urban Science Applications with their proprietary software Gainsmarts, chose a two-step approach as well. In the first step, a logistic regression was used. The second step consisted of linear regression.
Their software automated feature-engineering (by trying several different transformations for each feature) and feature selection through an expert system^[see [https://www.kdnuggets.com/meetings-past/kdd98/gain-kddcup98-release.html](https://www.kdnuggets.com/meetings-past/kdd98/gain-kddcup98-release.html)]. They selected a distinct subset of features for each step.

No information is available on SAS's apprach. Decisionhouse's Quadstone on 3rd rank also invested heavily in feature engineering. The proprietary software was specially designed for customer behaviour modeling. They used decision trees and scorecard models in combination.


## Achieved

* A solid preprocessing package, albeit data set specific, extensible and easy to use

## Shortcomings

* Poor prediction performance: Non-gaussian distribution of $\hat{y}_b$, which violates assumptions for Heckman-correctio., bias-variance tradeoff

* Problems with lax implementation of two-stage Heckman: @bushway2007magic

Prediction Error $PE(z) = \sigma_{\epsilon}^2+\text{Bias}^2(\hat{f}(z))+\text{Var}(\hat{f}(z))$. When model more complex, local structure picked up, coefficient estimates suffer from high Var as more terms included in model -> more bias can lead to decrease in variance, decreasing PE.

## Outlook

* Next iteration:
	* Try other imputation strategies:
        * Median perhaps too simple, introducing bias
        * Iterative imputer struggles with non-normal data due to linear models
        * CART imputation could be interesting
        * kNN problematic because of high dimensionality -> distances small, but maybe worth a try on powerful hardware

    * Revise outliers:
        * Relied on Yeo-Johnson transformation to normalize
        * Other possibilities: 

	* Feature Extraction
		* Based on domain knowledge, create new features from promotion / giving history

	* Feature Selection
		* Tune boruta to select less features

	* Choice of Models

* Will be easy to work on these specific areas given the infrastructure created in this thesis
