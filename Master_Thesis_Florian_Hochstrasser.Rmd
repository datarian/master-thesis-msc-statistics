---
author: "Florian Hochstrasser"
date: "`r Sys.Date()`"
title: Profit maximization for direct marketing campaigns
description: Master Thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Statistics
site: bookdown::bookdown_site
link-citations: yes
papersize: a4
fontsize: 11pt
mainfont: Times New Roman
sansfont: Lato
monofont: Source Code Pro
toc-depth: 2
bibliography: thesis.bib
biblio-style: apalike
documentclass: scrbook
classoption: DIV=12,captions=tableheading,oneside,titlepage
always_allow_html: yes
output: 
  html_document: 
    df_print: kable
---

# Introduction {#intro}

Placeholder


## Task Background
## Goal
## Conventions and Notes

<!--chapter:end:index.Rmd-->


# Data

Placeholder


## General Structure
## Exploratory Data Analysis
### Data Types
### Targets
### Skewness
### Correlations
### Donation Patterns

<!--chapter:end:02-data.Rmd-->


# Experimental Setup and Methods

Placeholder


## Tools Used
## Data Handling
## Data Preprocessing
### Cleaning
### Feature Engineering {#methods-feature-engineering}
### Imputation
#### K-Nearest Neighbors
#### Iterative imputation
#### Median Imputation and Categorical Indicator
### Feature Selection {#methods-feature-selection}
## Prediction {#methods-prediction}
### Optimization of $\alpha^*$
## Model Evaluation and -Selection {#eval-and-select}
### Evaluation
#### Randomized Grid Search
#### Cross-Validation
#### Performance Metrics
### Dealing With Imbalanced Data
### Algorithms
#### Random Forest
#### Gradient Boosting Machine
#### GLMnet
#### Multilayer Perceptron
#### Support Vector Machine
#### Bayesian Ridge Regression

<!--chapter:end:03-methods.Rmd-->

# Results and Discussion

Below, the results from data preprocessing, model evaluation, -selection and predictions are shown.

## Preprocessing With Package kdd98

The self-written package `kdd98` ensures consistent data provisioning. It handles downloading and preprocessing of the data set for both the learning and validation set. All preprocessing steps are trained on the learning data set. The individual trained transformations are persisted on disk. After training, the transformations can be applied on the validation data. This process is transparent to the user. It is enough to instantiate the data provider for either learning or validation data set and request the data. Examples for usage can be found in the Jupyter notebooks.

The data sets can be obtained at the following intermediate steps from `kdd98.data_handler.KDD98DataProvider`:

* **raw**, as imported through `pandas.read_csv()`
* **preprocessed**, input errors removed, correct data types for all features, missing at random (MAR) imputations applied
* **numeric**, after feature engineering (encoded categories, date and zip code transformations)
* **imputed**, with missing values imputed
* **all-relevant**, filtered down to a set of relevant features

For some transformers, behavior can be controlled by specifying parameters. The package's architecture furthermore makes it easy to implement additional transformation steps.

The source code, along with a short introduction, is available online^[see `r make_github_link("kdd98")`].



## Imputation

The evaluation of several imputation strategies led to a straightforward approach: Categorical features had a *missing* level added during encoding (Section \@ref(methods-feature-engineering)). All other features were imputed by their median value to account for the skewed distributions. The notebook `4_Imputation.ipynb`^[see `r make_github_link("notebooks", "4_Imputation.ipynb")`] contains details on the other approaches studied.

In concordance with the cup documentation's requirements, *sparse* features were dropped from the data set before imputation. A balance had to be found so as to not exclude too much information on the donation patterns. As a consequence, the threshold was set to $\geq 90$ % missing values.

Missing data after removing sparse features is shown in Figure \@ref(fig:before-impute). The matrix displays the complete data set, with missing values indicated by white cells.

(ref:before-impute-legend) Data before imputation of numeric features. The big complete block at the center is the US census data.

```{r before-impute, fig.cap="(ref:before-impute-legend) ", echo=F}
include_graphics("figures/imputation/missing-matrix.png")
```

The features with most and least missing values are shown in Figure \@ref(fig:most-fewest-missing). It is not surprising to find the *MONTHS_TO_DONATION_\** features among those with most missing because few examples respond to the promotions with a donation.

Among the incomplete features with least missing values, we find several of the US census features. The *RFA_\** features give the status in reference to promotion $i$. All examples who did donate at some point before have an RFA status. Thus, we can see when new members were added from the missing values in these features because newly added members do not have an RFA status yet.

(ref:m-f-missing-legend) Features with most (left) and fewest (right) missing values.

```{r most-fewest-missing, fig.cap="(ref:m-f-missing-legend)", out.width="49%", fig.show="hold", echo=F}
include_graphics(c("figures/imputation/most-missing.png", "figures/imputation/fewest-missing.png"))
```

## Feature Selection

After preprocessing and feature engineering, `r ncol(py$numeric$data)` features were present in the data. Using boruta, `r ncol(py$all_relevant$data)` features were identified as important, resulting in a `r round(1 - ncol(py$all_relevant$data) / ncol(py$numeric$data),2)*100` % reduction of the number of features. For details, refer to^[see make_github_link("notebooks", "5_Feature Extraction.ipynb")].

Three groups of features were selected.

Features from the **giving history** broadly correspond to those used in classical RFM models mentioned in literature (Section \@ref(intro)). It is reassuring to find them among the all-relevant features:

* Donation amount for promotion 14
* Summary features: All-time donation amount, all-time number of donations, smallest, average and largest donation, donation amount of most recent donation
* 24 Features on frequency and amount of donations as per the date of past donations
* Time since first donation, Time since largest donation, time since last donation
* Number of donations in response to card promotions
* Number of months between first and second donation
* An indicator for *star* donors

The **promotion history** features can be interpreted as a measure of the importance of the examples to the organization. Those who receive many promotions are deemed valuable:

* Number of promotions received
* Number promotions in last 12 months before the current promotion
* Number of card promotions received
* Number of card promotions in last 12 months before the current promotion

Features from the **US census** data seem to be concerned with the social status and wealth of the neighborhood of donors and by intuition make sense to be deemed relevant.

* Median and average home value
* Percentage of home values above some threshold (5 features)
* Percentage of renters paying more than 500 $
* DMA (designated market area, a geographical grouping)
* Median / average family / house income
* Per capita income
* Percentage of households with interest, rental or dividend income
* Percentage of adults with a Bachelor's degree
* Percentage of people born in state of residence



## Classifiers

The four classifiers that were evaluated can be compared in terms of their receiver operating characteristic (ROC), which indicates overall model performance through an area under the curve value (ROC-AUC), precision-recall (PR) curves, or by their individual performance expressed through confusion matrices and the scores for several different metrics.

(ref:conf-mat-legend) Confusion matrices for the 5 classifiers evaluated.

```{r conf-matrices, fig.show="hold", out.width="49%", echo=F}
knitr::include_graphics(c(
	"figures/learning/confusion_matrix_model_RF_refit_f1.png",
	"figures/learning/confusion_matrix_model_GBM_refit_f1.png",
	"figures/learning/confusion_matrix_model_GLMnet_refit_f1.png",
	"figures/learning/confusion_matrix_model_NNet_refit_f1.png",
	"figures/learning/confusion_matrix_model_SVM_refit_f1.png"
	))
```

The ROC-AUC curve is constructed by evaluating the false positive rate (FPR) against the true positive rate (TPR) at various thresholds for the predicted class probabilities of examples in the training data. The closer the curve is to the top-left corner, the better a model performs (we have a large TPR and at the same time a low FPR). Looking at the ROC-AUC curves shown in Figure \@ref(fig:roc-auc-curve), we see that all classifiers performed rather weak. In the case of imbalanced data, the majority class dominates this metric. The false positive rate is $FPR = \frac{FP}{TN}$. This means that as the false positives (FP) decrease, FPR does not change a lot.

(ref:roc-auc-legend) Comparison of ROC-AUC for the evaluated classifiers.

```{r roc-auc-curve, fig.cap="(ref:roc-auc-legend)", echo=F}

knitr::include_graphics("figures/learning/roc_auc_compared_refit_f1.png")
```

The PR curve with precision $P = \frac{TP}{TP+FP}$ "how many of the predicted positive cases are true positives?" plotted against recall $R = \frac{TP}{{TP+FP}}$ "how many of the true positive values can we predict correctly?" at different threshold values, is sensitive to false positives and, since TN is not  involved, better suited for the imbalanced data at hand. Figure \@ref(fig:p-r-curve) shows the models in direct comparison. Here, the more the curve is towards the top right corner, the better. In that case, a high precision can be maintained at high recall levels.

(ref:p-r-legend) Comparison of PR curves for the classifiers. Recall is plotted against precision for various threshold values of the predicted class probabilities. For good models, the curve is close to the top-right corner.

```{r p-r-curve, fig.cap="(ref:roc-auc-legend)", echo=F}

knitr::include_graphics("figures/learning/prec_rec_compared_refit_f1.png")
```

Evidently, all learned algorithms perform rather weak.

```{python clfs-result, include=F}
import pickle
with open(pathlib.Path(Config.get("data_store"), "classifier_performance_compared.pkl"), "rb") as f:
    clf_scores = pickle.load(f)
```

The three performance metrics for all classifiers are summarized in \@ref(tab:clf-result-table). By F1, the best classifier is the neural network. That estimator is used for the final prediction.

````{r clf-result-table, echo=F, results="asis"}
kable(py$clf_scores,
      booktabs = T,
      caption="Comparison of classifiers. Three metrics were evaluated for each classifier.") %>%
    kable_styling(latex_options=c("hold_position", "scale_down", full_width=T, position="Center"))
```


## Regressors

```{python regressor-stuff, include=F, cache=F}
import pickle
with open("models/target_d_transformer.pkl", "rb") as f:
    target_d_transformer = pickle.load(f)
box_cox_lambda = target_d_transformer.lambdas_[0]
```

Regressors were learned on the learning data set with *TARGET_D* as 

For the regression models, the target was transformed using a Box-Cox transformation with parameter $\lambda=`r py$box_cox_lambda`$. The goal was to linearize the target to improve regression model's performance. The transformed data somewhat resembles a normal distribution, although there are several modes to be made out.

```{r reg-targ-transform, fig.cap="Target before transformation (left) and after a Box-Cox transformation (right).", fig.show="hold", out.width="49%", echo=F}

knitr::include_graphics(c("figures/predictions/target_d-distribution.png", "figures/predictions/target_d-distribution-transformed.png"))
```

The resulting distribution of predictions on the training data from the models evaluated is shown in Figure \@ref(fig:reg-distrib). Except for SVR, the models produce very similar results. We again find the multimodal distribution, with the SVR capturing the true frequencies of the donation amounts better than the other models.

(ref:reg-distrib-legend) Distributions of the predicted donation amount for all regressors evaluated. Except for SVR, the models perform very similar.

```{r reg-distrib, fig.cap="(ref:reg-distrib-legend)", echo=F}

knitr::include_graphics("figures/predictions/regressor-predictions-comparison.png")
```


The comparison of $R^2$ for the models evaluated shows SVR as the best performing model (see Figure \@ref(fig:reg-eval)). The SVR was therefore chosen for use in the final profit prediction.

(ref:reg-eval-legend) Evaluation metric $R^2$ for all regression models evaluated. SVR performs superior, with near-perfect $R^2$.

```{r reg-eval, fig.cap="(ref:reg-eval-legend)", echo=F}

knitr::include_graphics("figures/predictions/regressor-score-comparison.png")
```

## Prediction

The intermediate steps for arriving at the final prediction will be examined in this section.

### Conditional Prediction of the Donation Amount

Using the regression model selected in section \@ref(regressors), the predicted donation amount for the learning data is strictly positive because the model was learned conditionally on the examples that made a donation (see Figure \@ref(fig:y-d-predict)). Compared to the original distribution in Figure \@ref(fig:reg-targ-transform), we see a similar distribution, with the mode at approximately 13 \$ and the largest donations in excess of 175 \$.

(ref:y-d-predict-legend) Conditionally predicted donation amounts, transformed back to the original scale (the model was trained on the Box-Cox transformed target).

```{r y-d-predict, fig.cap="(ref:y-d-predict-legend)", echo=F}

knitr::include_graphics("figures/predictions/y_d_predicted-inverse-transform.png")
```

### Profit Optimization

The correction factor $\alpha$, used to account for the bias introduced by learning the regression models on a non-random sample, was found as described in section \@ref(methods-prediction). First, expected profit $E(Z)$ was calculated using equation \@ref(eq:pi-alpha) for a grid of $\alpha$ values. Then, a polynomial model and a cubic spline were fitted to the data and $\alpha$ optimized subsequently.

As can be seen in Figure \@ref(fig:alpha-grid), profit is very sensitive to $\alpha$. This is not surprising given the distribution in Figure \@ref(fig:y-d-predict) which is narrowly concentrated around 13 $. Furthermore, the curve is constant over much of the domain, meaning that all examples were selected from approximately $\alpha=0.15$. The sharp spike combined with the large proportion of constant expected profit makes fitting a polynomial difficult. The cubic spline was therefore used to find the optimal value $\alpha^*$.

(ref:alpha-grid-legend) Expected profit for a range of $\alpha$ values in $[0,1]$ with overlay-ed cubic spline and polynomial function of order 12. $\alpha^*$ was found by considering the roots of the derivate of the cubic spline, choosing the root where expected profit was highest.

```{r alpha-grid, fig.cap="(ref:alpha-grid-legend)", echo=F}
knitr::include_graphics("figures/predictions/comparison-alpha-profit-models.png")
```

### Final Prediction

For final predictions, the complete learning data set was used to train the best performing classifier and regressor and find $\alpha^*$.

The learned estimators were then applied on the test data set. The results are shown in \@ref(tab:prediction-results). The theoretical maximum was calculated with: $\sum_{i=1}^n \mathbb{1}_{\{\text{TARGET}_{D,i} > 0.0\}}*(\text{TARGET}_{D,i} - u)$ with $u$ the unit cost of 0.68 $ per mailing.


```{r prediction-results, echo=F}
predictions <- t(data.frame(Learning = c(31530, 28951, 72375),
                            Test = c(31880, 3047, 72775)))
colnames(predictions) <- c('Members mailed, #','Net Profit, $ US', 'Theoretical maximum profit, $ US')

knitr::kable(predictions,
             booktabs = T,
             caption = "Prediction results for the learning and test data set.") %>%
    kableExtra::kable_styling(latex_options=c("hold_position", position="Center"))
```

<!--chapter:end:04-results-discussion.Rmd-->


# Conclusions

Placeholder


## Comparison With Cup Winners
## Biggest Problems Remaining

<!--chapter:end:05-conclusions.Rmd-->

# References {-}
<div id="refs"></div>

<!--chapter:end:06-references.Rmd-->


# (APPENDIX) Appendix {-} 

Placeholder


## Python Environment
## Data Set Dictionary {#data-set-dictionary}

<!--chapter:end:07-Appendix.Rmd-->

